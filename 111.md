# 🤖Intelligent Agents for Software Engineering: A Systematic Literature Review

## Table of Contents
* [Code Generation and Improvement](#Code-Generation-and-Improvement)
  * [Automated Code Generation](#Automated-Code-Generation)
  * [Code Optimization](#Code-Optimization)
  * [Code Refactoring and Repair](#Code-Refactoring-and-Repair)
    * [Code Refactoring](#Code-Refactoring)
    * [Bug Fixing](#Bug-Fixing)
  * [Security and Vulnerability Repair](#Security-and-Vulnerability-Repair)
    * [Vulnerability Detection](#Vulnerability-Detection)
    * [Vulnerability Fixing](#Vulnerability-Fixing)
* [Agent Collaboration and Coordination](#Agent-Collaboration-and-Coordination)
  * [Multi-Agent Collaboration](#Multi-Agent-Collaboration)
  * [Autonomous Multi-Agent Systems](#Autonomous-Multi-Agent-Systems)
  * [Cross-Agent Collaboration](#Cross-Agent-Collaboration)
* [Software Testing, Debugging, and Quality Assurance](#Software-Testing,-Debugging,-and-Quality-Assurance)
  * [Automated Testing](#Automated-Testing)
  * [Bug Fixing and Fault Localization](#Bug-Fixing-and-Fault-Localization)
    * [Fault Localization](#Fault-Localization)
    * [Automated Bug Fixing](#Automated-Bug-Fixing)
  * [Debugging Assistance](#Debugging-Assistance)
  * [Code Quality Assurance](#Code-Quality-Assurance)
* [Software Design and Architecture](#Software-Design-and-Architecture)
  * [System and Software Design](#System-and-Software-Design)
  * [Modeling and Design Representation](#Modeling-and-Design-Representation)
  * [Requirements Engineering](#Requirements-Engineering)
* [Human-AI Collaboration and Interaction](#Human-AI-Collaboration-and-Interaction)
  * [Human-in-the-Loop Systems](#Human-in-the-Loop-Systems)
  * [Interactive Coding](#Interactive-Coding)
  * [Personalized and Explainable Systems](#Personalized-and-Explainable-Systems)
* [Tool Integration and Automation](#Tool-Integration-and-Automation)
  * [Tool-Augmented Development](#Tool-Augmented-Development)
  * [Automation of Development Workflows](#Automation-of-Development-Workflows)
  * [Cross-Tool Integration](#Cross-Tool-Integration)
* [Evaluation and Benchmarking](#Evaluation-and-Benchmarking)
  * [Performance Evaluation](#Performance-Evaluation)
  * [Benchmarking Frameworks](#Benchmarking-Frameworks)
* [Safety, Robustness, and Alignment](#Safety-Robustness-and-Alignment)
  * [Safety and Alignment Concerns](#Safety-and-Alignment-Concerns)


## Code Generation and Improvement

### Automated Code Generation

seo2025vispathautomatedvisualizationcode, pan2025codecor, jin2025towards, hu2025qualityflow, mi2024promptv, zhang2024seeker, karanjai2024generating, ni2024treeofcodehybridapproachrobust, wang2024a2h, ni2024tree, hou2024chain, lin2024llms, palavalli2024using, zhang2024instruct, liu2024agents4plc, chi2024sela, sadik2024llm, zhao2024visioncoder, xia2024scenegenagent, zhang2024pair, wang2024python, yuan2024transagent, yang2024autoverus, islam2024mapcoder, lei2024autocoderenhancingcodelarge, hu2024scenecraft, mishra2024granite, fakhoury20243dgen, deshpande2024class, lin2024llm, lin2024soen, rasheed2024codepori, akl2024nesy, niu2024solution, fakih2024llm4plc, zhang2024codeagent, xu2023lemur, holtl2mac, tian2023test, sakib2024extending, dong2024self

- **[2025-02] VisPath: Automated Visualization Code Synthesis via Multi-Path Reasoning
  and Feedback-Driven Optimization** [[arXiv](http://arxiv.org/abs/2502.11140v1)]
  *Wonduk Seo, Seungyong Lee, Daye Kang, Zonghao Yuan, Seunghyun Lee*
  Abstract: Unprecedented breakthroughs in Large Language Models (LLMs) has amplified its
penetration into application of automated visualization code generation.
Few-shot prompting and query expansion techniques have notably enhanced data
visualization performance, however, still fail to overcome ambiguity and
complexity of natural language queries - imposing an inherent burden for manual
human intervention. To mitigate such limitations, we propose a holistic
framework VisPath : A Multi-Path Reasoning and Feedback-Driven Optimization
Framework for Visualization Code Generation, which systematically enhances code
quality through structured reasoning and refinement. VisPath is a multi-stage
framework, specially designed to handle underspecified queries. To generate a
robust final visualization code, it first utilizes initial query to generate
diverse reformulated queries via Chain-of-Thought (CoT) prompting, each
representing a distinct reasoning path. Refined queries are used to produce
candidate visualization scripts, consequently executed to generate multiple
images. Comprehensively assessing correctness and quality of outputs, VisPath
generates feedback for each image, which are then fed to aggregation module to
generate optimal result. Extensive experiments on benchmarks including
MatPlotBench and the Qwen-Agent Code Interpreter Benchmark show that VisPath
significantly outperforms state-of-the-art (SOTA) methods, increased up to
average 17%, offering a more reliable solution for AI-driven visualization code
generation.

- **[2025-01] CodeCoR: An LLM-Based Self-Reflective Multi-Agent Framework for Code
  Generation** [[arXiv](http://arxiv.org/abs/2501.07811v1)]
  *Ruwei Pan, Hongyu Zhang, Chao Liu*
  Abstract: Code generation aims to produce code that fulfills requirements written in
natural languages automatically. Large language Models (LLMs) like ChatGPT have
demonstrated promising effectiveness in this area. Nonetheless, these LLMs
often fail to ensure the syntactic and semantic correctness of the generated
code. Recently, researchers proposed multi-agent frameworks that guide LLMs
with different prompts to analyze programming tasks, generate code, perform
testing in a sequential workflow. However, the performance of the workflow is
not robust as the code generation depends on the performance of each agent. To
address this challenge, we propose CodeCoR, a self-reflective multi-agent
framework that evaluates the effectiveness of each agent and their
collaborations. Specifically, for a given task description, four agents in
CodeCoR generate prompts, code, test cases, and repair advice, respectively.
Each agent generates more than one output and prunes away the low-quality ones.
The generated code is tested in the local environment: the code that fails to
pass the generated test cases is sent to the repair agent and the coding agent
re-generates the code based on repair advice. Finally, the code that passes the
most number of generated test cases is returned to users. Our experiments on
four widely used datasets, HumanEval, HumanEval-ET, MBPP, and MBPP-ET,
demonstrate that CodeCoR significantly outperforms existing baselines (e.g.,
CodeCoT and MapCoder), achieving an average Pass@1 score of 77.8%.

- **[2025-01] Towards Advancing Code Generation with Large Language Models: A Research
  Roadmap** [[arXiv](http://arxiv.org/abs/2501.11354v1)]
  *Haolin Jin, Huaming Chen, Qinghua Lu, Liming Zhu*
  Abstract: Recently, we have witnessed the rapid development of large language models,
which have demonstrated excellent capabilities in the downstream task of code
generation. However, despite their potential, LLM-based code generation still
faces numerous technical and evaluation challenges, particularly when embedded
in real-world development. In this paper, we present our vision for current
research directions, and provide an in-depth analysis of existing studies on
this task. We propose a six-layer vision framework that categorizes code
generation process into distinct phases, namely Input Phase, Orchestration
Phase, Development Phase, and Validation Phase. Additionally, we outline our
vision workflow, which reflects on the currently prevalent frameworks. We
systematically analyse the challenges faced by large language models, including
those LLM-based agent frameworks, in code generation tasks. With these, we
offer various perspectives and actionable recommendations in this area. Our aim
is to provide guidelines for improving the reliability, robustness and
usability of LLM-based code generation systems. Ultimately, this work seeks to
address persistent challenges and to provide practical suggestions for a more
pragmatic LLM-based solution for future code generation endeavors.

- **[2025-01] QualityFlow: An Agentic Workflow for Program Synthesis Controlled by LLM
  Quality Checks** [[arXiv](http://arxiv.org/abs/2501.17167v1)]
  *Yaojie Hu, Qiang Zhou, Qihong Chen, Xiaopeng Li, Linbo Liu, Dejiao Zhang, Amit Kachroo, Talha Oz, Omer Tripp*
  Abstract: We introduce QualityFlow, a dynamic agentic workflow for program synthesis.
Given the English description of a programming problem and a set of unit tests,
the model's goal is to synthesize the correct program that solves the problem
and passes the tests. QualityFlow consists of multiple large language model
(LLM) agents that resemble a software development team, including code
generation, testing, and self-debugging. Existing program synthesis methods
face three major limitations: assumption of visible unit test conformity,
bottleneck of synthesized test quality, and deviation of self-debugging
trajectory. To address them, we propose the LLM Quality Checker, which
explicitly "imagines" whether the synthesized programs' execution would conform
to the unit tests. The Quality Checks dynamically control the workflow,
including actions to submit the final answer, clarify the problem statement,
and revert previous workflow steps. As a result, our Quality Checker can
precisely accept any correct program, mitigate faulty synthesized tests, and
prevent potential workflow deviation. The success of the Quality Checker
further enables Diversified Prompting, which encourages variations in LLM
responses to maximize the possibility that a correct program appears and passes
the quality check. In experiments, QualityFlow establishes the state-of-the-art
results on four program synthesis benchmarks: MBPP, HumanEval, and the stricter
evaluations of both MBPP and HumanEval from EvalPlus. Our systematic analysis
shows that the dynamic workflow controlled by LLM quality checks can outperform
static workflows and single-attempt zero-shot synthesis. The Quality Checker is
the center of our investigation, and we dissect its individual performance and
integrated impact on the workflow accuracy, as well as other ablations
experiments to justify our workflow design.

- **[2024-12] PromptV: Leveraging LLM-powered Multi-Agent Prompting for High-quality
  Verilog Generation** [[arXiv](http://arxiv.org/abs/2412.11014v1)]
  *Zhendong Mi, Renming Zheng, Haowen Zhong, Yue Sun, Shaoyi Huang*
  Abstract: Recent advances in agentic LLMs have demonstrated remarkable automated
Verilog code generation capabilities. However, existing approaches either
demand substantial computational resources or rely on LLM-assisted single-agent
prompt learning techniques, which we observe for the first time has a
degeneration issue - characterized by deteriorating generative performance and
diminished error detection and correction capabilities. This paper proposes a
novel multi-agent prompt learning framework to address these limitations and
enhance code generation quality. We show for the first time that multi-agent
architectures can effectively mitigate the degeneration risk while improving
code error correction capabilities, resulting in higher-quality Verilog code
generation. Experimental results show that the proposed method could achieve
96.4% and 96.5% pass@10 scores on VerilogEval Machine and Human benchmarks,
respectively while attaining 100% Syntax and 99.9% Functionality pass@5 metrics
on the RTLLM benchmark.

- **[2024-12] Seeker: Towards Exception Safety Code Generation with Intermediate
  Language Agents Framework** [[arXiv](http://arxiv.org/abs/2412.11713v1)]
  *Xuanming Zhang, Yuxuan Chen, Yiming Zheng, Zhexin Zhang, Yuan Yuan, Minlie Huang*
  Abstract: In real world software development, improper or missing exception handling
can severely impact the robustness and reliability of code. Exception handling
mechanisms require developers to detect, capture, and manage exceptions
according to high standards, but many developers struggle with these tasks,
leading to fragile code. This problem is particularly evident in open-source
projects and impacts the overall quality of the software ecosystem. To address
this challenge, we explore the use of large language models (LLMs) to improve
exception handling in code. Through extensive analysis, we identify three key
issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception
Block, and Distorted Handling Solution. These problems are widespread across
real world repositories, suggesting that robust exception handling practices
are often overlooked or mishandled. In response, we propose Seeker, a
multi-agent framework inspired by expert developer strategies for exception
handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler
to assist LLMs in detecting, capturing, and resolving exceptions more
effectively. Our work is the first systematic study on leveraging LLMs to
enhance exception handling practices in real development scenarios, providing
valuable insights for future improvements in code reliability.

- **[2024-12] Generating Move Smart Contracts based on Concepts** [[arXiv](http://arxiv.org/abs/2412.12513v1)]
  *Rabimba Karanjai, Sam Blackshear, Lei Xu, Weidong Shi*
  Abstract: The growing adoption of formal verification for smart contracts has spurred
the development of new verifiable languages like Move. However, the limited
availability of training data for these languages hinders effective code
generation by large language models (LLMs). This paper presents ConMover, a
novel framework that enhances LLM-based code generation for Move by leveraging
a knowledge graph of Move concepts and a small set of verified code examples.
ConMover integrates concept retrieval, planning, coding, and debugging agents
in an iterative process to refine generated code. Evaluations with various
open-source LLMs demonstrate substantial accuracy improvements over baseline
models. These results underscore ConMover's potential to address low-resource
code generation challenges, bridging the gap between natural language
descriptions and reliable smart contract development.

- **[2024-12] Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and
  Execution** [[arXiv](http://arxiv.org/abs/2412.14212v1)]
  *Ziyi Ni, Yifan Li, Daxiang Dong*
  Abstract: The exceptional capabilities of large language models (LLMs) have
substantially accelerated the rapid rise and widespread adoption of agents.
Recent studies have demonstrated that generating Python code to consolidate
LLM-based agents' actions into a unified action space (CodeAct) is a promising
approach for developing real-world LLM agents. However, this step-by-step code
generation approach often lacks consistency and robustness, leading to
instability in agent applications, particularly for complex reasoning and
out-of-domain tasks. In this paper, we propose a novel approach called
Tree-of-Code (ToC) to tackle the challenges of complex problem planning and
execution with an end-to-end mechanism. By integrating key ideas from both
Tree-of-Thought and CodeAct, ToC combines their strengths to enhance solution
exploration. In our framework, each final code execution result is treated as a
node in the decision tree, with a breadth-first search strategy employed to
explore potential solutions. The final outcome is determined through a voting
mechanism based on the outputs of the nodes.

- **[2024-12] A2H: A UI Converter from Android to HarmonyOS Platform** [[arXiv](http://arxiv.org/abs/2412.13693v2)]
  *Chen Wang, Lina Gong, Yujun Huang, Di Cui, Mingqiang Wei*
  Abstract: With the growing importance of smartphones, developers face the challenge of
creating separate applications for multiple platforms (e.g., Android, iOS, and
HarmonyOS), leading to increased development costs and longer iteration cycles.
One potential solution is to develop an app on one platform and then
automatically convert it to other platforms, reducing the need for separate
development efforts. However, migrating user interfaces (UIs) between platforms
is particularly challenging due to significant differences in layout structures
and development paradigms, such as the disparity between XML layout files in
Android and ArkUI framework in HarmonyOS. Manual conversion of UIs is
time-consuming, error-prone, and inefficient, necessitating an automated
solution to streamline the process and enable seamless migration from Android
to HarmonyOS. To address this challenge, we propose the A2H Converter, an
automated tool for migrating Android UIs to HarmonyOS. The tool employs an
large language model (LLM)-driven multi-agent framework to convert Android XML
layouts into HarmonyOS ArkUI layouts. Using the RAG combing with decision
rules, the system maps Android UI components to ArkUI equivalents, while a
reflective mechanism continuously improves conversion accuracy. A2H Converter
handles project-level layouts, ensuring consistency across multiple files and
addressing complex UI logic. Experiments on six Android applications collected
from GitHub demonstrate that our A2H Converter achieves a migration success
rate of over 90.1%, 89.3%, and 89.2% at the component, page, and project
levels, respectively. The demo video is available at. The tool is available at
http://124.70.54.129:37860/.

- **[2024-12] Tree-of-Code: A Tree-Structured Exploring Framework for End-to-End Code
  Generation and Execution in Complex Task Handling** [[arXiv](http://arxiv.org/abs/2412.15305v1)]
  *Ziyi Ni, Yifan Li, Ning Yang, Dou Shen, Pin Lv, Daxiang Dong*
  Abstract: Solving complex reasoning tasks is a key real-world application of agents.
Thanks to the pretraining of Large Language Models (LLMs) on code data, recent
approaches like CodeAct successfully use code as LLM agents' action, achieving
good results. However, CodeAct greedily generates the next action's code block
by relying on fragmented thoughts, resulting in inconsistency and instability.
Moreover, CodeAct lacks action-related ground-truth (GT), making its
supervision signals and termination conditions questionable in multi-turn
interactions. To address these issues, we first introduce a simple yet
effective end-to-end code generation paradigm, CodeProgram, which leverages
code's systematic logic to align with global reasoning and enable cohesive
problem-solving. Then, we propose Tree-of-Code (ToC), which self-grows
CodeProgram nodes based on the executable nature of the code and enables
self-supervision in a GT-free scenario. Experimental results on two datasets
using ten popular zero-shot LLMs show ToC remarkably boosts accuracy by nearly
20% over CodeAct with less than 1/4 turns. Several LLMs even perform better on
one-turn CodeProgram than on multi-turn CodeAct. To further investigate the
trade-off between efficacy and efficiency, we test different ToC tree sizes and
exploration mechanisms. We also highlight the potential of ToC's end-to-end
data generation for supervised and reinforced fine-tuning.

- **[2024-11] Chain-of-Programming (CoP) : Empowering Large Language Models for
  Geospatial Code Generation** [[arXiv](https://arxiv.org/pdf/2411.10753)]
  *Shuyang Hou, Haoyue Jiao, Zhangxiao Shen, Jianyuan Liang, Anqi Zhao, Xiaopu Zhang, Jianxun Wang, Huayi Wu*
  Abstract: With the rapid growth of interdisciplinary demands for geospatial modeling
and the rise of large language models (LLMs), geospatial code generation
technology has seen significant advancements. However, existing LLMs often face
challenges in the geospatial code generation process due to incomplete or
unclear user requirements and insufficient knowledge of specific platform
syntax rules, leading to the generation of non-executable code, a phenomenon
known as "code hallucination." To address this issue, this paper proposes a
Chain of Programming (CoP) framework, which decomposes the code generation
process into five steps: requirement analysis, algorithm design, code
implementation, code debugging, and code annotation. The framework incorporates
a shared information pool, knowledge base retrieval, and user feedback
mechanisms, forming an end-to-end code generation flow from requirements to
code without the need for model fine-tuning. Based on a geospatial problem
classification framework and evaluation benchmarks, the CoP strategy
significantly improves the logical clarity, syntactical correctness, and
executability of the generated code, with improvements ranging from 3.0% to
48.8%. Comparative and ablation experiments further validate the superiority of
the CoP strategy over other optimization approaches and confirm the rationality
and necessity of its key components. Through case studies on building data
visualization and fire data analysis, this paper demonstrates the application
and effectiveness of CoP in various geospatial scenarios. The CoP framework
offers a systematic, step-by-step approach to LLM-based geospatial code
generation tasks, significantly enhancing code generation performance in
geospatial tasks and providing valuable insights for code generation in other
vertical domains.

- **[2024-11] LLMs as Continuous Learners: Improving the Reproduction of Defective
  Code in Software Issues** [[arXiv](http://arxiv.org/abs/2411.13941v1)]
  *Yalan Lin, Yingwei Ma, Rongyu Cao, Binhua Li, Fei Huang, Xiaodong Gu, Yongbin Li*
  Abstract: Reproducing buggy code is the first and crucially important step in issue
resolving, as it aids in identifying the underlying problems and validating
that generated patches resolve the problem. While numerous approaches have been
proposed for this task, they primarily address common, widespread errors and
struggle to adapt to unique, evolving errors specific to individual code
repositories. To fill this gap, we propose EvoCoder, a multi-agent continuous
learning framework for issue code reproduction. EvoCoder adopts a reflection
mechanism that allows the LLM to continuously learn from previously resolved
problems and dynamically refine its strategies to new emerging challenges. To
prevent experience bloating, EvoCoder introduces a novel hierarchical
experience pool that enables the model to adaptively update common and
repo-specific experiences. Our experimental results show a 20\% improvement in
issue reproduction rates over existing SOTA methods. Furthermore, integrating
our reproduction mechanism significantly boosts the overall accuracy of the
existing issue-resolving pipeline.

- **[2024-11] Using a Feedback Loop for LLM-based Infrastructure as Code Generation** [[arXiv](http://arxiv.org/abs/2411.19043v1)]
  *Mayur Amarnath Palavalli, Mark Santolucito*
  Abstract: Code generation with Large Language Models (LLMs) has helped to increase
software developer productivity in coding tasks, but has yet to have
significant impact on the tasks of software developers that surround this code.
In particular, the challenge of infrastructure management remains an open
question. We investigate the ability of an LLM agent to construct
infrastructure using the Infrastructure as Code (IaC) paradigm. We particularly
investigate the use of a feedback loop that returns errors and warnings on the
generated IaC to allow the LLM agent to improve the code. We find that, for
each iteration of the loop, its effectiveness decreases exponentially until it
plateaus at a certain point and becomes ineffective.

- **[2024-11] Instruct or Interact? Exploring and Eliciting LLMs' Capability in Code
  Snippet Adaptation Through Prompt Engineering** [[arXiv](https://arxiv.org/abs/2411.15501)]
  *Tanghaoran Zhang, Yue Yu, Xinjun Mao, Shangwen Wang, Kang Yang, Yao Lu, Zhang Zhang, Yuxin Zhao*
  Abstract: Code snippet adaptation is a fundamental activity in the software development
process. Unlike code generation, code snippet adaptation is not a "free
creation", which requires developers to tailor a given code snippet in order to
fit specific requirements and the code context. Recently, large language models
(LLMs) have confirmed their effectiveness in the code generation task with
promising results. However, their performance on adaptation, a reuse-oriented
and context-dependent code change prediction task, is still unclear. To bridge
this gap, we conduct an empirical study to investigate the performance and
issues of LLMs on the adaptation task. We first evaluate the adaptation
performances of three popular LLMs and compare them to the code generation
task. Our result indicates that their adaptation ability is weaker than
generation, with a nearly 15% decrease on pass@1 and more context-related
errors. By manually inspecting 200 cases, we further investigate the causes of
LLMs' sub-optimal performance, which can be classified into three categories,
i.e., Unclear Requirement, Requirement Misalignment and Context Misapplication.
Based on the above empirical research, we propose an interactive prompting
approach to eliciting LLMs' adaptation ability. Experimental result reveals
that our approach greatly improve LLMs' adaptation performance. The
best-performing Human-LLM interaction successfully solves 159 out of the 202
identified defects and improves the pass@1 and pass@5 by over 40% compared to
the initial instruction-based prompt. Considering human efforts, we suggest
multi-agent interaction as a trade-off, which can achieve comparable
performance with excellent generalization ability. We deem that our approach
could provide methodological assistance for autonomous code snippet reuse and
adaptation with LLMs.

- **[2024-10] Agents4PLC: Automating Closed-loop PLC Code Generation and Verification
  in Industrial Control Systems using LLM-based Agents** [[arXiv](http://arxiv.org/abs/2410.14209v2)]
  *Zihan Liu, Ruinan Zeng, Dongxia Wang, Gengyun Peng, Jingyi Wang, Qiang Liu, Peiyu Liu, Wenhai Wang*
  Abstract: In industrial control systems, the generation and verification of
Programmable Logic Controller (PLC) code are critical for ensuring operational
efficiency and safety. While Large Language Models (LLMs) have made strides in
automated code generation, they often fall short in providing correctness
guarantees and specialized support for PLC programming. To address these
challenges, this paper introduces Agents4PLC, a novel framework that not only
automates PLC code generation but also includes code-level verification through
an LLM-based multi-agent system. We first establish a comprehensive benchmark
for verifiable PLC code generation area, transitioning from natural language
requirements to human-written-verified formal specifications and reference PLC
code. We further enhance our `agents' specifically for industrial control
systems by incorporating Retrieval-Augmented Generation (RAG), advanced prompt
engineering techniques, and Chain-of-Thought strategies. Evaluation against the
benchmark demonstrates that Agents4PLC significantly outperforms previous
methods, achieving superior results across a series of increasingly rigorous
metrics. This research not only addresses the critical challenges in PLC
programming but also highlights the potential of our framework to generate
verifiable code applicable to real-world industrial applications.

- **[2024-10] SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning** [[arXiv](http://arxiv.org/abs/2410.17238v1)]
  *Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei, Bangbang Liu, Tianqi Pang, Jacky Kwok, Ceyao Zhang, Bang Liu, Chenglin Wu*
  Abstract: Automated Machine Learning (AutoML) approaches encompass traditional methods
that optimize fixed pipelines for model selection and ensembling, as well as
newer LLM-based frameworks that autonomously build pipelines. While LLM-based
agents have shown promise in automating machine learning tasks, they often
generate low-diversity and suboptimal code, even after multiple iterations. To
overcome these limitations, we introduce Tree-Search Enhanced LLM Agents
(SELA), an innovative agent-based system that leverages Monte Carlo Tree Search
(MCTS) to optimize the AutoML process. By representing pipeline configurations
as trees, our framework enables agents to conduct experiments intelligently and
iteratively refine their strategies, facilitating a more effective exploration
of the machine learning solution space. This novel approach allows SELA to
discover optimal pathways based on experimental feedback, improving the overall
quality of the solutions. In an extensive evaluation across 20 machine learning
datasets, we compare the performance of traditional and agent-based AutoML
methods, demonstrating that SELA achieves a win rate of 65% to 80% against each
baseline across all datasets. These results underscore the significant
potential of agent-based strategies in AutoML, offering a fresh perspective on
tackling complex machine learning challenges.

- **[2024-10] LLM as a code generator in Agile Model Driven Development** [[arXiv](http://arxiv.org/abs/2410.18489v1)]
  *Ahmed R. Sadik, Sebastian Brulin, Markus Olhofer, Antonello Ceravola, Frank Joublin*
  Abstract: Leveraging Large Language Models (LLM) like GPT4 in the auto generation of
code represents a significant advancement, yet it is not without its
challenges. The ambiguity inherent in natural language descriptions of software
poses substantial obstacles to generating deployable, structured artifacts.
This research champions Model Driven Development (MDD) as a viable strategy to
overcome these challenges, proposing an Agile Model Driven Development (AMDD)
approach that employs GPT4 as a code generator. This approach enhances the
flexibility and scalability of the code auto generation process and offers
agility that allows seamless adaptation to changes in models or deployment
environments. We illustrate this by modeling a multi agent Unmanned Vehicle
Fleet (UVF) system using the Unified Modeling Language (UML), significantly
reducing model ambiguity by integrating the Object Constraint Language (OCL)
for code structure meta modeling, and the FIPA ontology language for
communication semantics meta modeling. Applying GPT4 auto generation
capabilities yields Java and Python code that is compatible with the JADE and
PADE frameworks, respectively. Our thorough evaluation of the auto generated
code verifies its alignment with expected behaviors and identifies enhancements
in agent interactions. Structurally, we assessed the complexity of code derived
from a model constrained solely by OCL meta models, against that influenced by
both OCL and FIPA ontology meta models. The results indicate that the ontology
constrained meta model produces inherently more complex code, yet its
cyclomatic complexity remains within manageable levels, suggesting that
additional meta model constraints can be incorporated without exceeding the
high risk threshold for complexity.

- **[2024-10] VisionCoder: Empowering Multi-Agent Auto-Programming for Image
  Processing with Hybrid LLMs** [[arXiv](http://arxiv.org/abs/2410.19245v1)]
  *Zixiao Zhao, Jing Sun, Zhiyuan Wei, Cheng-Hao Cai, Zhe Hou, Jin Song Dong*
  Abstract: In the field of automated programming, large language models (LLMs) have
demonstrated foundational generative capabilities when given detailed task
descriptions. However, their current functionalities are primarily limited to
function-level development, restricting their effectiveness in complex project
environments and specific application scenarios, such as complicated
image-processing tasks. This paper presents a multi-agent framework that
utilises a hybrid set of LLMs, including GPT-4o and locally deployed
open-source models, which collaboratively complete auto-programming tasks. Each
agent plays a distinct role in the software development cycle, collectively
forming a virtual organisation that works together to produce software
products. By establishing a tree-structured thought distribution and
development mechanism across project, module, and function levels, this
framework offers a cost-effective and efficient solution for code generation.
We evaluated our approach using benchmark datasets, and the experimental
results demonstrate that VisionCoder significantly outperforms existing methods
in image processing auto-programming tasks.

- **[2024-10] SceneGenAgent: Precise Industrial Scene Generation with Coding Agent** [[arXiv](http://arxiv.org/abs/2410.21909v1)]
  *Xiao Xia, Dan Zhang, Zibo Liao, Zhenyu Hou, Tianrui Sun, Jing Li, Ling Fu, Yuxiao Dong*
  Abstract: The modeling of industrial scenes is essential for simulations in industrial
manufacturing. While large language models (LLMs) have shown significant
progress in generating general 3D scenes from textual descriptions, generating
industrial scenes with LLMs poses a unique challenge due to their demand for
precise measurements and positioning, requiring complex planning over spatial
arrangement. To address this challenge, we introduce SceneGenAgent, an
LLM-based agent for generating industrial scenes through C# code. SceneGenAgent
ensures precise layout planning through a structured and calculable format,
layout verification, and iterative refinement to meet the quantitative
requirements of industrial scenarios. Experiment results demonstrate that LLMs
powered by SceneGenAgent exceed their original performance, reaching up to
81.0% success rate in real-world industrial scene generation tasks and
effectively meeting most scene generation requirements. To further enhance
accessibility, we construct SceneInstruct, a dataset designed for fine-tuning
open-source LLMs to integrate into SceneGenAgent. Experiments show that
fine-tuning open-source LLMs on SceneInstruct yields significant performance
improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our
code and data are available at https://github.com/THUDM/SceneGenAgent .

- **[2024/09] A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement.** [[Link](https://dl.acm.org/doi/10.1145/3691620.3695506)]
  *Huan Zhang, Wei Cheng, Yuhan Wu, Wei Hu*
Large language models (LLMs) have achieved impressive performance on code generation. Although prior studies enhanced LLMs with prompting techniques and code refinement, they still struggle with complex programming problems due to rigid solution plans. In this paper, we draw on pair programming practices to propose PairCoder, a novel LLM-based framework for code generation. PairCoder incorporates two collaborative LLM agents, namely a Navigator agent for high-level planning and a Driver agent for specific implementation. The Navigator is responsible for proposing promising solution plans, selecting the current optimal plan, and directing the next iteration round based on execution feedback. The Driver follows the guidance of Navigator to undertake initial code generation, code testing, and refinement. This interleaved and iterative workflow involves multi-plan exploration and feedback-based refinement, which mimics the collaboration of pair programmers. We evaluate PairCoder with both open-source and closed-source LLMs on various code generation benchmarks. Extensive experimental results demonstrate the superior accuracy of PairCoder, achieving relative pass@1 improvements of 12.00%-162.43% compared to prompting LLMs directly.

- **[2024-09] Python Symbolic Execution with LLM-powered Code Generation** [[arXiv](http://arxiv.org/abs/2409.09271v1)]
  *Wenhan Wang, Kaibo Liu, An Ran Chen, Ge Li, Zhi Jin, Gang Huang, Lei Ma*
  Abstract: Symbolic execution is a key technology in software testing, which generates
test cases by collecting symbolic path constraints and then solving constraints
with SMT solvers. Symbolic execution has been proven helpful in generating
high-coverage test cases, but its limitations, e.g., the difficulties in
solving path constraints, prevent it from broader usage in software testing.
Moreover, symbolic execution has encountered many difficulties when applied to
dynamically typed languages like Python, because it is extremely challenging to
translate the flexible Python grammar into rigid solvers.
  To overcome the main challenges of applying symbolic execution in Python, we
proposed an LLM-empowered agent, LLM-Sym, that automatically calls an SMT
solver, Z3, to solve execution path constraints. Based on an introductory-level
symbolic execution engine, our LLM agent can extend it to supporting programs
with complex data type `list'. The core contribution of LLM-Sym is translating
complex Python path constraints into Z3 code. To enable accurate path-to-Z3
translation, we design a multiple-step code generation pipeline including type
inference, retrieval and self-refine. Our experiments demonstrate that LLM-Sym
is capable of solving path constraints on Leetcode problems with complicated
control flows and list data structures, which is impossible for the backbone
symbolic execution engine. Our approach paves the way for the combination of
the generation ability of LLMs with the reasoning ability of symbolic solvers,
and opens up new opportunities in LLM-augmented test case generation.

- **[2024-09] TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation** [[arXiv](http://arxiv.org/abs/2409.19894v2)]
  *Zhiqiang Yuan, Weitong Chen, Hanlin Wang, Kai Yu, Xin Peng, Yiling Lou*
  Abstract: Code translation converts code from one programming language to another while
maintaining its original functionality, which is crucial for software
migration, system refactoring, and cross-platform development. Traditional
rule-based methods rely on manually-written rules, which can be time-consuming
and often result in less readable code. To overcome this, learning-based
methods have been developed, leveraging parallel data to train models for
automated code translation. More recently, the advance of Large Language Models
(LLMs) further boosts learning-based code translation. Although promising,
LLM-translated program still suffers from diverse quality issues (e.g., syntax
errors and semantic errors). In particular, it can be challenging for LLMs to
self-debug these errors when simply provided with the corresponding error
messages.
  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,
which enhances LLM-based code translation by fixing the syntax errors and
semantic errors with the synergy between four LLM-based agents, including
Initial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error
Fixer. The main insight of TRANSAGENT is to first localize the error code block
in the target program based on the execution alignment between the target and
source program, which can narrow down the fixing space and thus lower down the
fixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark
from recent programming tasks to mitigate the potential data leakage issue. On
our benchmark, TRANSAGENT outperforms the latest LLM-based code translation
technique UniTrans in both translation effectiveness and efficiency;
additionally, our evaluation on different LLMs show the generalization of
TRANSAGENT and our ablation study shows the contribution of each agent.

- **[2024-09] AutoVerus: Automated Proof Generation for Rust Code** [[arXiv](http://arxiv.org/abs/2409.13082v1)]
  *Chenyuan Yang, Xuheng Li, Md Rakib Hossain Misu, Jianan Yao, Weidong Cui, Yeyun Gong, Chris Hawblitzel, Shuvendu Lahiri, Jacob R. Lorch, Shuai Lu, Fan Yang, Ziqiao Zhou, Shan Lu*
  Abstract: Generative AI has shown its values for many software engineering tasks. Still
in its infancy, large language model (LLM)-based proof generation lags behind
LLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses
LLM to automatically generate correctness proof for Rust code. AutoVerus is
designed to match the unique features of Verus, a verification tool that can
prove the correctness of Rust code using proofs and specifications also written
in Rust. AutoVerus consists of a network of LLM agents that are crafted and
orchestrated to mimic human experts' three phases of proof construction:
preliminary proof generation, proof refinement guided by generic tips, and
proof debugging guided by verification errors. To thoroughly evaluate AutoVerus
and help foster future research in this direction, we have built a benchmark
suite of 150 non-trivial proof tasks, based on existing code-generation
benchmarks and verification benchmarks. Our evaluation shows that AutoVerus can
automatically generate correct proof for more than 90% of them, with more than
half of them tackled in less than 30 seconds or 3 LLM calls.

- **[2024/05] MapCoder: Multi-Agent Code Generation for Competitive Problem Solving.** [[Link](https://aclanthology.org/2024.acl-long.269.pdf)]
  *Md. Ashraful Islam, Mohammed Eunus Ali, Md Rizwan Parvez*
Code synthesis, which requires a deep understanding of complex natural language problem descriptions, generation of code instructions for complex algorithms and data structures, and the successful execution of comprehensive unit tests, presents a significant challenge. While large language models (LLMs) demonstrate impressive proficiency in natural language processing, their performance in code generation tasks remains limited. In this paper, we introduce a new approach to code generation tasks leveraging multi-agent prompting that uniquely replicates the full cycle of program synthesis as observed in human developers. Our framework, MapCoder, consists of four LLM agents specifically designed to emulate the stages of this cycle: recalling relevant examples, planning, code generation, and debugging. After conducting thorough experiments, with multiple LLM ablations and analyses across eight challenging competitive problem-solving and program synthesis benchmarks, MapCoder showcases remarkable code generation capabilities, achieving new state-of-the-art results (pass@1) on HumanEval (93.9%), MBPP (83.1%), APPS (22.0%), CodeContests (28.5%), and xCodeEval (45.3%). Moreover, our method consistently delivers superior performance across various programming languages and varying problem difficulties. We open-source our framework at this https URL.

- **[2024-05] AutoCoder: Enhancing Code Large Language Model with
  \textsc{AIEV-Instruct}** [[arXiv](https://arxiv.org/pdf/2405.14906)]
  *Bin Lei, Yuchen Li, Qiuwu Chen*
  Abstract: We introduce AutoCoder, the first Large Language Model to surpass GPT-4 Turbo
(April 2024) and GPT-4o in pass@1 on the Human Eval benchmark test
($\mathbf{90.9\%}$ vs. $\mathbf{90.2\%}$). In addition, AutoCoder offers a more
versatile code interpreter compared to GPT-4 Turbo and GPT-4o. It's code
interpreter can install external packages instead of limiting to built-in
packages. AutoCoder's training data is a multi-turn dialogue dataset created by
a system combining agent interaction and external code execution verification,
a method we term \textbf{\textsc{AIEV-Instruct}} (Instruction Tuning with
Agent-Interaction and Execution-Verified). Compared to previous large-scale
code dataset generation methods, \textsc{AIEV-Instruct} reduces dependence on
proprietary large models and provides execution-validated code dataset. The
code and the demo video is available in
\url{https://github.com/bin123apple/AutoCoder}.

- **[2024/05] SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code.** [[Link](https://dl.acm.org/doi/10.5555/3692070.3692846)]
  *Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, Alireza Fathi*
This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.

- **[2024-05] Granite Code Models: A Family of Open Foundation Models for Code
  Intelligence** [[arXiv](http://arxiv.org/abs/2405.04324v1)]
  *Mayank Mishra, Matt Stallone, Gaoyuan Zhang, Yikang Shen, Aditya Prasad, Adriana Meza Soria, Michele Merler, Parameswaran Selvam, Saptha Surendran, Shivdeep Singh, Manish Sethi, Xuan-Hong Dang, Pengyuan Li, Kun-Lung Wu, Syed Zawad, Andrew Coleman, Matthew White, Mark Lewis, Raju Pavuluri, Yan Koyfman, Boris Lublinsky, Maximilien de Bayser, Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Yi Zhou, Chris Johnson, Aanchal Goyal, Hima Patel, Yousaf Shah, Petros Zerfos, Heiko Ludwig, Asim Munawar, Maxwell Crouse, Pavan Kapanipathi, Shweta Salaria, Bob Calio, Sophia Wen, Seetharami Seelam, Brian Belgodere, Carlos Fonseca, Amith Singhee, Nirmit Desai, David D. Cox, Ruchir Puri, Rameswar Panda*
  Abstract: Large Language Models (LLMs) trained on code are revolutionizing the software
development process. Increasingly, code LLMs are being integrated into software
development environments to improve the productivity of human programmers, and
LLM-based agents are beginning to show promise for handling complex tasks
autonomously. Realizing the full potential of code LLMs requires a wide range
of capabilities, including code generation, fixing bugs, explaining and
documenting code, maintaining repositories, and more. In this work, we
introduce the Granite series of decoder-only code models for code generative
tasks, trained with code written in 116 programming languages. The Granite Code
models family consists of models ranging in size from 3 to 34 billion
parameters, suitable for applications ranging from complex application
modernization tasks to on-device memory-constrained use cases. Evaluation on a
comprehensive set of tasks demonstrates that Granite Code models consistently
reaches state-of-the-art performance among available open-source code LLMs. The
Granite Code model family was optimized for enterprise software development
workflows and performs well across a range of coding tasks (e.g. code
generation, fixing and explanation), making it a versatile all around code
model. We release all our Granite Code models under an Apache 2.0 license for
both research and commercial use.

- **[2024-04] 3DGen: AI-Assisted Generation of Provably Correct Binary Format Parsers** [[arXiv](https://arxiv.org/pdf/2404.10362)]
  *Sarah Fakhoury, Markus Kuppe, Shuvendu K. Lahiri, Tahina Ramananandro, Nikhil Swamy*
  Abstract: Improper parsing of attacker-controlled input is a leading source of software
security vulnerabilities, especially when programmers transcribe informal
format descriptions in RFCs into efficient parsing logic in low-level, memory
unsafe languages. Several researchers have proposed formal specification
languages for data formats from which efficient code can be extracted. However,
distilling informal requirements into formal specifications is challenging and,
despite their benefits, new, formal languages are hard for people to learn and
use.
  In this work, we present 3DGen, a framework that makes use of AI agents to
transform mixed informal input, including natural language documents (i.e.,
RFCs) and example inputs into format specifications in a language called 3D. To
support humans in understanding and trusting the generated specifications,
3DGen uses symbolic methods to also synthesize test inputs that can be
validated against an external oracle. Symbolic test generation also helps in
distinguishing multiple plausible solutions. Through a process of repeated
refinement, 3DGen produces a 3D specification that conforms to a test suite,
and which yields safe, efficient, provably correct, parsing code in C.
  We have evaluated 3DGen on 20 Internet standard formats, demonstrating the
potential for AI-agents to produce formally verified C code at a non-trivial
scale. A key enabler is the use of a domain-specific language to limit AI
outputs to a class for which automated, symbolic analysis is tractable.

- **[2024-04] Class-Level Code Generation from Natural Language Using Iterative,
  Tool-Enhanced Reasoning over Repository** [[arXiv](http://arxiv.org/abs/2405.01573v2)]
  *Ajinkya Deshpande, Anmol Agarwal, Shashank Shet, Arun Iyer, Aditya Kanade, Ramakrishna Bairi, Suresh Parthasarathy*
  Abstract: LLMs have demonstrated significant potential in code generation tasks,
achieving promising results at the function or statement level across various
benchmarks. However, the complexities associated with creating code artifacts
like classes, particularly within the context of real-world software
repositories, remain underexplored. Prior research treats class-level
generation as an isolated task, neglecting the intricate dependencies &
interactions that characterize real-world software environments. To address
this gap, we introduce RepoClassBench, a comprehensive benchmark designed to
rigorously evaluate LLMs in generating complex, class-level code within
real-world repositories. RepoClassBench includes "Natural Language to Class
generation" tasks across Java, Python & C# from a selection of repositories. We
ensure that each class in our dataset not only has cross-file dependencies
within the repository but also includes corresponding test cases to verify its
functionality. We find that current models struggle with the realistic
challenges posed by our benchmark, primarily due to their limited exposure to
relevant repository contexts. To address this shortcoming, we introduce
Retrieve-Repotools-Reflect (RRR), a novel approach that equips LLMs with static
analysis tools to iteratively navigate & reason about repository-level context
in an agent-based framework. Our experiments demonstrate that RRR significantly
outperforms existing baselines on RepoClassBench, showcasing its effectiveness
across programming languages & under various settings. Our findings emphasize
the critical need for code-generation benchmarks to incorporate repo-level
dependencies to more accurately reflect the complexities of software
development. Our work shows the benefits of leveraging specialized tools to
enhance LLMs' understanding of repository context. We plan to make our dataset
& evaluation harness public.

- **[2024-03] When LLM-based Code Generation Meets the Software Development Process** [[arXiv](https://arxiv.org/html/2403.15852v1)]
  *Feng Lin, Dong Jae Kim,  Tse-Husn,  Chen*
  Abstract: Software process models play a pivotal role in fostering collaboration and
communication within software teams, enabling them to tackle intricate
development tasks effectively. This paper introduces LCG, a code generation
framework inspired by established software engineering practices. LCG leverages
multiple Large Language Model (LLM) agents to emulate various software process
models, namely LCGWaterfall, LCGTDD, and LCGScrum. Each model assigns LLM
agents specific roles such as requirement engineer, architect, developer,
tester, and scrum master, mirroring typical development activities and
communication patterns. Through collaborative efforts utilizing
chain-of-thought and prompt composition techniques, the agents continuously
refine themselves to enhance code quality. Utilizing GPT3.5 as the underlying
LLM and baseline (GPT), we evaluate LCG across four code generation benchmarks:
HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Results indicate LCGScrum
outperforms other models, achieving Pass@1 scores of 75.2, 65.5, 82.5, and 56.7
in HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively - an average 15%
improvement over GPT. Analysis reveals distinct impacts of development
activities on generated code, with design and code reviews contributing to
enhanced exception handling, while design, testing, and code reviews mitigate
code smells. Furthermore, temperature values exhibit negligible influence on
Pass@1 across all models. However, variations in Pass@1 are notable for
different GPT3.5 model versions, ranging from 5 to over 60 in HumanEval,
highlighting the stability of LCG across model versions. This stability
underscores the importance of adopting software process models to bolster the
quality and consistency of LLM-generated code.

- **[2024-03] SOEN-101: Code Generation by Emulating Software Process Models Using
  Large Language Model Agents** [[arXiv](http://arxiv.org/abs/2403.15852v2)]
  *Feng Lin, Dong Jae Kim,  Tse-Husn,  Chen*
  Abstract: Software process models are essential to facilitate collaboration and
communication among software teams to solve complex development tasks. Inspired
by these software engineering practices, we present FlowGen - a code generation
framework that emulates software process models based on multiple Large
Language Model (LLM) agents. We emulate three process models, FlowGenWaterfall,
FlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e.,
requirement engineer, architect, developer, tester, and scrum master) that
correspond to everyday development activities and organize their communication
patterns. The agents work collaboratively using chain-of-thought and prompt
composition with continuous self-refinement to improve the code quality. We use
GPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion)
to evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP,
and MBPP-ET. Our findings show that FlowGenScrum excels compared to other
process models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval,
HumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement
over RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum
achieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming
Reflexion. Notably, integrating CodeT into FlowGenScrum resulted in
statistically significant improvements, achieving the highest Pass@1 scores.
Our analysis also reveals that the development activities impacted code smell
and exception handling differently, with design and code review adding more
exception handling and reducing code smells. Finally, FlowGen models maintain
stable Pass@1 scores across GPT3.5 versions and temperature values,
highlighting the effectiveness of software process models in enhancing the
quality and stability of LLM-generated code.

- **[2024-02] CodePori: Large-Scale System for Autonomous Software Development Using
  Multi-Agent Technology** [[arXiv](http://arxiv.org/abs/2402.01411v2)]
  *Zeeshan Rasheed, Malik Abdul Sami, Kai-Kristian Kemell, Muhammad Waseem, Mika Saari, Kari Systä, Pekka Abrahamsson*
  Abstract: Context: Large Language Models (LLMs) and Generative Pre-trained Transformers
(GPTs) have transformed the field of Software Engineering (SE). Existing
LLM-based multi-agent models have successfully addressed basic dialogue tasks.
However, the potential of LLMs for more challenging tasks, such as automated
code generation for large and complex projects, has been investigated in only a
few existing works. Objective: This paper aims to investigate the potential of
LLM-based agents in the software industry, particularly in enhancing
productivity and reducing time-to-market for complex software solutions. Our
primary objective is to gain insights into how these agents can fundamentally
transform the development of large-scale software. Methods: We introduce
CodePori, a novel system designed to automate code generation for large and
complex software projects based on functional and non-functional requirements
defined by stakeholders. To assess the proposed system performance, we utilized
the HumanEval benchmark and manually tested the CodePori model, providing 20
different project descriptions as input and then evaluated the code accuracy by
manually executing the code. Results: CodePori is able to generate running code
for large-scale projects, aligned with the typical software development
process. The HumanEval benchmark results indicate that CodePori improves code
accuracy by 89%. A manual assessment conducted by the first author shows that
the CodePori system achieved an accuracy rate of 85%. Conclusion: Based on the
results, our conclusion is that proposed system demonstrates the transformative
potential of LLM-based agents in SE, highlighting their practical applications
and opening new opportunities for broader adoption in both industry and
academia. Our project is publicly available at
https://github.com/GPT-Laboratory/CodePori.

- **[2024/02] NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification.** [[Link](https://openreview.net/pdf?id=2xNzIXRyAU)]
  *Hanna Abi Akl*
We present a neuro-symbolic (NeSy) workflow combining a symbolic-based learning technique with a large language model (LLM) agent to generate synthetic data for code comment classification in the C programming language. We also show how generating controlled synthetic data using this workflow fixes some of the notable weaknesses of LLM-based generation and increases the performance of classical machine learning models on the code comment classification task. Our best model, a Neural Network, achieves a Macro-F1 score of 91.412% with an increase of 1.033% after data augmentation.

- **[2024/02] Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning.** [[Link](https://dl.acm.org/doi/10.5555/3635637.3663007)]
  *Tong Niu, Weihao Zhang, Rong Zhao*
Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural network training, SAGE establishes a verifier-assisted iterative in-context learning process employing large language models (LLMs) to leverages their inherent cross-domain knowledge for tackling intricate demands from diverse domain scenarios. In SAGE, we introduce an semi-structured conceptual representation expliciting the intricate structures of ABMs and an objective representation to guide LLMs in modeling scenarios and proposing hypothetical solutions through in-context learning. To ensure the model executability and solution feasibility, SAGE devises a two-level verifier with chain-of-thought prompting tailored to the complex interactions and non-linear dynamics of ABMs, driving the iterative generation optimization. Moreover, we construct an evaluation dataset of solution-oriented ABMs from open this http URL contains practical models across various domains.

- **[2024/01] LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems.** [[Link](https://dl.acm.org/doi/pdf/10.1145/3639477.3639743)]
  *Mohamad Fakih, Rahul Dharmaji, Yasamin Moghaddas, Gustavo Quiros Araya, Oluwatosin Ogundare, Mohammad Abdullah Al Faruque*
Although Large Language Models (LLMs) have established pre-dominance in automated code generation, they are not devoid of shortcomings. The pertinent issues primarily relate to the absence of execution guarantees for generated code, a lack of explainability, and suboptimal support for essential but niche programming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to produce valid programs for Industrial Control Systems (ICS) operated by Programmable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided iterative pipeline leveraging user feedback and external verification tools including grammar checkers, compilers and SMV verifiers to guide the LLM's generation. We further enhance the generation potential of LLM by employing Prompt Engineering and model fine-tuning through the creation and usage of LoRAs. We validate this system using a FischerTechnik Manufacturing TestBed (MFTB), illustrating how LLMs can evolve from generating structurally flawed code to producing verifiably correct programs for industrial applications. We run a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned Code Llama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. The proposed pipeline improved the generation success rate from 47% to 72%, and the Survey-of-Experts code quality from 2.25/10 to 7.75/10. To promote open research, we share the complete experimental setup, the LLM Fine-Tuning Weights, and the video demonstrations of the different programs on our dedicated webpage.

- **[2024/01] CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges.** [[Link](https://aclanthology.org/2024.acl-long.737.pdf)]
  *Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, Zhi Jin*
Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. Real-world software development, however, often involves complex code repositories (named repo) with complex dependencies and extensive documentation. To fill this gap, our research pivots towards evaluating LLMs in a more realistic setting -- real-world repo-level code generation. We introduce CodeAgentBench, a manually curated benchmark for repo-level code generation. This benchmark comprises five high-quality Python projects, encompassing a total of 101 samples. We assess nine leading LLMs on repo-level tasks and observe a decline in their performance. To tackle this, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code symbol navigation, and code testing. We implement four agent strategies to optimize these tools' usage. Our experiments on CodeAgentBench show that CodeAgent enhances LLM performance significantly, with improvements ranging from 18.1\% to 250\%. Further tests on the HumanEval benchmark confirm CodeAgent's adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent's robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.

- **[2023/10] Lemur: Harmonizing Natural Language and Code for Language Agents.** [[Link](https://openreview.net/pdf?id=hNhwSmtXRh)]
  *Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Yusheng Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu*
We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human interaction, reasoning, and planning but also ensure grounding in the relevant environments. This calls for a harmonious blend of language and coding capabilities in the models. Lemur and Lemur-Chat are proposed to address this necessity, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either. Through meticulous pre-training using a code-intensive corpus and instruction fine-tuning on text and code data, our models achieve state-of-the-art averaged performance across diverse text and coding benchmarks among open-source models. Comprehensive experiments demonstrate Lemur's superiority over existing open-source models and its proficiency across various agent tasks involving human communication, tool usage, and interaction under fully- and partially- observable environments. The harmonization between natural and programming languages enables Lemur-Chat to significantly narrow the gap with proprietary models on agent abilities, providing key insights into developing advanced open-source agents adept at reasoning, planning, and operating seamlessly across environments. this https URL

- **[2023/10] L2MAC: Large Language Model Automatic Computer for Extensive Code Generation.** [[Link](https://openreview.net/pdf?id=EhrzQwsV4K)]
  *Samuel Holt, Max Ruiz Luyten, Mihaela van der Schaar*
Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and coherent outputs. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long output generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based general-purpose stored-program automatic computer (von Neumann architecture) framework, an LLM-based multi-agent system, for long and consistent output generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction in turn is executed by a separate LLM agent, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective interaction with the file store. These components enable L2MAC to generate extensive outputs, bypassing the constraints of the finite context window while producing outputs that fulfill a complex user-specified task. We empirically demonstrate that L2MAC achieves state-of-the-art performance in generating large codebases for system design tasks, significantly outperforming other coding methods in implementing the detailed user-specified task; we show that L2MAC works for general-purpose extensive text-based tasks, such as writing an entire book; and we provide valuable insights into L2MAC's performance improvement over existing methods.

- **[2023-09] Fixing Large Language Models' Specification Misunderstanding for Better
  Code Generation** [[arXiv](https://arxiv.org/pdf/2309.16120)]
  *Zhao Tian, Junjie Chen, Xiangyu Zhang*
  Abstract: Code generation is to automatically generate source code conforming to a
given programming specification, which has received extensive attention
especially with the development of large language models (LLMs). Due to the
inherent difficulty of code generation, the code generated by LLMs may not be
aligned with the specification. Although thought-eliciting prompting techniques
have been proposed to enhance the code generation performance of LLMs,
producing correct understanding for complicated programming problems remains
challenging, resulting in unsatisfactory performance. Also, some feedback-based
prompting techniques have been proposed to fix incorrect code using error
messages produced by test execution. However, when the generated code deviates
significantly from the ground truth, they encounter difficulties in improving
performance based on such coarse-grained information. In this work, we propose
a novel prompting technique, called {\mu}FiX, to improve the code generation
performance of LLMs by devising both sophisticated thought-eliciting prompting
and feedback-based prompting and making the first exploration on their synergy.
It first exploits test case analysis to obtain specification understanding and
enables a self-improvement process to identify and refine the misunderstanding
in the thought-eliciting prompting phase. {\mu}FiX further fixes the
specification understanding towards the direction reducing the gap between the
provided understanding (from the first phase) and the actual understanding
implicitly utilized by LLMs for code generation in the feedback-based prompting
phase. By improving the understanding with {\mu}FiX, the code generation
performance of LLMs can be largely improved. Our evaluation on two advanced
LLMs (ChatGPT and DeepSeek-Coder) with six widely-used benchmarks by comparing
with 15 baselines, demonstrates the effectiveness of {\mu}FiX ...

- **[2023/07] Extending the Frontier of ChatGPT: Code Generation and Debugging.** [[Link](https://ieeexplore.ieee.org/document/10698405)]
  *Fardin Ahsan Sakib, Saadat Hasan Khan, A. H. M. Rezaul Karim*
Large-scale language models (LLMs) have emerged as a groundbreaking innovation in the realm of question-answering and conversational agents. These models, leveraging different deep learning architectures such as Transformers, are trained on vast corpora to predict sentences based on given queries. Among these LLMs, ChatGPT, developed by OpenAI, has ushered in a new era by utilizing artificial intelligence (AI) to tackle diverse problem domains, ranging from composing essays and biographies to solving intricate mathematical integrals. The versatile applications enabled by ChatGPT offer immense value to users. However, assessing the performance of ChatGPT's output poses a challenge, particularly in scenarios where queries lack clear objective criteria for correctness. For instance, evaluating the quality of generated essays becomes arduous and relies heavily on manual labor, in stark contrast to evaluating solutions to well-defined, closed-ended questions such as mathematical problems. This research paper delves into the efficacy of ChatGPT in solving programming problems, examining both the correctness and the efficiency of its solution in terms of time and memory complexity. The research reveals a commendable overall success rate of 71.875\%, denoting the proportion of problems for which ChatGPT was able to provide correct solutions that successfully satisfied all the test cases present in Leetcode. It exhibits strengths in structured problems and shows a linear correlation between its success rate and problem acceptance rates. However, it struggles to improve solutions based on feedback, pointing to potential shortcomings in debugging tasks. These findings provide a compact yet insightful glimpse into ChatGPT's capabilities and areas for improvement.

- **[2023/04] Self-collaboration Code Generation via ChatGPT.** [[Link](https://dl.acm.org/doi/10.1145/3672459)]
  *Yihong Dong, Xue Jiang, Zhi Jin, Ge Li*
Although Large Language Models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, 1) Multiple LLM agents act as distinct `experts', each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other's work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development's analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9%-47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.







### Code Optimization
- **[2025-02] VisPath: Automated Visualization Code Synthesis via Multi-Path Reasoning
  and Feedback-Driven Optimization** [[arXiv](http://arxiv.org/abs/2502.11140v1)]
  *Wonduk Seo, Seungyong Lee, Daye Kang, Zonghao Yuan, Seunghyun Lee*
  Abstract: Unprecedented breakthroughs in Large Language Models (LLMs) has amplified its
penetration into application of automated visualization code generation.
Few-shot prompting and query expansion techniques have notably enhanced data
visualization performance, however, still fail to overcome ambiguity and
complexity of natural language queries - imposing an inherent burden for manual
human intervention. To mitigate such limitations, we propose a holistic
framework VisPath : A Multi-Path Reasoning and Feedback-Driven Optimization
Framework for Visualization Code Generation, which systematically enhances code
quality through structured reasoning and refinement. VisPath is a multi-stage
framework, specially designed to handle underspecified queries. To generate a
robust final visualization code, it first utilizes initial query to generate
diverse reformulated queries via Chain-of-Thought (CoT) prompting, each
representing a distinct reasoning path. Refined queries are used to produce
candidate visualization scripts, consequently executed to generate multiple
images. Comprehensively assessing correctness and quality of outputs, VisPath
generates feedback for each image, which are then fed to aggregation module to
generate optimal result. Extensive experiments on benchmarks including
MatPlotBench and the Qwen-Agent Code Interpreter Benchmark show that VisPath
significantly outperforms state-of-the-art (SOTA) methods, increased up to
average 17%, offering a more reliable solution for AI-driven visualization code
generation.

- **[2025-01] GREEN-CODE: Optimizing Energy Efficiency in Large Language Models for
  Code Generation** [[arXiv](http://arxiv.org/abs/2501.11006v1)]
  *Shashikant Ilager, Lukas Florian Briem, Ivona Brandic*
  Abstract: Large Language Models (LLMs) are becoming integral to daily life, showcasing
their vast potential across various Natural Language Processing (NLP) tasks.
Beyond NLP, LLMs are increasingly used in software development tasks, such as
code completion, modification, bug fixing, and code translation. Software
engineers widely use tools like GitHub Copilot and Amazon Q, streamlining
workflows and automating tasks with high accuracy. While the resource and
energy intensity of LLM training is often highlighted, inference can be even
more resource-intensive over time, as it's a continuous process with a high
number of invocations. Therefore, developing resource-efficient alternatives
for LLM inference is crucial for sustainability. This work proposes GREEN-CODE,
a framework for energy-aware code generation in LLMs. GREEN-CODE performs
dynamic early exit during LLM inference. We train a Reinforcement Learning (RL)
agent that learns to balance the trade-offs between accuracy, latency, and
energy consumption. Our approach is evaluated on two open-source LLMs, Llama
3.2 3B and OPT 2.7B, using the JavaCorpus and PY150 datasets. Results show that
our method reduces the energy consumption between 23-50 % on average for code
generation tasks without significantly affecting accuracy.

- **[2024-10] Improving Parallel Program Performance with LLM Optimizers via
  Agent-System Interface** [[arXiv](https://arxiv.org/pdf/2410.15625)]
  *Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken*
  Abstract: Modern scientific discovery increasingly relies on high-performance computing
for complex modeling and simulation. A key challenge in improving parallel
program performance is efficiently mapping tasks to processors and data to
memory, a process dictated by intricate, low-level system code known as
mappers. Developing high-performance mappers demands days of manual tuning,
posing a significant barrier for domain scientists without systems expertise.
We introduce a framework that automates mapper development with generative
optimization, leveraging richer feedback beyond scalar performance metrics. Our
approach features the Agent-System Interface, which includes a Domain-Specific
Language (DSL) to abstract away low-level complexity of system code and define
a structured search space, as well as AutoGuide, a mechanism that interprets
raw execution output into actionable feedback. Unlike traditional reinforcement
learning methods such as OpenTuner, which rely solely on scalar feedback, our
method finds superior mappers in far fewer iterations. With just 10 iterations,
it outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster
performance. Our approach finds mappers that surpass expert-written mappers by
up to 1.34X speedup across nine benchmarks while reducing tuning time from days
to minutes.

- **[2024-09] RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for
  Code Generation** [[arXiv](http://arxiv.org/abs/2409.09584v1)]
  *Qingyao Li, Wei Xia, Kounianhua Du, Xinyi Dai, Ruiming Tang, Yasheng Wang, Yong Yu, Weinan Zhang*
  Abstract: LLM agents enhanced by tree search algorithms have yielded notable
performances in code generation. However, current search algorithms in this
domain suffer from low search quality due to several reasons: 1) Ineffective
design of the search space for the high-reasoning demands of code generation
tasks, 2) Inadequate integration of code feedback with the search algorithm,
and 3) Poor handling of negative feedback during the search, leading to reduced
search efficiency and quality. To address these challenges, we propose to
search for the reasoning process of the code and use the detailed feedback of
code execution to refine erroneous thoughts during the search. In this paper,
we introduce RethinkMCTS, which employs the Monte Carlo Tree Search (MCTS)
algorithm to conduct thought-level searches before generating code, thereby
exploring a wider range of strategies. More importantly, we construct verbal
feedback from fine-grained code execution feedback to refine erroneous thoughts
during the search. This ensures that the search progresses along the correct
reasoning paths, thus improving the overall search quality of the tree by
leveraging execution feedback. Through extensive experiments, we demonstrate
that RethinkMCTS outperforms previous search-based and feedback-based code
generation baselines. On the HumanEval dataset, it improves the pass@1 of
GPT-3.5-turbo from 70.12 to 89.02 and GPT-4o-mini from 87.20 to 94.51. It
effectively conducts more thorough exploration through thought-level searches
and enhances the search quality of the entire tree by incorporating rethink
operation.

- **[2024-07] Patched MOA: optimizing inference for diverse software development tasks** [[arXiv](http://arxiv.org/abs/2407.18521v2)]
  *Asankhaya Sharma*
  Abstract: This paper introduces Patched MOA (Mixture of Agents), an inference
optimization technique that significantly enhances the performance of large
language models (LLMs) across diverse software development tasks. We evaluate
three inference optimization algorithms - Best of N, Mixture of Agents, and
Monte Carlo Tree Search and demonstrate that Patched MOA can boost the
performance of smaller models to surpass that of larger, more expensive models.
Notably, our approach improves the gpt-4o-mini model's performance on the
Arena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of
the cost. We also apply Patched MOA to various software development workflows,
showing consistent improvements in task completion rates. Our method is
model-agnostic, transparent to end-users, and can be easily integrated into
existing LLM pipelines. This work contributes to the growing field of LLM
optimization, offering a cost-effective solution for enhancing model
performance without the need for fine-tuning or larger models. Our
implementation is open-source and available at
https://github.com/codelion/optillm.

- **[2024-06] LLM-Vectorizer: LLM-based Verified Loop Vectorizer** [[arXiv](http://arxiv.org/abs/2406.04693v1)]
  *Jubi Taneja, Avery Laird, Cong Yan, Madan Musuvathi, Shuvendu K. Lahiri*
  Abstract: Vectorization is a powerful optimization technique that significantly boosts
the performance of high performance computing applications operating on large
data arrays. Despite decades of research on auto-vectorization, compilers
frequently miss opportunities to vectorize code. On the other hand, writing
vectorized code manually using compiler intrinsics is still a complex,
error-prone task that demands deep knowledge of specific architecture and
compilers.
  In this paper, we evaluate the potential of large-language models (LLMs) to
generate vectorized (Single Instruction Multiple Data) code from scalar
programs that process individual array elements. We propose a novel
finite-state machine multi-agents based approach that harnesses LLMs and
test-based feedback to generate vectorized code. Our findings indicate that
LLMs are capable of producing high performance vectorized code with run-time
speedup ranging from 1.1x to 9.4x as compared to the state-of-the-art compilers
such as Intel Compiler, GCC, and Clang.
  To verify the correctness of vectorized code, we use Alive2, a leading
bounded translation validation tool for LLVM IR. We describe a few
domain-specific techniques to improve the scalability of Alive2 on our
benchmark dataset. Overall, our approach is able to verify 38.2% of
vectorizations as correct on the TSVC benchmark dataset.

- **[2024/06] Agent-Driven Automatic Software Improvement** [[Link](https://dl.acm.org/doi/10.1145/3661167.3661171)]
  *Fernando Vallecillos Ruiz*
With software maintenance accounting for 50% of the cost of developing software, enhancing code quality and reliability has become more critical than ever. In response to this challenge, this doctoral research proposal aims to explore innovative solutions by focusing on the deployment of agents powered by Large Language Models (LLMs) to perform software maintenance tasks. The iterative nature of agents, which allows for continuous learning and adaptation, can help surpass common challenges in code generation. One distinct challenge is the last-mile problems, errors at the final stage of producing functionally and contextually relevant code. Furthermore, this project aims to surpass the inherent limitations of current LLMs in source code through a collaborative framework where agents can correct and learn from each other's errors. We aim to use the iterative feedback in these systems to further fine-tune the LLMs underlying the agents, becoming better aligned to the task of automated software improvement. Our main goal is to achieve a leap forward in the field of automatic software improvement by developing new tools and frameworks that can enhance the efficiency and reliability of software development.

- **[2024-04] Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra
  Large-Scale Code Generation and Optimization** [[arXiv](http://arxiv.org/abs/2404.02183v1)]
  *Yoichi Ishibashi, Yoshimasa Nishimura*
  Abstract: Recent advancements in automatic code generation using large language model
(LLM) agent have brought us closer to the future of automated software
development. However, existing single-agent approaches face limitations in
generating and improving large-scale, complex codebases due to constraints in
context length. To tackle this challenge, we propose Self-Organized multi-Agent
framework (SoA), a novel multi-agent framework that enables the scalable and
efficient generation and optimization of large-scale code. In SoA,
self-organized agents operate independently to generate and modify code
components while seamlessly collaborating to construct the overall codebase. A
key feature of our framework is the automatic multiplication of agents based on
problem complexity, allowing for dynamic scalability. This enables the overall
code volume to be increased indefinitely according to the number of agents,
while the amount of code managed by each agent remains constant. We evaluate
SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent
system, each agent in SoA handles significantly less code, yet the overall
generated code is substantially greater. Moreover, SoA surpasses the powerful
single-agent baseline by 5% in terms of Pass@1 accuracy.

- **[2024-02] Executable Code Actions Elicit Better LLM Agents** [[arXiv](https://arxiv.org/pdf/2402.01030)]
  *Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji*
  Abstract: Large Language Model (LLM) agents, capable of performing a broad range of
actions, such as invoking tools and controlling robots, show great potential in
tackling real-world challenges. LLM agents are typically prompted to produce
actions by generating JSON or text in a pre-defined format, which is usually
limited by constrained action space (e.g., the scope of pre-defined tools) and
restricted flexibility (e.g., inability to compose multiple tools). This work
proposes to use executable Python code to consolidate LLM agents' actions into
a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct
can execute code actions and dynamically revise prior actions or emit new
actions upon new observations through multi-turn interactions. Our extensive
analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that
CodeAct outperforms widely used alternatives (up to 20% higher success rate).
The encouraging performance of CodeAct motivates us to build an open-source LLM
agent that interacts with environments by executing interpretable code and
collaborates with users using natural language. To this end, we collect an
instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn
interactions using CodeAct. We show that it can be used with existing data to
improve models in agent-oriented tasks without compromising their general
capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with
Python interpreter and uniquely tailored to perform sophisticated tasks (e.g.,
model training) using existing libraries and autonomously self-debug.

  
### Code Refactoring and Repair

#### Code Refactoring
- **[2025-02] Distributed Approach to Haskell Based Applications Refactoring with LLMs
  Based Multi-Agent Systems** [[arXiv](http://arxiv.org/abs/2502.07928v1)]
  *Shahbaz Siddeeq, Zeeshan Rasheed, Malik Abdul Sami, Mahade Hasan, Muhammad Waseem, Jussi Rasku, Mika Saari, Kai-Kristian Kemell, Pekka Abrahamsson*
  Abstract: We present a large language models (LLMs) based multi-agent system to
automate the refactoring of Haskell codebases. The multi-agent system consists
of specialized agents performing tasks such as context analysis, refactoring,
validation, and testing. Refactoring improvements are using metrics such as
cyclomatic complexity, run-time, and memory allocation. Experimental
evaluations conducted on Haskell codebases demonstrate improvements in code
quality. Cyclomatic complexity was reduced by 13.64% and 47.06% in the
respective codebases. Memory allocation improved by 4.17% and 41.73%, while
runtime efficiency increased by up to 50%. These metrics highlight the systems
ability to optimize Haskells functional paradigms while maintaining correctness
and scalability. Results show reductions in complexity and performance
enhancements across codebases. The integration of LLMs based multi-agent system
enables precise task execution and inter-agent collaboration, addressing the
challenges of refactoring in functional programming. This approach aims to
address the challenges of refactoring functional programming languages through
distributed and modular systems.

- **[2024-11] Lingma SWE-GPT: An Open Development-Process-Centric Language Model for
  Automated Software Improvement** [[arXiv](http://arxiv.org/abs/2411.00622v1)]
  *Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei Huang, Yongbin Li*
  Abstract: Recent advancements in LLM-based agents have led to significant progress in
automatic software engineering, particularly in software maintenance and
evolution. Despite these encouraging advances, current research faces two major
challenges. First, SOTA performance primarily depends on closed-source models,
which significantly limits the technology's accessibility, and potential for
customization in diverse SE tasks. Second, these models are predominantly
trained on static code data, lacking a deep understanding of the dynamic
interactions, iterative problem-solving processes, and evolutionary
characteristics inherent in software development. To address these challenges,
our study adopts a software engineering perspective. We recognize that
real-world software maintenance and evolution processes encompass not only
static code data but also developers' thought processes, utilization of
external tools, and the interaction between different functional personnel.
Consequently, we introduce the Lingma SWE-GPT series, comprising Lingma SWE-GPT
7B and 72B. By learning from and simulating real-world code submission
activities, Lingma SWE-GPT systematically incorporates the dynamic interactions
and iterative problem-solving inherent in software development process, thereby
achieving a more comprehensive understanding of software improvement processes.
We conducted experimental evaluations using SWE-bench Verified benchmark. The
results demonstrate that Lingma SWE-GPT 72B successfully resolves 30.20% of the
GitHub issues, marking a significant improvement in automatic issue resolution
(22.76% relative improvement compared to Llama 3.1 405B), approaching the
performance of closed-source models (31.80\% issues of GPT-4o resolved).
Notably, Lingma SWE-GPT 7B resolves 18.20% of the issues, highlighting the
potential for applying smaller models to ASE tasks.

- **[2023/11] Intervenor: Prompting the coding ability of large language models with the interactive chain of repair.** [[Link](https://aclanthology.org/2024.findings-acl.124.pdf)]
  *Hanbin Wang, Zhenghao Liu, Shuo Wang, Ganqu Cui, Ning Ding, Zhiyuan Liu, Ge Yu*
This paper introduces INTERVENOR (INTERactiVE chaiN Of Repair), a system designed to emulate the interactive code repair processes observed in humans, encompassing both code diagnosis and code repair. INTERVENOR prompts Large Language Models (LLMs) to play distinct roles during the code repair process, functioning as both a Code Learner and a Code Teacher. Specifically, the Code Learner is tasked with adhering to instructions to generate or repair code, while the Code Teacher is responsible for crafting a Chain-of-Repair (CoR) to serve as guidance for the Code Learner. During generating the CoR, the Code Teacher needs to check the generated codes from Code Learner and reassess how to address code bugs based on error feedback received from compilers. Experimental results demonstrate that INTERVENOR surpasses baseline models, exhibiting improvements of approximately 18% and 4.3% over GPT-3.5 in code generation and code translation tasks, respectively. Our further analyses show that CoR is effective to illuminate the reasons behind bugs and outline solution plans in natural language. With the feedback of code compilers, INTERVENOR can accurately identify syntax errors and assertion errors and provide precise instructions to repair codes. All data and codes are available at this https URL




#### Bug Fixing
- **[2025-01] Evaluating Agent-based Program Repair at Google** [[arXiv](http://arxiv.org/abs/2501.07531v1)]
  *Pat Rondon, Renyao Wei, José Cambronero, Jürgen Cito, Aaron Sun, Siddhant Sanyam, Michele Tufano, Satish Chandra*
  Abstract: Agent-based program repair offers to automatically resolve complex bugs
end-to-end by combining the planning, tool use, and code generation abilities
of modern LLMs. Recent work has explored the use of agent-based repair
approaches on the popular open-source SWE-Bench, a collection of bugs from
highly-rated GitHub Python projects. In addition, various agentic approaches
such as SWE-Agent have been proposed to solve bugs in this benchmark. This
paper explores the viability of using an agentic approach to address bugs in an
enterprise context. To investigate this, we curate an evaluation set of 178
bugs drawn from Google's issue tracking system. This dataset spans both
human-reported (78) and machine-reported bugs (100).
  To establish a repair performance baseline on this benchmark, we implement
Passerine, an agent similar in spirit to SWE-Agent that can work within
Google's development environment. We show that with 20 trajectory samples and
Gemini 1.5 Pro, Passerine can produce a patch that passes bug tests (i.e.,
plausible) for 73% of machine-reported and 25.6% of human-reported bugs in our
evaluation set. After manual examination, we found that 43% of machine-reported
bugs and 17.9% of human-reported bugs have at least one patch that is
semantically equivalent to the ground-truth patch.
  These results establish a baseline on an industrially relevant benchmark,
which as we show, contains bugs drawn from a different distribution -- in terms
of language diversity, size, and spread of changes, etc. -- compared to those
in the popular SWE-Bench dataset.

- **[2024-11] LLMs as Continuous Learners: Improving the Reproduction of Defective
  Code in Software Issues** [[arXiv](http://arxiv.org/abs/2411.13941v1)]
  *Yalan Lin, Yingwei Ma, Rongyu Cao, Binhua Li, Fei Huang, Xiaodong Gu, Yongbin Li*
  Abstract: Reproducing buggy code is the first and crucially important step in issue
resolving, as it aids in identifying the underlying problems and validating
that generated patches resolve the problem. While numerous approaches have been
proposed for this task, they primarily address common, widespread errors and
struggle to adapt to unique, evolving errors specific to individual code
repositories. To fill this gap, we propose EvoCoder, a multi-agent continuous
learning framework for issue code reproduction. EvoCoder adopts a reflection
mechanism that allows the LLM to continuously learn from previously resolved
problems and dynamically refine its strategies to new emerging challenges. To
prevent experience bloating, EvoCoder introduces a novel hierarchical
experience pool that enables the model to adaptively update common and
repo-specific experiences. Our experimental results show a 20\% improvement in
issue reproduction rates over existing SOTA methods. Furthermore, integrating
our reproduction mechanism significantly boosts the overall accuracy of the
existing issue-resolving pipeline.

- **[2024-09] MarsCode Agent: AI-native Automated Bug Fixing** [[arXiv](http://arxiv.org/abs/2409.00899v2)]
  *Yizhou Liu, Pengfei Gao, Xinchen Wang, Jie Liu, Yexuan Shi, Zhao Zhang, Chao Peng*
  Abstract: Recent advances in large language models (LLMs) have shown significant
potential to automate various software development tasks, including code
completion, test generation, and bug fixing. However, the application of LLMs
for automated bug fixing remains challenging due to the complexity and
diversity of real-world software systems. In this paper, we introduce MarsCode
Agent, a novel framework that leverages LLMs to automatically identify and
repair bugs in software code. MarsCode Agent combines the power of LLMs with
advanced code analysis techniques to accurately localize faults and generate
patches. Our approach follows a systematic process of planning, bug
reproduction, fault localization, candidate patch generation, and validation to
ensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a
comprehensive benchmark of real-world software projects, and our results show
that MarsCode Agent achieves a high success rate in bug fixing compared to most
of the existing automated approaches.

- **[2024-08] Automated Code Fix Suggestions for Accessibility Issues in Mobile Apps** [[arXiv](http://arxiv.org/abs/2408.03827v1)]
  *Forough Mehralian, Titus Barik, Jeff Nichols, Amanda Swearngin*
  Abstract: Accessibility is crucial for inclusive app usability, yet developers often
struggle to identify and fix app accessibility issues due to a lack of
awareness, expertise, and inadequate tools. Current accessibility testing tools
can identify accessibility issues but may not always provide guidance on how to
address them. We introduce FixAlly, an automated tool designed to suggest
source code fixes for accessibility issues detected by automated accessibility
scanners. FixAlly employs a multi-agent LLM architecture to generate fix
strategies, localize issues within the source code, and propose code
modification suggestions to fix the accessibility issue. Our empirical study
demonstrates FixAlly's capability in suggesting fixes that resolve issues found
by accessibility scanners -- with an effectiveness of 77% in generating
plausible fix suggestions -- and our survey of 12 iOS developers finds they
would be willing to accept 69.4% of evaluated fix suggestions.

- **[2024-05] Can Github issues be solved with Tree Of Thoughts?** [[arXiv](http://arxiv.org/abs/2405.13057v1)]
  *Ricardo La Rosa, Corey Hulse, Bangdi Liu*
  Abstract: While there have been extensive studies in code generation by large language
models (LLM), where benchmarks like HumanEval have been surpassed with an
impressive 96.3% success rate, these benchmarks predominantly judge a model's
performance on basic function-level code generation and lack the critical
thinking and concept of scope required of real-world scenarios such as solving
GitHub issues. This research introduces the application of the Tree of Thoughts
(ToT) language model reasoning framework for enhancing the decision-making and
problem-solving abilities of LLMs for this complex task. Compared to
traditional input-output (IO) prompting and Retrieval Augmented Generation
(RAG) techniques, ToT is designed to improve performance by facilitating a
structured exploration of multiple reasoning trajectories and enabling
self-assessment of potential solutions. We experimentally deploy ToT in
tackling a Github issue contained within an instance of the SWE-bench. However,
our results reveal that the ToT framework alone is not enough to give LLMs the
critical reasoning capabilities to outperform existing methods. In this paper
we analyze the potential causes of these shortcomings and identify key areas
for improvement such as deepening the thought process and introducing agentic
capabilities. The insights of this research are aimed at informing future
directions for refining the application of ToT and better harnessing the
potential of LLMs in real-world problem-solving scenarios.

- **[2024-04] An Empirical Evaluation of Pre-trained Large Language Models for
  Repairing Declarative Formal Specifications** [[arXiv](http://arxiv.org/abs/2404.11050v1)]
  *Mohannad Alhanahnah, Md Rashedul Hasan, Hamid Bagheri*
  Abstract: Automatic Program Repair (APR) has garnered significant attention as a
practical research domain focused on automatically fixing bugs in programs.
While existing APR techniques primarily target imperative programming languages
like C and Java, there is a growing need for effective solutions applicable to
declarative software specification languages. This paper presents a systematic
investigation into the capacity of Large Language Models (LLMs) for repairing
declarative specifications in Alloy, a declarative formal language used for
software specification. We propose a novel repair pipeline that integrates a
dual-agent LLM framework, comprising a Repair Agent and a Prompt Agent. Through
extensive empirical evaluation, we compare the effectiveness of LLM-based
repair with state-of-the-art Alloy APR techniques on a comprehensive set of
benchmarks. Our study reveals that LLMs, particularly GPT-4 variants,
outperform existing techniques in terms of repair efficacy, albeit with a
marginal increase in runtime and token usage. This research contributes to
advancing the field of automatic repair for declarative specifications and
highlights the promising potential of LLMs in this domain.

- **[2024/04] AutoCodeRover: Autonomous Program Improvement** [[Link](https://dl.acm.org/doi/10.1145/3650212.3680384)]
  *Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, Abhik Roychoudhury*
Researchers have made significant progress in automating the software development process in the past decades. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. bug fixing) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving GitHub issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM's understanding of the issue's root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on SWE-bench-lite (300 real-life GitHub issues) show increased efficacy in solving GitHub issues (19% on SWE-bench-lite), which is higher than the efficacy of the recently reported SWE-agent. In addition, AutoCodeRover achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.

- **[2024-03] RepairAgent: An Autonomous, LLM-Based Agent for Program Repair** [[arXiv](http://arxiv.org/abs/2403.17134v2)]
  *Islem Bouzenia, Premkumar Devanbu, Michael Pradel*
  Abstract: Automated program repair has emerged as a powerful technique to mitigate the
impact of software bugs on system reliability and user experience. This paper
introduces RepairAgent, the first work to address the program repair challenge
through an autonomous agent based on a large language model (LLM). Unlike
existing deep learning-based approaches, which prompt a model with a fixed
prompt or in a fixed feedback loop, our work treats the LLM as an agent capable
of autonomously planning and executing actions to fix bugs by invoking suitable
tools. RepairAgent freely interleaves gathering information about the bug,
gathering repair ingredients, and validating fixes, while deciding which tools
to invoke based on the gathered information and feedback from previous fix
attempts. Key contributions that enable RepairAgent include a set of tools that
are useful for program repair, a dynamically updated prompt format that allows
the LLM to interact with these tools, and a finite state machine that guides
the agent in invoking the tools. Our evaluation on the popular Defects4J
dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164
bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM
imposes an average cost of 270,000 tokens per bug, which, under the current
pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To
the best of our knowledge, this work is the first to present an autonomous,
LLM-based agent for program repair, paving the way for future agent-based
techniques in software engineering.

- **[2024-01] Experimenting a New Programming Practice with LLMs** [[arXiv](https://arxiv.org/pdf/2401.01062)]
  *Simiao Zhang, Jiaping Wang, Guoliang Dong, Jun Sun, Yueling Zhang, Geguang Pu*
  Abstract: The recent development on large language models makes automatically
constructing small programs possible. It thus has the potential to free
software engineers from low-level coding and allow us to focus on the perhaps
more interesting parts of software development, such as requirement engineering
and system testing. In this project, we develop a prototype named AISD
(AI-aided Software Development), which is capable of taking high-level
(potentially vague) user requirements as inputs, generates detailed use cases,
prototype system designs, and subsequently system implementation. Different
from existing attempts, AISD is designed to keep the user in the loop, i.e., by
repeatedly taking user feedback on use cases, high-level system designs, and
prototype implementations through system testing. AISD has been evaluated with
a novel benchmark of non-trivial software projects. The experimental results
suggest that it might be possible to imagine a future where software
engineering is reduced to requirement engineering and system testing only.

- **[2023/07] Isolating Compiler Bugs by Generating Effective Witness Programs with Large Language Models** [[Link](https://ieeexplore.ieee.org/abstract/document/10521881)]
  *Haoxin Tu, Zhide Zhou, He Jiang, Imam Nur Bani Yusuf, Yuxian Li, Lingxiao Jiang*
Compiler bugs pose a significant threat to safety-critical applications, and promptly as well as effectively isolating these bugs is crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates the compiler bug isolation task. Existing compiler bug isolation approaches convert the problem into a test program mutation problem, but they are still limited by ineffective mutation strategies or high human effort requirements. Drawing inspiration from the recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT, in code generation, we propose a new approach named LLM4CBI to utilize LLMs to generate effective test programs for compiler bug isolation. However, using LLMs directly for test program mutation may not yield the desired results due to the challenges associated with formulating precise prompts and selecting specialized prompts. To overcome the challenges, three new components are designed in LLM4CBI. First, LLM4CBI utilizes a program complexity-guided prompt production component, which leverages data and control flow analysis to identify the most valuable variables and locations in programs for mutation. Second, LLM4CBI employs a memorized prompt selection component, which adopts reinforcement learning to select specialized prompts for mutating test programs continuously. Third, a test program validation component is proposed to select specialized feedback prompts to avoid repeating the same mistakes during the mutation process. Compared with state-of-the-art approaches over 120 real bugs from GCC and LLVM, our evaluation demonstrates the advantages of LLM4CBI: It can isolate 69.70%/21.74% and 24.44%/8.92% more bugs than DiWi and RecBi within Top-1/Top-5 ranked results. We also demonstrate that the LLMs component used in LLM4CBI can be easily replaced while still achieving reasonable results.


### Security and Vulnerability Repair

#### Vulnerability Detection
- **[2025-01] VulnBot: Autonomous Penetration Testing for A Multi-Agent Collaborative
  Framework** [[arXiv](http://arxiv.org/abs/2501.13411v1)]
  *He Kong, Die Hu, Jingguo Ge, Liangxiong Li, Tong Li, Bingzhen Wu*
  Abstract: Penetration testing is a vital practice for identifying and mitigating
vulnerabilities in cybersecurity systems, but its manual execution is
labor-intensive and time-consuming. Existing large language model
(LLM)-assisted or automated penetration testing approaches often suffer from
inefficiencies, such as a lack of contextual understanding and excessive,
unstructured data generation. This paper presents VulnBot, an automated
penetration testing framework that leverages LLMs to simulate the collaborative
workflow of human penetration testing teams through a multi-agent system. To
address the inefficiencies and reliance on manual intervention in traditional
penetration testing methods, VulnBot decomposes complex tasks into three
specialized phases: reconnaissance, scanning, and exploitation. These phases
are guided by a penetration task graph (PTG) to ensure logical task execution.
Key design features include role specialization, penetration path planning,
inter-agent communication, and generative penetration behavior. Experimental
results demonstrate that VulnBot outperforms baseline models such as GPT-4 and
Llama3 in automated penetration testing tasks, particularly showcasing its
potential in fully autonomous testing on real-world machines.

- **[2024-11] Fixing Security Vulnerabilities with AI in OSS-Fuzz** [[arXiv](http://arxiv.org/abs/2411.03346v2)]
  *Yuntong Zhang, Jiawei Wang, Dominic Berzin, Martin Mirchev, Dongge Liu, Abhishek Arya, Oliver Chang, Abhik Roychoudhury*
  Abstract: Critical open source software systems undergo significant validation in the
form of lengthy fuzz campaigns. The fuzz campaigns typically conduct a biased
random search over the domain of program inputs, to find inputs which crash the
software system. Such fuzzing is useful to enhance the security of software
systems in general since even closed source software may use open source
components. Hence testing open source software is of paramount importance.
Currently OSS-Fuzz is the most significant and widely used infrastructure for
continuous validation of open source systems. Unfortunately even though
OSS-Fuzz has identified more than 10,000 vulnerabilities across 1000 or more
software projects, the detected vulnerabilities may remain unpatched, as
vulnerability fixing is often manual in practice. In this work, we rely on the
recent progress in Large Language Model (LLM) agents for autonomous program
improvement including bug fixing. We customise the well-known AutoCodeRover
agent for fixing security vulnerabilities. This is because LLM agents like
AutoCodeRover fix bugs from issue descriptions via code search. Instead for
security patching, we rely on the test execution of the exploit input to
extract code elements relevant to the fix. Our experience with OSS-Fuzz
vulnerability data shows that LLM agent autonomy is useful for successful
security patching, as opposed to approaches like Agentless where the control
flow is fixed. More importantly our findings show that we cannot measure
quality of patches by code similarity of the patch with reference codes (as in
CodeBLEU scores used in VulMaster), since patches with high CodeBLEU scores
still fail to pass given the given exploit input. Our findings indicate that
security patch correctness needs to consider dynamic attributes like test
executions as opposed to relying of standard text/code similarity metrics.

- **[2024-09] AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation
  through Static Analysis and Fuzz Testing** [[arXiv](http://arxiv.org/abs/2409.10737v2)]
  *Ana Nunez, Nafis Tanveer Islam, Sumit Kumar Jha, Peyman Najafirad*
  Abstract: Recent advancements in automatic code generation using large language models
(LLMs) have brought us closer to fully automated secure software development.
However, existing approaches often rely on a single agent for code generation,
which struggles to produce secure, vulnerability-free code. Traditional program
synthesis with LLMs has primarily focused on functional correctness, often
neglecting critical dynamic security implications that happen during runtime.
To address these challenges, we propose AutoSafeCoder, a multi-agent framework
that leverages LLM-driven agents for code generation, vulnerability analysis,
and security enhancement through continuous collaboration. The framework
consists of three agents: a Coding Agent responsible for code generation, a
Static Analyzer Agent identifying vulnerabilities, and a Fuzzing Agent
performing dynamic testing using a mutation-based fuzzing approach to detect
runtime errors. Our contribution focuses on ensuring the safety of multi-agent
code generation by integrating dynamic and static testing in an iterative
process during code generation by LLM that improves security. Experiments using
the SecurityEval dataset demonstrate a 13% reduction in code vulnerabilities
compared to baseline LLMs, with no compromise in functionality.

- **[2024-04] LLM Agents can Autonomously Exploit One-day Vulnerabilities** [[arXiv](https://arxiv.org/pdf/2404.08144)]
  *Richard Fang, Rohan Bindu, Akul Gupta, Daniel Kang*
  Abstract: LLMs have becoming increasingly powerful, both in their benign and malicious
uses. With the increase in capabilities, researchers have been increasingly
interested in their ability to exploit cybersecurity vulnerabilities. In
particular, recent work has conducted preliminary studies on the ability of LLM
agents to autonomously hack websites. However, these studies are limited to
simple vulnerabilities.
  In this work, we show that LLM agents can autonomously exploit one-day
vulnerabilities in real-world systems. To show this, we collected a dataset of
15 one-day vulnerabilities that include ones categorized as critical severity
in the CVE description. When given the CVE description, GPT-4 is capable of
exploiting 87% of these vulnerabilities compared to 0% for every other model we
test (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP
and Metasploit). Fortunately, our GPT-4 agent requires the CVE description for
high performance: without the description, GPT-4 can exploit only 7% of the
vulnerabilities. Our findings raise questions around the widespread deployment
of highly capable LLM agents.

- **[2024-03] AgentFL: Scaling LLM-based Fault Localization to Project-Level Context** [[arXiv](http://arxiv.org/abs/2403.16362v1)]
  *Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin Wang, Xiaoling Li, Xiaoguang Mao*
  Abstract: Fault Localization (FL) is an essential step during the debugging process.
With the strong capabilities of code comprehension, the recent Large Language
Models (LLMs) have demonstrated promising performance in diagnosing bugs in the
code. Nevertheless, due to LLMs' limited performance in handling long contexts,
existing LLM-based fault localization remains on localizing bugs within a small
code scope (i.e., a method or a class), which struggles to diagnose bugs for a
large code scope (i.e., an entire software system). To address the limitation,
this paper presents AgentFL, a multi-agent system based on ChatGPT for
automated fault localization. By simulating the behavior of a human developer,
AgentFL models the FL task as a three-step process, which involves
comprehension, navigation, and confirmation. Within each step, AgentFL hires
agents with diversified expertise, each of which utilizes different tools to
handle specific tasks. Particularly, we adopt a series of auxiliary strategies
such as Test Behavior Tracking, Document-Guided Search, and Multi-Round
Dialogue to overcome the challenges in each step. The evaluation on the widely
used Defects4J-V1.2.0 benchmark shows that AgentFL can localize 157 out of 395
bugs within Top-1, which outperforms the other LLM-based approaches and
exhibits complementarity to the state-of-the-art learning-based techniques.
Additionally, we confirm the indispensability of the components in AgentFL with
the ablation study and demonstrate the usability of AgentFL through a user
study. Finally, the cost analysis shows that AgentFL spends an average of only
0.074 dollars and 97 seconds for a single bug.
 
#### Vulnerability Fixing
- **[2024-03] ACFIX: Guiding LLMs with Mined Common RBAC Practices for Context-Aware
  Repair of Access Control Vulnerabilities in Smart Contracts** [[arXiv](https://arxiv.org/pdf/2403.06838)]
  *Lyuye Zhang, Kaixuan Li, Kairan Sun, Daoyuan Wu, Ye Liu, Haoye Tian, Yang Liu*
  Abstract: Smart contracts are susceptible to various security issues, among which
access control (AC) vulnerabilities are particularly critical. While existing
research has proposed multiple detection tools, the automatic and appropriate
repair of AC vulnerabilities in smart contracts remains a challenge. Unlike
commonly supported vulnerability types by existing repair tools, such as
reentrancy, which are usually fixed by template-based approaches, the main
obstacle of AC lies in identifying the appropriate roles or permissions amid a
long list of non-AC-related source code to generate proper patch code, a task
that demands human-level intelligence.
  Leveraging recent advancements in large language models (LLMs), we employ the
state-of-the-art GPT-4 model and enhance it with a novel approach called ACFIX.
The key insight is that we can mine common AC practices for major categories of
code functionality and use them to guide LLMs in fixing code with similar
functionality. To this end, ACFIX involves both offline and online phases.
First, during the offline phase, ACFIX mines a taxonomy of common Role-based
Access Control (RBAC) practices from 344,251 on-chain contracts, categorizing
49 role-permission pairs from the top 1,000 pairs mined. Second, during the
online phase, ACFIX tracks AC-related elements across the contract and uses
this context information along with a Chain-of-Thought pipeline to guide LLMs
in identifying the most appropriate role-permission pair for the subject
contract and subsequently generating a suitable patch. This patch will then
undergo a validity and effectiveness check. To evaluate ACFIX, we built the
first benchmark dataset of 118 real-world AC vulnerabilities, and our
evaluation revealed that ACFIX successfully repaired 94.92% of them. This
represents a significant improvement compared to the baseline GPT-4, which
achieved only 52.54%.

- **[2024-03] Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract
  Auditing with Justifications** [[arXiv](https://arxiv.org/pdf/2403.16073)]
  *Wei Ma, Daoyuan Wu, Yuqiang Sun, Tianwen Wang, Shangqing Liu, Jian Zhang, Yue Xue, Yang Liu*
  Abstract: Smart contracts are decentralized applications built atop blockchains like
Ethereum. Recent research has shown that large language models (LLMs) have
potential in auditing smart contracts, but the state-of-the-art indicates that
even GPT-4 can achieve only 30% precision (when both decision and justification
are correct). This is likely because off-the-shelf LLMs were primarily
pre-trained on a general text/code corpus and not fine-tuned on the specific
domain of Solidity smart contract auditing.
  In this paper, we propose iAudit, a general framework that combines
fine-tuning and LLM-based agents for intuitive smart contract auditing with
justifications. Specifically, iAudit is inspired by the observation that expert
human auditors first perceive what could be wrong and then perform a detailed
analysis of the code to identify the cause. As such, iAudit employs a two-stage
fine-tuning approach: it first tunes a Detector model to make decisions and
then tunes a Reasoner model to generate causes of vulnerabilities. However,
fine-tuning alone faces challenges in accurately identifying the optimal cause
of a vulnerability. Therefore, we introduce two LLM-based agents, the Ranker
and Critic, to iteratively select and debate the most suitable cause of
vulnerability based on the output of the fine-tuned Reasoner model. To evaluate
iAudit, we collected a balanced dataset with 1,734 positive and 1,810 negative
samples to fine-tune iAudit. We then compared it with traditional fine-tuned
models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) as well as prompt
learning-based LLMs (GPT4, GPT-3.5, and CodeLlama-13b/34b). On a dataset of 263
real smart contract vulnerabilities, iAudit achieves an F1 score of 91.21% and
an accuracy of 91.11%. The causes generated by iAudit achieved a consistency of
about 38% compared to the ground truth causes.

- **[2024-03] RepairAgent: An Autonomous, LLM-Based Agent for Program Repair** [[arXiv](http://arxiv.org/abs/2403.17134v2)]
  *Islem Bouzenia, Premkumar Devanbu, Michael Pradel*
  Abstract: Automated program repair has emerged as a powerful technique to mitigate the
impact of software bugs on system reliability and user experience. This paper
introduces RepairAgent, the first work to address the program repair challenge
through an autonomous agent based on a large language model (LLM). Unlike
existing deep learning-based approaches, which prompt a model with a fixed
prompt or in a fixed feedback loop, our work treats the LLM as an agent capable
of autonomously planning and executing actions to fix bugs by invoking suitable
tools. RepairAgent freely interleaves gathering information about the bug,
gathering repair ingredients, and validating fixes, while deciding which tools
to invoke based on the gathered information and feedback from previous fix
attempts. Key contributions that enable RepairAgent include a set of tools that
are useful for program repair, a dynamically updated prompt format that allows
the LLM to interact with these tools, and a finite state machine that guides
the agent in invoking the tools. Our evaluation on the popular Defects4J
dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164
bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM
imposes an average cost of 270,000 tokens per bug, which, under the current
pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To
the best of our knowledge, this work is the first to present an autonomous,
LLM-based agent for program repair, paving the way for future agent-based
techniques in software engineering.




## Agent Collaboration and Coordination

### Multi-Agent Collaboration
- **[2025-02] Flow-of-Action: SOP Enhanced LLM-Based Multi-Agent System for Root Cause
  Analysis** [[arXiv](http://arxiv.org/abs/2502.08224v1)]
  *Changhua Pei, Zexin Wang, Fengrui Liu, Zeyan Li, Yang Liu, Xiao He, Rong Kang, Tieying Zhang, Jianjun Chen, Jianhui Li, Gaogang Xie, Dan Pei*
  Abstract: In the realm of microservices architecture, the occurrence of frequent
incidents necessitates the employment of Root Cause Analysis (RCA) for swift
issue resolution. It is common that a serious incident can take several domain
experts hours to identify the root cause. Consequently, a contemporary trend
involves harnessing Large Language Models (LLMs) as automated agents for RCA.
Though the recent ReAct framework aligns well with the Site Reliability
Engineers (SREs) for its thought-action-observation paradigm, its
hallucinations often lead to irrelevant actions and directly affect subsequent
results. Additionally, the complex and variable clues of the incident can
overwhelm the model one step further. To confront these challenges, we propose
Flow-of-Action, a pioneering Standard Operation Procedure (SOP) enhanced
LLM-based multi-agent system. By explicitly summarizing the diagnosis steps of
SREs, SOP imposes constraints on LLMs at crucial junctures, guiding the RCA
process towards the correct trajectory. To facilitate the rational and
effective utilization of SOPs, we design an SOP-centric framework called SOP
flow. SOP flow contains a series of tools, including one for finding relevant
SOPs for incidents, another for automatically generating SOPs for incidents
without relevant ones, and a tool for converting SOPs into code. This
significantly alleviates the hallucination issues of ReAct in RCA tasks. We
also design multiple auxiliary agents to assist the main agent by removing
useless noise, narrowing the search space, and informing the main agent whether
the RCA procedure can stop. Compared to the ReAct method's 35.50% accuracy, our
Flow-of-Action method achieves 64.01%, meeting the accuracy requirements for
RCA in real-world systems.

- **[2025-02] SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software
  Engineering** [[arXiv](http://arxiv.org/abs/2502.06994v1)]
  *Xuehang Guo, Xingyao Wang, Yangyi Chen, Sha Li, Chi Han, Manling Li, Heng Ji*
  Abstract: Software engineering (SE) is increasingly collaborative, with developers
working together on shared complex codebases. Effective collaboration in shared
environments requires participants -- whether humans or AI agents -- to stay on
the same page as their environment evolves. When a collaborator's understanding
diverges from the current state -- what we term the out-of-sync challenge --
the collaborator's actions may fail, leading to integration issues. In this
work, we introduce SyncMind, a framework that systematically defines the
out-of-sync problem faced by large language model (LLM) agents in collaborative
software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark
featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE
derived from 21 popular GitHub repositories with executable verification tests.
Experiments on SyncBench uncover critical insights into existing LLM agents'
capabilities and limitations. Besides substantial performance gaps among agents
(from Llama-3.1 agent <= 3.33% to Claude-3.5-Sonnet >= 28.18%), their
consistently low collaboration willingness (<= 4.86%) suggests fundamental
limitations of existing LLM in CSE. However, when collaboration occurs, it
positively correlates with out-of-sync recovery success. Minimal performance
differences in agents' resource-aware out-of-sync recoveries further reveal
their significant lack of resource awareness and adaptability, shedding light
on future resource-efficient collaborative systems. Code and data are openly
available on our project website: https://xhguo7.github.io/SyncMind/.

- **[2025-01] RTLSquad: Multi-Agent Based Interpretable RTL Design** [[arXiv](http://arxiv.org/abs/2501.05470v1)]
  *Bowei Wang, Qi Xiong, Zeqing Xiang, Lei Wang, Renzhi Chen*
  Abstract: Optimizing Register-Transfer Level (RTL) code is crucial for improving
hardware PPA performance. Large Language Models (LLMs) offer new approaches for
automatic RTL code generation and optimization. However, existing methods often
lack decision interpretability (sufficient, understandable justification for
decisions), making it difficult for hardware engineers to trust the generated
results, thus preventing these methods from being integrated into the design
process. To address this, we propose RTLSquad, a novel LLM-Based Multi-Agent
system for interpretable RTL code generation. RTLSquad divides the design
process into exploration, implementation, and verification & evaluation stages
managed by specialized agent squads, generating optimized RTL code through
inter-agent collaboration, and providing decision interpretability through the
communication process. Experiments show that RTLSquad excels in generating
functionally correct RTL code and optimizing PPA performance, while also having
the capability to provide decision paths, demonstrating the practical value of
our system.

- **[2025-01] CodeCoR: An LLM-Based Self-Reflective Multi-Agent Framework for Code
  Generation** [[arXiv](http://arxiv.org/abs/2501.07811v1)]
  *Ruwei Pan, Hongyu Zhang, Chao Liu*
  Abstract: Code generation aims to produce code that fulfills requirements written in
natural languages automatically. Large language Models (LLMs) like ChatGPT have
demonstrated promising effectiveness in this area. Nonetheless, these LLMs
often fail to ensure the syntactic and semantic correctness of the generated
code. Recently, researchers proposed multi-agent frameworks that guide LLMs
with different prompts to analyze programming tasks, generate code, perform
testing in a sequential workflow. However, the performance of the workflow is
not robust as the code generation depends on the performance of each agent. To
address this challenge, we propose CodeCoR, a self-reflective multi-agent
framework that evaluates the effectiveness of each agent and their
collaborations. Specifically, for a given task description, four agents in
CodeCoR generate prompts, code, test cases, and repair advice, respectively.
Each agent generates more than one output and prunes away the low-quality ones.
The generated code is tested in the local environment: the code that fails to
pass the generated test cases is sent to the repair agent and the coding agent
re-generates the code based on repair advice. Finally, the code that passes the
most number of generated test cases is returned to users. Our experiments on
four widely used datasets, HumanEval, HumanEval-ET, MBPP, and MBPP-ET,
demonstrate that CodeCoR significantly outperforms existing baselines (e.g.,
CodeCoT and MapCoder), achieving an average Pass@1 score of 77.8%.

- **[2025-01] VulnBot: Autonomous Penetration Testing for A Multi-Agent Collaborative
  Framework** [[arXiv](http://arxiv.org/abs/2501.13411v1)]
  *He Kong, Die Hu, Jingguo Ge, Liangxiong Li, Tong Li, Bingzhen Wu*
  Abstract: Penetration testing is a vital practice for identifying and mitigating
vulnerabilities in cybersecurity systems, but its manual execution is
labor-intensive and time-consuming. Existing large language model
(LLM)-assisted or automated penetration testing approaches often suffer from
inefficiencies, such as a lack of contextual understanding and excessive,
unstructured data generation. This paper presents VulnBot, an automated
penetration testing framework that leverages LLMs to simulate the collaborative
workflow of human penetration testing teams through a multi-agent system. To
address the inefficiencies and reliance on manual intervention in traditional
penetration testing methods, VulnBot decomposes complex tasks into three
specialized phases: reconnaissance, scanning, and exploitation. These phases
are guided by a penetration task graph (PTG) to ensure logical task execution.
Key design features include role specialization, penetration path planning,
inter-agent communication, and generative penetration behavior. Experimental
results demonstrate that VulnBot outperforms baseline models such as GPT-4 and
Llama3 in automated penetration testing tasks, particularly showcasing its
potential in fully autonomous testing on real-world machines.

- **[2024-12] A Multi-Agent Framework for Extensible Structured Text Generation in
  PLCs** [[arXiv](http://arxiv.org/abs/2412.02410v1)]
  *Donghao Yang, Aolang Wu, Tianyi Zhang, Li Zhang, Fang Liu, Xiaoli Lian, Yuming Ren, Jiaji Tian*
  Abstract: Programmable Logic Controllers (PLCs) are microcomputers essential for
automating factory operations. Structured Text (ST), a high-level language
adhering to the IEC 61131-3 standard, is pivotal for PLCs due to its ability to
express logic succinctly and to seamlessly integrate with other languages
within the same standard. However, vendors develop their own customized
versions of ST, and the lack of comprehensive and standardized documentation
for the full semantics of ST has contributed to inconsistencies in how the
language is implemented. Consequently, the steep learning curve associated with
ST, combined with ever-evolving industrial requirements, presents significant
challenges for developers. In response to these issues, we present AutoPLC, an
LLM-based approach designed to automate the generation of vendor-specific ST
code. To facilitate effective code generation, we first built a comprehensive
knowledge base, including Rq2ST Case Library (requirements and corresponding
implementations) and Instruction libraries. Then we developed a retrieval
module to incorporate the domain-specific knowledge by identifying pertinent
cases and instructions, guiding the LLM to generate code that meets the
requirements. In order to verify and improve the quality of the generated code,
we designed an adaptable code checker. If errors are detected, we initiate an
iterative self-improvement process to instruct the LLM to revise the generated
code. We evaluate AutoPLC's performance against seven state-of-the-art
baselines using three benchmarks, one for open-source basic ST and two for
commercial Structured Control Language (SCL) from Siemens. The results show
that our approach consistently achieves superior performance across all
benchmarks. Ablation study emphasizes the significance of our modules. Further
manual analysis confirm the practical utility of the ST code generated by
AutoPLC.

- **[2024-12] The BrowserGym Ecosystem for Web Agent Research** [[arXiv](http://arxiv.org/abs/2412.05467v3)]
  *Thibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, Léo Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han Lù, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan Salakhutdinov, Nicolas Chapados, Alexandre Lacoste*
  Abstract: The BrowserGym ecosystem addresses the growing need for efficient evaluation
and benchmarking of web agents, particularly those leveraging automation and
Large Language Models (LLMs) for web interaction tasks. Many existing
benchmarks suffer from fragmentation and inconsistent evaluation methodologies,
making it challenging to achieve reliable comparisons and reproducible results.
BrowserGym aims to solve this by providing a unified, gym-like environment with
well-defined observation and action spaces, facilitating standardized
evaluation across diverse benchmarks. Combined with AgentLab, a complementary
framework that aids in agent creation, testing, and analysis, BrowserGym offers
flexibility for integrating new benchmarks while ensuring consistent evaluation
and comprehensive experiment management. This standardized approach seeks to
reduce the time and complexity of developing web agents, supporting more
reliable comparisons and facilitating in-depth analysis of agent behaviors, and
could result in more adaptable, capable agents, ultimately accelerating
innovation in LLM-driven automation. As a supporting evidence, we conduct the
first large-scale, multi-benchmark web agent experiment and compare the
performance of 6 state-of-the-art LLMs across all benchmarks currently
available in BrowserGym. Among other findings, our results highlight a large
discrepancy between OpenAI and Anthropic's latests models, with
Claude-3.5-Sonnet leading the way on almost all benchmarks, except on
vision-related tasks where GPT-4o is superior. Despite these advancements, our
results emphasize that building robust and efficient web agents remains a
significant challenge, due to the inherent complexity of real-world web
environments and the limitations of current models.

- **[2024-11] A Multi-Agent Approach for REST API Testing with Semantic Graphs and
  LLM-Driven Inputs** [[arXiv](http://arxiv.org/abs/2411.07098v2)]
  *Myeongsoo Kim, Tyler Stennett, Saurabh Sinha, Alessandro Orso*
  Abstract: As modern web services increasingly rely on REST APIs, their thorough testing
has become crucial. Furthermore, the advent of REST API documentation
languages, such as the OpenAPI Specification, has led to the emergence of many
black-box REST API testing tools. However, these tools often focus on
individual test elements in isolation (e.g., APIs, parameters, values),
resulting in lower coverage and less effectiveness in fault detection. To
address these limitations, we present AutoRestTest, the first black-box tool to
adopt a dependency-embedded multi-agent approach for REST API testing that
integrates multi-agent reinforcement learning (MARL) with a semantic property
dependency graph (SPDG) and Large Language Models (LLMs). Our approach treats
REST API testing as a separable problem, where four agents -- API, dependency,
parameter, and value agents -- collaborate to optimize API exploration. LLMs
handle domain-specific value generation, the SPDG model simplifies the search
space for dependencies using a similarity score between API operations, and
MARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest
on 12 real-world REST services shows that it outperforms the four leading
black-box REST API testing tools, including those assisted by RESTGPT (which
generates realistic test inputs using LLMs), in terms of code coverage,
operation coverage, and fault detection. Notably, AutoRestTest is the only tool
able to trigger an internal server error in the Spotify service. Our ablation
study illustrates that each component of AutoRestTest -- the SPDG, the LLM, and
the agent-learning mechanism -- contributes to its overall effectiveness.

- **[2024/10] Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback.** [[Link](https://openreview.net/pdf?id=st7XqFgbAH)]
  *Sanjiban Choudhury, Paloma Sodhi*
While large language models (LLMs) show impressive decision-making abilities, current methods lack a mechanism for automatic self-improvement from errors during task execution. We propose LEAP, an iterative fine-tuning framework that continually improves LLM agents using feedback from AI expert teachers. Our key insight is to equip the expert teachers with a privileged state -- information that is available during training but hidden at test time. This allows even weak experts to provide precise guidance, significantly improving the student agent's performance without access to privileged information at test time. We evaluate LEAP on diverse decision-making benchmarks, including text-based games (ALFWorld), web navigation (WebShop), and interactive coding (Intercode Bash). Our experiments show that LEAP (1) outperforms behavior cloning and ReAct baselines (2) enables weak student models (e.g., Llama3-8B) to exceed the performance of strong teacher models (GPT4-o), and (3) allows weak models to self-improve using privileged versions of themselves. We also provide a theoretical analysis showing that LEAP's success hinges on balancing privileged information with the student's realizability, which we empirically validate. Our code is available at this https URL

- **[2024/10] Self-Evolving Multi-Agent Collaboration Networks for Software Development** [[Link](https://openreview.net/pdf?id=4R71pdPBZp)]
  *Yue Hu, Yuzhu Cai, Yaxin Du, Xinyu Zhu, Xiangrui Liu, Zijie Yu, Yuchen Hou, Shuo Tang, Siheng Chen*
LLM-driven multi-agent collaboration (MAC) systems have demonstrated impressive capabilities in automatic software development at the function level. However, their heavy reliance on human design limits their adaptability to the diverse demands of real-world software development. To address this limitation, we introduce EvoMAC, a novel self-evolving paradigm for MAC networks. Inspired by traditional neural network training, EvoMAC obtains text-based environmental feedback by verifying the MAC network's output against a target proxy and leverages a novel textual backpropagation to update the network. To extend coding capabilities beyond function-level tasks to more challenging software-level development, we further propose rSDE-Bench, a requirement-oriented software development benchmark, which features complex and diverse software requirements along with automatic evaluation of requirement correctness. Our experiments show that: i) The automatic requirement-aware evaluation in rSDE-Bench closely aligns with human evaluations, validating its reliability as a software-level coding benchmark. ii) EvoMAC outperforms previous SOTA methods on both the software-level rSDE-Bench and the function-level HumanEval benchmarks, reflecting its superior coding capabilities. The benchmark can be downloaded at this https URL.

- **[2024-10] VisionCoder: Empowering Multi-Agent Auto-Programming for Image
  Processing with Hybrid LLMs** [[arXiv](http://arxiv.org/abs/2410.19245v1)]
  *Zixiao Zhao, Jing Sun, Zhiyuan Wei, Cheng-Hao Cai, Zhe Hou, Jin Song Dong*
  Abstract: In the field of automated programming, large language models (LLMs) have
demonstrated foundational generative capabilities when given detailed task
descriptions. However, their current functionalities are primarily limited to
function-level development, restricting their effectiveness in complex project
environments and specific application scenarios, such as complicated
image-processing tasks. This paper presents a multi-agent framework that
utilises a hybrid set of LLMs, including GPT-4o and locally deployed
open-source models, which collaboratively complete auto-programming tasks. Each
agent plays a distinct role in the software development cycle, collectively
forming a virtual organisation that works together to produce software
products. By establishing a tree-structured thought distribution and
development mechanism across project, module, and function levels, this
framework offers a cost-effective and efficient solution for code generation.
We evaluated our approach using benchmark datasets, and the experimental
results demonstrate that VisionCoder significantly outperforms existing methods
in image processing auto-programming tasks.

- **[2024-10] Improving Performance of Commercially Available AI Products in a
  Multi-Agent Configuration** [[arXiv](http://arxiv.org/abs/2410.22129v1)]
  *Cory Hymel, Sida Peng, Kevin Xu, Charath Ranganathan*
  Abstract: In recent years, with the rapid advancement of large language models (LLMs),
multi-agent systems have become increasingly more capable of practical
application. At the same time, the software development industry has had a
number of new AI-powered tools developed that improve the software development
lifecycle (SDLC). Academically, much attention has been paid to the role of
multi-agent systems to the SDLC. And, while single-agent systems have
frequently been examined in real-world applications, we have seen comparatively
few real-world examples of publicly available commercial tools working together
in a multi-agent system with measurable improvements. In this experiment we
test context sharing between Crowdbotics PRD AI, a tool for generating software
requirements using AI, and GitHub Copilot, an AI pair-programming tool. By
sharing business requirements from PRD AI, we improve the code suggestion
capabilities of GitHub Copilot by 13.8% and developer task success rate by
24.5% -- demonstrating a real-world example of commercially-available AI
systems working together with improved outcomes.

- **[2024-09] HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks
  at Scale** [[arXiv](http://arxiv.org/abs/2409.16299v2)]
  *Huy Nhat Phan, Tien N. Nguyen, Phong X. Nguyen, Nghi D. Q. Bui*
  Abstract: Large Language Models (LLMs) have revolutionized software engineering (SE),
showcasing remarkable proficiency in various coding tasks. Despite recent
advancements that have enabled the creation of autonomous software agents
utilizing LLMs for end-to-end development tasks, these systems are typically
designed for specific SE functions. We introduce HyperAgent, an innovative
generalist multi-agent system designed to tackle a wide range of SE tasks
across different programming languages by mimicking the workflows of human
developers. HyperAgent features four specialized agents-Planner, Navigator,
Code Editor, and Executor-capable of handling the entire lifecycle of SE tasks,
from initial planning to final verification. HyperAgent sets new benchmarks in
diverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench
benchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates
exceptional performance in repository-level code generation (RepoExec) and
fault localization and program repair (Defects4J), often surpassing
state-of-the-art baselines.

- **[2024-09] Think-on-Process: Dynamic Process Generation for Collaborative
  Development of Multi-Agent System** [[arXiv](http://arxiv.org/abs/2409.06568v1)]
  *Leilei Lin, Yingming Zhou, Wenlong Chen, Chen Qian*
  Abstract: Software development is a collaborative endeavor that requires individuals
from different departments to work together in order to collectively develop a
high-quality software system. In this context, people have begun to explore a
method that leverages multi-agent systems based on LLMs to carry out software
development. However, existing research tends to rigidly fix the software
development process in a framework in code form, thus failing to dynamically
adjust the software development process in real-time to meet the more flexible
and variable software environment. In this paper, we propose a dynamic process
generation framework, named ToP (Think-on-Process). The core idea of ToP is to
leverage experiential knowledge (i.e., process models) to guide LLMs in
generating software development processes (i.e., instances). These instances
will guide multi-agent in software development and employ a compiler to provide
feedback on the development outcomes. Subsequently, we utilize heuristic
algorithms to filter the instances and apply process mining algorithms to
derive process model. Finally, the process model will be converted into text,
formatted as prompts, to enhance the ability of LLMs to generate other
instances. Experiments demonstrate that our framework ToP significantly
enhances the dynamic process generation capability of the GPT-3.5 and GPT-4 for
five categories of software development tasks.

- **[2024/08] ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems** [[Link](https://aclanthology.org/2024.emnlp-demo.17/)]
  *Andrew Zhu, Liam Dugan, Chris Callison-Burch*
Recently, there has been increasing interest in using Large Language Models (LLMs) to construct complex multi-agent systems to perform tasks such as compiling literature reviews, drafting consumer reports, and planning vacations. Many tools and libraries exist for helping create such systems, however none support recursive multi-agent systems -- where the models themselves flexibly decide when to delegate tasks and how to organize their delegation structure. In this work, we introduce ReDel: a toolkit for recursive multi-agent systems that supports custom tool-use, delegation schemes, event-based logging, and interactive replay in an easy-to-use web interface. We show that, using ReDel, we are able to easily identify potential areas of improvements through the visualization and debugging tools. Our code, documentation, and PyPI package are open-source and free to use under the MIT license at this https URL.

- **[2024-08] DreamFactory: Pioneering Multi-Scene Long Video Generation with a
  Multi-Agent Framework** [[arXiv](http://arxiv.org/abs/2408.11788v1)]
  *Zhifei Xie, Daniel Tang, Dingwei Tan, Jacques Klein, Tegawend F. Bissyand, Saad Ezzini*
  Abstract: Current video generation models excel at creating short, realistic clips, but
struggle with longer, multi-scene videos. We introduce \texttt{DreamFactory},
an LLM-based framework that tackles this challenge. \texttt{DreamFactory}
leverages multi-agent collaboration principles and a Key Frames Iteration
Design Method to ensure consistency and style across long videos. It utilizes
Chain of Thought (COT) to address uncertainties inherent in large language
models. \texttt{DreamFactory} generates long, stylistically coherent, and
complex videos. Evaluating these long-form videos presents a challenge. We
propose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene
Style Consistency Score. To further research in this area, we contribute the
Multi-Scene Videos Dataset containing over 150 human-rated videos.

- **[2024-08] GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility:
  Opportunities and Challenges for Integrating Large Language Models (LLMs) and
  Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems** [[arXiv](http://arxiv.org/abs/2409.00494v2)]
  *Haowen Xu, Jinghui Yuan, Anye Zhou, Guanhao Xu, Wan Li, Xuegang Ban, Xinyue Ye*
  Abstract: Leveraging recent advances in generative AI, multi-agent systems are
increasingly being developed to enhance the functionality and efficiency of
smart city applications. This paper explores the transformative potential of
large language models (LLMs) and emerging Retrieval-Augmented Generation (RAG)
technologies in Intelligent Transportation Systems (ITS), paving the way for
innovative solutions to address critical challenges in urban mobility. We begin
by providing a comprehensive overview of the current state-of-the-art in
mobility data, ITS, and Connected Vehicles (CV) applications. Building on this
review, we discuss the rationale behind RAG and examine the opportunities for
integrating these Generative AI (GenAI) technologies into the smart mobility
sector. We propose a conceptual framework aimed at developing multi-agent
systems capable of intelligently and conversationally delivering smart mobility
services to urban commuters, transportation operators, and decision-makers. Our
approach seeks to foster an autonomous and intelligent approach that (a)
promotes science-based advisory to reduce traffic congestion, accidents, and
carbon emissions at multiple scales, (b) facilitates public education and
engagement in participatory mobility management, and (c) automates specialized
transportation management tasks and the development of critical ITS platforms,
such as data analytics and interpretation, knowledge representation, and
traffic simulations. By integrating LLM and RAG, our approach seeks to overcome
the limitations of traditional rule-based multi-agent systems, which rely on
fixed knowledge bases and limited reasoning capabilities. This integration
paves the way for a more scalable, intuitive, and automated multi-agent
paradigm, driving advancements in ITS and urban mobility.

- **[2024/06] Scaling Large-Language-Model-based Multi-Agent Collaboration.** [[Link](https://openreview.net/pdf?id=K3n5jPkrU6)]
  *Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun*
Pioneering advancements in large language model-powered agents have underscored the design pattern of multi-agent collaboration, demonstrating that collective intelligence can surpass the capabilities of each individual. Inspired by the neural scaling law, which posits that increasing neurons leads to emergent abilities, this study investigates whether a similar principle applies to increasing agents in multi-agent collaboration. Technically, we propose multi-agent collaboration networks (MacNet), which utilize directed acyclic graphs to organize agents and streamline their interactive reasoning via topological ordering, with solutions derived from their dialogues. Extensive experiments show that MacNet consistently outperforms baseline models, enabling effective agent collaboration across various network topologies and supporting cooperation among more than a thousand agents. Notably, we observed a small-world collaboration phenomenon, where topologies resembling small-world properties achieved superior performance. Additionally, we identified a collaborative scaling law, indicating that normalized solution quality follows a logistic growth pattern as scaling agents, with collaborative emergence occurring much earlier than previously observed instances of neural emergence. The code and data will be available at this https URL.

- **[2024-06] Experimenting with Multi-Agent Software Development: Towards a Unified
  Platform** [[arXiv](https://arxiv.org/pdf/2406.05381)]
  *Malik Abdul Sami, Muhammad Waseem, Zeeshan Rasheed, Mika Saari, Kari Systä, Pekka Abrahamsson*
  Abstract: Large language models are redefining software engineering by implementing
AI-powered techniques throughout the whole software development process,
including requirement gathering, software architecture, code generation,
testing, and deployment. However, it is still difficult to develop a cohesive
platform that consistently produces the best outcomes across all stages. The
objective of this study is to develop a unified platform that utilizes multiple
artificial intelligence agents to automate the process of transforming user
requirements into well-organized deliverables. These deliverables include user
stories, prioritization, and UML sequence diagrams, along with the modular
approach to APIs, unit tests, and end-to-end tests. Additionally, the platform
will organize tasks, perform security and compliance, and suggest design
patterns and improvements for non-functional requirements. We allow users to
control and manage each phase according to their preferences. In addition, the
platform provides security and compliance checks following European standards
and proposes design optimizations. We use multiple models, such as GPT-3.5,
GPT-4, and Llama3 to enable to generation of modular code as per user choice.
The research also highlights the limitations and future research discussions to
overall improve the software development life cycle. The source code for our
uniform platform is hosted on GitHub, enabling additional experimentation and
supporting both research and practical uses. \end

- **[2024-06] Multi-Agent Software Development through Cross-Team Collaboration** [[arXiv](http://arxiv.org/abs/2406.08979v1)]
  *Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, Cheng Yang*
  Abstract: The latest breakthroughs in Large Language Models (LLMs), eg., ChatDev, have
catalyzed profound transformations, particularly through multi-agent
collaboration for software development. LLM agents can collaborate in teams
like humans, and follow the waterfall model to sequentially work on
requirements analysis, development, review, testing, and other phases to
perform autonomous software generation. However, for an agent team, each phase
in a single development process yields only one possible outcome. This results
in the completion of only one development chain, thereby losing the opportunity
to explore multiple potential decision paths within the solution space.
Consequently, this may lead to obtaining suboptimal results. To address this
challenge, we introduce Cross-Team Collaboration (CTC), a scalable multi-team
framework that enables orchestrated teams to jointly propose various decisions
and communicate with their insights in a cross-team collaboration environment
for superior content generation. Experimental results in software development
reveal a notable increase in quality compared to state-of-the-art baselines,
underscoring the efficacy of our framework. The significant improvements in
story generation demonstrate the promising generalization ability of our
framework across various domains. We anticipate that our work will guide LLM
agents towards a cross-team paradigm and contribute to their significant growth
in but not limited to software development. The code and data will be available
at https://github.com/OpenBMB/ChatDev.

- **[2024-05] Iterative Experience Refinement of Software-Developing Agents** [[arXiv](http://arxiv.org/abs/2405.04219v1)]
  *Chen Qian, Jiahao Li, Yufan Dang, Wei Liu, YiFei Wang, Zihao Xie, Weize Chen, Cheng Yang, Yingli Zhang, Zhiyuan Liu, Maosong Sun*
  Abstract: Autonomous agents powered by large language models (LLMs) show significant
potential for achieving high autonomy in various scenarios such as software
development. Recent research has shown that LLM agents can leverage past
experiences to reduce errors and enhance efficiency. However, the static
experience paradigm, reliant on a fixed collection of past experiences acquired
heuristically, lacks iterative refinement and thus hampers agents'
adaptability. In this paper, we introduce the Iterative Experience Refinement
framework, enabling LLM agents to refine experiences iteratively during task
execution. We propose two fundamental patterns: the successive pattern,
refining based on nearest experiences within a task batch, and the cumulative
pattern, acquiring experiences across all previous task batches. Augmented with
our heuristic experience elimination, the method prioritizes high-quality and
frequently-used experiences, effectively managing the experience space and
enhancing efficiency. Extensive experiments show that while the successive
pattern may yield superior results, the cumulative pattern provides more stable
performance. Moreover, experience elimination facilitates achieving better
performance using just 11.54% of a high-quality subset.

- **[2024/03] MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution** [[Link](https://openreview.net/pdf?id=qevq3FZ63J)]
  *Wei Tao, Yucheng Zhou, Yanlin Wang, Wenqiang Zhang, Hongyu Zhang, Yu Cheng*
In software development, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing code. Large Language Models (LLMs) have shown promise in code generation but face difficulties in resolving Github issues, particularly at the repository level. To overcome this challenge, we empirically study the reason why LLMs fail to resolve GitHub issues and analyze the major factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four agents customized for software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, significantly outperforming the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the advanced LLM.

- **[2024/02] CodeAgent: Collaborative Agents for Software Engineering** [[Link](https://aclanthology.org/2024.emnlp-main.632.pdf)]
  *Xunzhu Tang, Kisub Kim, Yewei Song, Cedric Lothritz, Bei Li, Saad Ezzini, Haoye Tian, Jacques Klein, Tegawendé F. Bissyandé*
 Code review is a heavily collaborative process, which aims at ensuring the overall quality and reliability of software. While it provides massive benefits, the implementation of code review in an organization faces several challenges that make its automation appealing. Automated code review tools have been around for a while and are now improving thanks to the adoption of novel AI models, which help can learn about standard practices and systematically check that the reviewed code adheres to them. Unfortunately, existing methods fall short: they often target a single input-output generative model, which cannot simulate the collaboration interactions in code review to account for various perspectives; they are also sub-performing on various critical code review sub-tasks. In this paper, we advance the state of the art in code review automation by introducing CodeAgent, a novel multi-agent-based system for code review. Fundamentally, CodeAgent is steered by QA-Checker (short for "Question-Answer Checking"), a supervision agent, designed specifically to ensure that all agents' contributions remain relevant to the initial review question. CodeAgent is autonomous, multi-agent, and Large language model-driven. To demonstrate the effectiveness of CodeAgent, we performed experiments to assess its capabilities in various tasks including 1) detection of inconsistencies between code changes and commit messages, 2) detection of vulnerability introduction by commits, and 3) validation of adherence to code style. Our website is accessed in \url{https://code-agent-new.vercel.app/index.html}.

- **[2024-02] Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted
  Approach for Qualitative Data Analysis** [[arXiv](http://arxiv.org/abs/2402.01386v1)]
  *Zeeshan Rasheed, Muhammad Waseem, Aakash Ahmad, Kai-Kristian Kemell, Wang Xiaofeng, Anh Nguyen Duc, Pekka Abrahamsson*
  Abstract: Recent advancements in Large Language Models (LLMs) have enabled
collaborative human-bot interactions in Software Engineering (SE), similar to
many other professions. However, the potential benefits and implications of
incorporating LLMs into qualitative data analysis in SE have not been
completely explored. For instance, conducting qualitative data analysis
manually can be a time-consuming, effort-intensive, and error-prone task for
researchers. LLM-based solutions, such as generative AI models trained on
massive datasets, can be utilized to automate tasks in software development as
well as in qualitative data analysis. To this end, we utilized LLMs to automate
and expedite the qualitative data analysis processes. We employed a multi-agent
model, where each agent was tasked with executing distinct, individual research
related activities. Our proposed model interpreted large quantities of textual
documents and interview transcripts to perform several common tasks used in
qualitative analysis. The results show that this technical assistant speeds up
significantly the data analysis process, enabling researchers to manage larger
datasets much more effectively. Furthermore, this approach introduces a new
dimension of scalability and accuracy in qualitative research, potentially
transforming data interpretation methodologies in SE.

- **[2023-12] AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and
  Optimisation** [[arXiv](https://arxiv.org/pdf/2312.13010)]
  *Dong Huang, Jie M. Zhang, Michael Luck, Qingwen Bu, Yuhao Qing, Heming Cui*
  Abstract: The advancement of natural language processing (NLP) has been significantly
boosted by the development of transformer-based large language models (LLMs).
These models have revolutionized NLP tasks, particularly in code generation,
aiding developers in creating software with enhanced efficiency. Despite their
advancements, challenges in balancing code snippet generation with effective
test case generation and execution persist. To address these issues, this paper
introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution
comprising a multi-agent framework with specialized agents: the programmer
agent, the test designer agent, and the test executor agent. During the coding
procedure, the programmer agent will focus on the code generation and
refinement based on the test executor agent's feedback. The test designer agent
will generate test cases for the generated code, and the test executor agent
will run the code with the test cases and write the feedback to the programmer.
This collaborative system ensures robust code generation, surpassing the
limitations of single-agent models and traditional methodologies. Our extensive
experiments on 9 code generation models and 12 enhancement approaches showcase
AgentCoder's superior performance over existing code generation models and
prompt engineering techniques across various benchmarks. For example,
AgentCoder (GPT-4) achieves 96.3\% and 91.8\% pass@1 in HumanEval and MBPP
datasets with an overall token overhead of 56.9K and 66.3K, while
state-of-the-art obtains only 90.2\% and 78.9\% pass@1 with an overall token
overhead of 138.2K and 206.5K.

- **[2023/12] Experiential Co-Learning of Software-Developing Agents** [[Link](https://aclanthology.org/2024.acl-long.305.pdf)]
  *Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Zihao Xie, Yifei Wang, Weize Chen, Cheng Yang, Xin Cong, Xiaoyin Che, Zhiyuan Liu, Maosong Sun*
Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. A representative scenario is in software development, where LLM agents demonstrate efficient collaboration, task division, and assurance of software quality, markedly reducing the need for manual involvement. However, these agents frequently perform a variety of tasks independently, without benefiting from past experiences, which leads to repeated mistakes and inefficient attempts in multi-step task execution. To this end, we introduce Experiential Co-Learning, a novel LLM-agent learning framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for future task execution. The extensive experiments demonstrate that the framework enables agents to tackle unseen software-developing tasks more effectively. We anticipate that our insights will guide LLM agents towards enhanced autonomy and contribute to their evolutionary growth in cooperative learning. The code and data are available at this https URL.

- **[2023/10] Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models.** [[Link](https://dl.acm.org/doi/10.5555/3692070.3694642)]
  *Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang*
While language models (LMs) have shown potential across a range of decision-making tasks, their reliance on simple acting processes limits their broad deployment as autonomous agents. In this paper, we introduce Language Agent Tree Search (LATS) -- the first general framework that synergizes the capabilities of LMs in reasoning, acting, and planning. By leveraging the in-context learning ability of LMs, we integrate Monte Carlo Tree Search into LATS to enable LMs as agents, along with LM-powered value functions and self-reflections for proficient exploration and enhanced decision-making. A key feature of our approach is the incorporation of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that surpasses the constraints of existing techniques. Our experimental evaluation across diverse domains, including programming, interactive question-answering (QA), web navigation, and math, validates the effectiveness and generality of LATS in decision-making while maintaining competitive or improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1 accuracy (92.7%) for programming on HumanEval with GPT-4 and demonstrates gradient-free performance (average score of 75.9) comparable to gradient-based fine-tuning for web navigation on WebShop with GPT-3.5. Code can be found at this https URL

- **[2023-10] A Dynamic LLM-Powered Agent Network for Task-Oriented Agent
  Collaboration** [[arXiv](https://arxiv.org/pdf/2310.02170)]
  *Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang*
  Abstract: Recent studies show that collaborating multiple large language model (LLM)
powered agents is a promising way for task solving. However, current approaches
are constrained by using a fixed number of agents and static communication
structures. In this work, we propose automatically selecting a team of agents
from candidates to collaborate in a dynamic communication structure toward
different tasks and domains. Specifically, we build a framework named Dynamic
LLM-Powered Agent Network ($\textbf{DyLAN}$) for LLM-powered agent
collaboration, operating a two-stage paradigm: (1) Team Optimization and (2)
Task Solving. During the first stage, we utilize an $\textit{agent selection}$
algorithm, based on an unsupervised metric called $\textit{Agent Importance
Score}$, enabling the selection of best agents according to their contributions
in a preliminary trial, oriented to the given task. Then, in the second stage,
the selected agents collaborate dynamically according to the query.
Empirically, we demonstrate that DyLAN outperforms strong baselines in code
generation, decision-making, general reasoning, and arithmetic reasoning tasks
with moderate computational cost. On specific subjects in MMLU, selecting a
team of agents in the team optimization stage improves accuracy by up to 25.0%
in DyLAN.

- **[2023/08] AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors** [[Link](https://openreview.net/pdf?id=EHg5GDnyq1)]
  *Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou*
Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \framework will soon be released at \url{this https URL}.

- **[2023-06] Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM
  Agents** [[arXiv](https://arxiv.org/pdf/2306.03314)]
  *Yashar Talebirad, Amirhossein Nadiri*
  Abstract: In this paper, we present a novel framework for enhancing the capabilities of
large language models (LLMs) by leveraging the power of multi-agent systems.
Our framework introduces a collaborative environment where multiple intelligent
agent components, each with distinctive attributes and roles, work together to
handle complex tasks more efficiently and effectively. We demonstrate the
practicality and versatility of our framework through case studies in
artificial general intelligence (AGI), specifically focusing on the Auto-GPT
and BabyAGI models. We also examine the "Gorilla" model, which integrates
external APIs into the LLM. Our framework addresses limitations and challenges
such as looping issues, security risks, scalability, system evaluation, and
ethical considerations. By modeling various domains such as courtroom
simulations and software development scenarios, we showcase the potential
applications and benefits of our proposed multi-agent system. Our framework
provides an avenue for advancing the capabilities and performance of LLMs
through collaboration and knowledge exchange among intelligent agents.




### Autonomous Multi-Agent Systems
- **[2025-01] Autonomous Legacy Web Application Upgrades Using a Multi-Agent System** [[arXiv](http://arxiv.org/abs/2501.19204v1)]
  *Valtteri Ala-Salmi, Zeeshan Rasheed, Abdul Malik Sami, Zheying Zhang, Kai-Kristian Kemell, Jussi Rasku, Shahbaz Siddeeq, Mika Saari, Pekka Abrahamsson*
  Abstract: The use of Large Language Models (LLMs) for autonomous code generation is
gaining attention in emerging technologies. As LLM capabilities expand, they
offer new possibilities such as code refactoring, security enhancements, and
legacy application upgrades. Many outdated web applications pose security and
reliability challenges, yet companies continue using them due to the complexity
and cost of upgrades. To address this, we propose an LLM-based multi-agent
system that autonomously upgrades legacy web applications to the latest
versions. The system distributes tasks across multiple phases, updating all
relevant files. To evaluate its effectiveness, we employed Zero-Shot Learning
(ZSL) and One-Shot Learning (OSL) prompts, applying identical instructions in
both cases. The evaluation involved updating view files and measuring the
number and types of errors in the output. For complex tasks, we counted the
successfully met requirements. The experiments compared the proposed system
with standalone LLM execution, repeated multiple times to account for
stochastic behavior. Results indicate that our system maintains context across
tasks and agents, improving solution quality over the base model in some cases.
This study provides a foundation for future model implementations in legacy
code updates. Additionally, findings highlight LLMs' ability to update small
outdated files with high precision, even with basic prompts. The source code is
publicly available on GitHub: https://github.com/alasalm1/Multi-agent-pipeline.

- **[2025-01] Enabling Autonomic Microservice Management through Self-Learning Agents** [[arXiv](http://arxiv.org/abs/2501.19056v1)]
  *Fenglin Yu, Fangkai Yang, Xiaoting Qin, Zhiyang Zhang, Jue Zhang, Qingwei Lin, Hongyu Zhang, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang, Qi Zhang*
  Abstract: The increasing complexity of modern software systems necessitates robust
autonomic self-management capabilities. While Large Language Models (LLMs)
demonstrate potential in this domain, they often face challenges in adapting
their general knowledge to specific service contexts. To address this
limitation, we propose ServiceOdyssey, a self-learning agent system that
autonomously manages microservices without requiring prior knowledge of
service-specific configurations. By leveraging curriculum learning principles
and iterative exploration, ServiceOdyssey progressively develops a deep
understanding of operational environments, reducing dependence on human input
or static documentation. A prototype built with the Sock Shop microservice
demonstrates the potential of this approach for autonomic microservice
management.

- **[2025-01] Simulation Streams: A Programming Paradigm for Controlling Large
  Language Models and Building Complex Systems with Generative AI** [[arXiv](http://arxiv.org/abs/2501.18668v1)]
  *Peter Sunehag, Joel Z. Leibo*
  Abstract: We introduce Simulation Streams, a programming paradigm designed to
efficiently control and leverage Large Language Models (LLMs) for complex,
dynamic simulations and agentic workflows. Our primary goal is to create a
minimally interfering framework that harnesses the agentic abilities of LLMs
while addressing their limitations in maintaining consistency, selectively
ignoring/including information, and enforcing strict world rules. Simulation
Streams achieves this through a state-based approach where variables are
modified in sequential steps by "operators," producing output on a recurring
format and adhering to consistent rules for state variables. This approach
focus the LLMs on defined tasks, while aiming to have the context stream remain
"in-distribution". The approach incorporates an Entity-Component-System (ECS)
architecture to write programs in a more intuitive manner, facilitating reuse
of workflows across different components and entities. This ECS approach
enhances the modularity of the output stream, allowing for complex,
multi-entity simulations while maintaining format consistency, information
control, and rule enforcement. It is supported by a custom editor that aids in
creating, running, and analyzing simulations. We demonstrate the versatility of
simulation streams through an illustrative example of an ongoing market economy
simulation, a social simulation of three characters playing a game of catch in
a park and a suite of classical reinforcement learning benchmark tasks. These
examples showcase Simulation Streams' ability to handle complex, evolving
scenarios over 100s-1000s of iterations, facilitate comparisons between
different agent workflows and models, and maintain consistency and continued
interesting developments in LLM-driven simulations.

- **[2025-01] AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling
  Autonomous Clouds** [[arXiv](http://arxiv.org/abs/2501.06706v1)]
  *Yinfang Chen, Manish Shetty, Gagan Somashekar, Minghua Ma, Yogesh Simmhan, Jonathan Mace, Chetan Bansal, Rujia Wang, Saravan Rajmohan*
  Abstract: AI for IT Operations (AIOps) aims to automate complex operational tasks, such
as fault localization and root cause analysis, to reduce human workload and
minimize customer impact. While traditional DevOps tools and AIOps algorithms
often focus on addressing isolated operational tasks, recent advances in Large
Language Models (LLMs) and AI agents are revolutionizing AIOps by enabling
end-to-end and multitask automation. This paper envisions a future where AI
agents autonomously manage operational tasks throughout the entire incident
lifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps.
Realizing this vision requires a comprehensive framework to guide the design,
development, and evaluation of these agents. To this end, we present AIOPSLAB,
a framework that not only deploys microservice cloud environments, injects
faults, generates workloads, and exports telemetry data but also orchestrates
these components and provides interfaces for interacting with and evaluating
agents. We discuss the key requirements for such a holistic framework and
demonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps
agents. Through evaluations of state-of-the-art LLM agents within the benchmark
created by AIOPSLAB, we provide insights into their capabilities and
limitations in handling complex operational tasks in cloud environments.

- **[2025-01] Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps** [[arXiv](http://arxiv.org/abs/2501.08243v1)]
  *Kannan Parthasarathy, Karthik Vaidhyanathan, Rudra Dhar, Venkat Krishnamachari, Basil Muhammed, Adyansh Kakran, Sreemaee Akshathala, Shrikara Arun, Sumant Dubey, Mohan Veerubhotla, Amey Karan*
  Abstract: Cloud Operations (CloudOps) is a rapidly growing field focused on the
automated management and optimization of cloud infrastructure which is
essential for organizations navigating increasingly complex cloud environments.
MontyCloud Inc. is one of the major companies in the CloudOps domain that
leverages autonomous bots to manage cloud compliance, security, and continuous
operations. To make the platform more accessible and effective to the
customers, we leveraged the use of GenAI.
  Developing a GenAI-based solution for autonomous CloudOps for the existing
MontyCloud system presented us with various challenges such as i) diverse data
sources; ii) orchestration of multiple processes; and iii) handling complex
workflows to automate routine tasks. To this end, we developed MOYA, a
multi-agent framework that leverages GenAI and balances autonomy with the
necessary human control. This framework integrates various internal and
external systems and is optimized for factors like task orchestration,
security, and error mitigation while producing accurate, reliable, and relevant
insights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our
multi-agent system with the help of practitioners as well as using automated
checks demonstrate enhanced accuracy, responsiveness, and effectiveness over
non-agentic approaches across complex workflows.

- **[2024-12] SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to
  Question Answering** [[arXiv](http://arxiv.org/abs/2412.06832v1)]
  *Michael Iannelli, Sneha Kuchipudi, Vera Dvorak*
  Abstract: Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to
generalize to new information by decoupling reasoning capabilities from static
knowledge bases. Traditional RAG enhancements have explored vertical scaling --
assigning subtasks to specialized modules -- and horizontal scaling --
replicating tasks across multiple agents -- to improve performance. However,
real-world applications impose diverse Service Level Agreements (SLAs) and
Quality of Service (QoS) requirements, involving trade-offs among objectives
such as reducing cost, ensuring answer quality, and adhering to specific
operational constraints.
  In this work, we present a systems-oriented approach to multi-agent RAG
tailored for real-world Question Answering (QA) applications. By integrating
task-specific non-functional requirements -- such as answer quality, cost, and
latency -- into the system, we enable dynamic reconfiguration to meet diverse
SLAs. Our method maps these Service Level Objectives (SLOs) to system-level
parameters, allowing the generation of optimal results within specified
resource constraints.
  We conduct a case study in the QA domain, demonstrating how dynamic
re-orchestration of a multi-agent RAG system can effectively manage the
trade-off between answer quality and cost. By adjusting the system based on
query intent and operational conditions, we systematically balance performance
and resource utilization. This approach allows the system to meet SLOs for
various query types, showcasing its practicality for real-world applications.

- **[2024-12] Creating an LLM-based AI-agent: A high-level methodology towards
  enhancing LLMs with APIs** [[arXiv](http://arxiv.org/abs/2412.13233v2)]
  *Ioannis Tzachristas*
  Abstract: Large Language Models (LLMs) have revolutionized various aspects of
engineering and science. Their utility is often bottlenecked by the lack of
interaction with the external digital environment. To overcome this limitation
and achieve integration of LLMs and Artificial Intelligence (AI) into
real-world applications, customized AI agents are being constructed. Based on
the technological trends and techniques, we extract a high-level approach for
constructing these AI agents, focusing on their underlying architecture. This
thesis serves as a comprehensive guide that elucidates a multi-faceted approach
for empowering LLMs with the capability to leverage Application Programming
Interfaces (APIs). We present a 7-step methodology that begins with the
selection of suitable LLMs and the task decomposition that is necessary for
complex problem-solving. This methodology includes techniques for generating
training data for API interactions and heuristics for selecting the appropriate
API among a plethora of options. These steps eventually lead to the generation
of API calls that are both syntactically and semantically aligned with the
LLM's understanding of a given task. Moreover, we review existing frameworks
and tools that facilitate these processes and highlight the gaps in current
attempts. In this direction, we propose an on-device architecture that aims to
exploit the functionality of carry-on devices by using small models from the
Hugging Face community. We examine the effectiveness of these approaches on
real-world applications of various domains, including the generation of a piano
sheet. Through an extensive analysis of the literature and available
technologies, this thesis aims to set a compass for researchers and
practitioners to harness the full potential of LLMs augmented with external
tool capabilities, thus paving the way for more autonomous, robust, and
context-aware AI agents.

- **[2024/10] AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories.** [[Link](https://aclanthology.org/2024.findings-emnlp.116.pdf)]
  *Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng, Sujian Li*
Fine-tuning on agent-environment interaction trajectory data holds significant promise for surfacing generalized agent capabilities in open-source large language models (LLMs). In this work, we introduce AgentBank, by far the largest trajectory tuning data collection featuring more than 50k diverse high-quality interaction trajectories which comprises 16 tasks covering five distinct agent skill dimensions. Leveraging a novel annotation pipeline, we are able to scale the annotated trajectories and generate a trajectory dataset with minimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a series of agent models, Samoyed. Our comparative experiments demonstrate the effectiveness of scaling the interaction trajectory data to acquire generalized agent capabilities. Additional studies also reveal some key observations regarding trajectory tuning and agent skill generalization.

- **[2024/10] Self-Evolving Multi-Agent Collaboration Networks for Software Development.** [[Link](https://openreview.net/pdf?id=4R71pdPBZp)]
  *Yue Hu, Yuzhu Cai, Yaxin Du, Xinyu Zhu, Xiangrui Liu, Zijie Yu, Yuchen Hou, Shuo Tang, Siheng Chen*
LLM-driven multi-agent collaboration (MAC) systems have demonstrated impressive capabilities in automatic software development at the function level. However, their heavy reliance on human design limits their adaptability to the diverse demands of real-world software development. To address this limitation, we introduce EvoMAC, a novel self-evolving paradigm for MAC networks. Inspired by traditional neural network training, EvoMAC obtains text-based environmental feedback by verifying the MAC network's output against a target proxy and leverages a novel textual backpropagation to update the network. To extend coding capabilities beyond function-level tasks to more challenging software-level development, we further propose rSDE-Bench, a requirement-oriented software development benchmark, which features complex and diverse software requirements along with automatic evaluation of requirement correctness. Our experiments show that: i) The automatic requirement-aware evaluation in rSDE-Bench closely aligns with human evaluations, validating its reliability as a software-level coding benchmark. ii) EvoMAC outperforms previous SOTA methods on both the software-level rSDE-Bench and the function-level HumanEval benchmarks, reflecting its superior coding capabilities. The benchmark can be downloaded at this https URL.

- **[2024-09] SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as
  Autonomous Programmer** [[arXiv](http://arxiv.org/abs/2409.11190v2)]
  *Anmol Gautam, Kishore Kumar, Adarsh Jha, Mukunda NS, Ishaan Bhola*
  Abstract: We present SuperCoder2.0, an advanced autonomous system designed to enhance
software development through artificial intelligence. The system combines an
AI-native development approach with intelligent agents to enable fully
autonomous coding. Key focus areas include a retry mechanism with error output
traceback, comprehensive code rewriting and replacement using Abstract Syntax
Tree (ast) parsing to minimize linting issues, code embedding technique for
retrieval-augmented generation, and a focus on localizing methods for
problem-solving rather than identifying specific line numbers. The methodology
employs a three-step hierarchical search space reduction approach for code base
navigation and bug localization:utilizing Retrieval Augmented Generation (RAG)
and a Repository File Level Map to identify candidate files, (2) narrowing down
to the most relevant files using a File Level Schematic Map, and (3) extracting
'relevant locations' within these files. Code editing is performed through a
two-part module comprising CodeGeneration and CodeEditing, which generates
multiple solutions at different temperature values and replaces entire methods
or classes to maintain code integrity. A feedback loop executes
repository-level test cases to validate and refine solutions. Experiments
conducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's
effectiveness, achieving correct file localization in 84.33% of cases within
the top 5 candidates and successfully resolving 34% of test instances. This
performance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.
The system's ability to handle diverse repositories and problem types
highlights its potential as a versatile tool for autonomous software
development. Future work will focus on refining the code editing process and
exploring advanced embedding models for improved natural language to code
mapping.

- **[2024/08] GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining Automotive Software Release Decision-Making.** [[Link](https://link.springer.com/chapter/10.1007/978-3-031-80889-0_3)]
  *Arsham Gholamzadeh Khoee, Yinan Yu, Robert Feldt, Andris Freimanis, Patrick Andersson Rhodin, Dhasarathy Parthasarathy*
Traditional methods for making software deployment decisions in the automotive industry typically rely on manual analysis of tabular software test data. These methods often lead to higher costs and delays in the software release cycle due to their labor-intensive nature. Large Language Models (LLMs) present a promising solution to these challenges. However, their application generally demands multiple rounds of human-driven prompt engineering, which limits their practical deployment, particularly for industrial end-users who need reliable and efficient results. In this paper, we propose GoNoGo, an LLM agent system designed to streamline automotive software deployment while meeting both functional requirements and practical industrial constraints. Unlike previous systems, GoNoGo is specifically tailored to address domain-specific and risk-sensitive systems. We evaluate GoNoGo's performance across different task difficulties using zero-shot and few-shot examples taken from industrial practice. Our results show that GoNoGo achieves a 100% success rate for tasks up to Level 2 difficulty with 3-shot examples, and maintains high performance even for more complex tasks. We find that GoNoGo effectively automates decision-making for simpler tasks, significantly reducing the need for manual intervention. In summary, GoNoGo represents an efficient and user-friendly LLM-based solution currently employed in our industrial partner's company to assist with software release decision-making, supporting more informed and timely decisions in the release process for risk-sensitive vehicle systems.

- **[2024-07] Agentless: Demystifying LLM-based Software Engineering Agents** [[arXiv](http://arxiv.org/abs/2407.01489v2)]
  *Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, Lingming Zhang*
  Abstract: Recent advancements in large language models (LLMs) have significantly
advanced the automation of software development tasks, including code
synthesis, program repair, and test generation. More recently, researchers and
industry practitioners have developed various autonomous LLM agents to perform
end-to-end software development tasks. These agents are equipped with the
ability to use tools, run commands, observe feedback from the environment, and
plan for future actions. However, the complexity of these agent-based
approaches, together with the limited abilities of current LLMs, raises the
following question: Do we really have to employ complex autonomous software
agents? To attempt to answer this question, we build Agentless -- an agentless
approach to automatically solve software development problems. Compared to the
verbose and complex setup of agent-based approaches, Agentless employs a
simplistic three-phase process of localization, repair, and patch validation,
without letting the LLM decide future actions or operate with complex tools.
Our results on the popular SWE-bench Lite benchmark show that surprisingly the
simplistic Agentless is able to achieve both the highest performance (32.00%,
96 correct fixes) and low cost ($0.70) compared with all existing open-source
software agents! Furthermore, we manually classified the problems in SWE-bench
Lite and found problems with exact ground truth patch or
insufficient/misleading issue descriptions. As such, we construct SWE-bench
Lite-S by excluding such problematic issues to perform more rigorous evaluation
and comparison. Our work highlights the current overlooked potential of a
simple, interpretable technique in autonomous software development. We hope
Agentless will help reset the baseline, starting point, and horizon for
autonomous software agents, and inspire future work along this crucial
direction.

- **[2024/02] More Agents Is All You Need.** [[Link](https://openreview.net/pdf?id=bgzUSZ8aeg)]
  *Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, Deheng Ye*
We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method, termed as Agent Forest, is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: this https URL

- **[2024-02] CodePori: Large-Scale System for Autonomous Software Development Using
  Multi-Agent Technology** [[arXiv](http://arxiv.org/abs/2402.01411v2)]
  *Zeeshan Rasheed, Malik Abdul Sami, Kai-Kristian Kemell, Muhammad Waseem, Mika Saari, Kari Systä, Pekka Abrahamsson*
  Abstract: Context: Large Language Models (LLMs) and Generative Pre-trained Transformers
(GPTs) have transformed the field of Software Engineering (SE). Existing
LLM-based multi-agent models have successfully addressed basic dialogue tasks.
However, the potential of LLMs for more challenging tasks, such as automated
code generation for large and complex projects, has been investigated in only a
few existing works. Objective: This paper aims to investigate the potential of
LLM-based agents in the software industry, particularly in enhancing
productivity and reducing time-to-market for complex software solutions. Our
primary objective is to gain insights into how these agents can fundamentally
transform the development of large-scale software. Methods: We introduce
CodePori, a novel system designed to automate code generation for large and
complex software projects based on functional and non-functional requirements
defined by stakeholders. To assess the proposed system performance, we utilized
the HumanEval benchmark and manually tested the CodePori model, providing 20
different project descriptions as input and then evaluated the code accuracy by
manually executing the code. Results: CodePori is able to generate running code
for large-scale projects, aligned with the typical software development
process. The HumanEval benchmark results indicate that CodePori improves code
accuracy by 89%. A manual assessment conducted by the first author shows that
the CodePori system achieved an accuracy rate of 85%. Conclusion: Based on the
results, our conclusion is that proposed system demonstrates the transformative
potential of LLM-based agents in SE, highlighting their practical applications
and opening new opportunities for broader adoption in both industry and
academia. Our project is publicly available at
https://github.com/GPT-Laboratory/CodePori.

- **[2023-10] Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for
  Autonomous LLM-powered Multi-Agent Architectures** [[arXiv](http://arxiv.org/abs/2310.03659v1)]
  *Thorsten Händler*
  Abstract: Large language models (LLMs) have revolutionized the field of artificial
intelligence, endowing it with sophisticated language understanding and
generation capabilities. However, when faced with more complex and
interconnected tasks that demand a profound and iterative thought process, LLMs
reveal their inherent limitations. Autonomous LLM-powered multi-agent systems
represent a strategic response to these challenges. Such systems strive for
autonomously tackling user-prompted goals by decomposing them into manageable
tasks and orchestrating their execution and result synthesis through a
collective of specialized intelligent agents. Equipped with LLM-powered
reasoning capabilities, these agents harness the cognitive synergy of
collaborating with their peers, enhanced by leveraging contextual resources
such as tools and datasets. While these architectures hold promising potential
in amplifying AI capabilities, striking the right balance between different
levels of autonomy and alignment remains the crucial challenge for their
effective operation. This paper proposes a comprehensive multi-dimensional
taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems
balance the dynamic interplay between autonomy and alignment across various
aspects inherent to architectural viewpoints such as goal-driven task
management, agent composition, multi-agent collaboration, and context
interaction. It also includes a domain-ontology model specifying fundamental
architectural concepts. Our taxonomy aims to empower researchers, engineers,
and AI practitioners to systematically analyze the architectural dynamics and
balancing strategies employed by these increasingly prevalent AI systems. The
exploratory taxonomic classification of selected representative LLM-powered
multi-agent systems illustrates its practical utility and reveals potential for
future research and development.

- **[2023/09] AutoAgents: A Framework for Automatic Agent Generation.** [[Link](https://www.ijcai.org/proceedings/2024/0003.pdf)]
  *Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje Karlsson, Jie Fu, Yemin Shi*
Large language models (LLMs) have enabled remarkable advances in automated task-solving with multi-agent systems. However, most existing LLM-based multi-agent approaches rely on predefined agents to handle simple tasks, limiting the adaptability of multi-agent collaboration to different scenarios. Therefore, we introduce AutoAgents, an innovative framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks. Specifically, AutoAgents couples the relationship between tasks and roles by dynamically generating multiple required agents based on task content and planning solutions for the current task based on the generated expert agents. Multiple specialized agents collaborate with each other to efficiently accomplish tasks. Concurrently, an observer role is incorporated into the framework to reflect on the designated plans and agents' responses and improve upon them. Our experiments on various benchmarks demonstrate that AutoAgents generates more coherent and accurate solutions than the existing multi-agent methods. This underscores the significance of assigning different roles to different tasks and of team cooperation, offering new perspectives for tackling complex tasks. The repository of this project is available at this https URL.

- **[2023/08] MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework.** [[Link](https://openreview.net/pdf?id=VtmBAGCN7o)]
  *Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, Jürgen Schmidhuber*
Remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Existing LLM-based multi-agent systems can already solve simple dialogue tasks. Solutions to more complex tasks, however, are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors. MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together. On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems. Our project can be found at this https URL

- **[2023/08] GPT-in-the-Loop: Adaptive Decision-Making for Multiagent Systems.** [[Link](https://ieeexplore.ieee.org/document/10386490)]
  *Nathalia Nascimento, Paulo Alencar, Donald Cowan*
This paper introduces the "GPT-in-the-loop" approach, a novel method combining the advanced reasoning capabilities of Large Language Models (LLMs) like Generative Pre-trained Transformers (GPT) with multiagent (MAS) systems. Venturing beyond traditional adaptive approaches that generally require long training processes, our framework employs GPT-4 for enhanced problem-solving and explanation skills. Our experimental backdrop is the smart streetlight Internet of Things (IoT) application. Here, agents use sensors, actuators, and neural networks to create an energy-efficient lighting system. By integrating GPT-4, these agents achieve superior decision-making and adaptability without the need for extensive training. We compare this approach with both traditional neuroevolutionary methods and solutions provided by software engineers, underlining the potential of GPT-driven multiagent systems in IoT. Structurally, the paper outlines the incorporation of GPT into the agent-driven Framework for the Internet of Things (FIoT), introduces our proposed GPT-in-the-loop approach, presents comparative results in the IoT context, and concludes with insights and future directions.

- **[2023/04] Towards autonomous system: flexible modular production system enhanced with large language model agents.** [[Link](https://ieeexplore.ieee.org/abstract/document/10275362)]
  *Yuchen Xia, Manthan Shenoy, Nasser Jazdi, Michael Weyrich*
In this paper, we present a novel framework that combines large language models (LLMs), digital twins and industrial automation system to enable intelligent planning and control of production processes. We retrofit the automation system for a modular production facility and create executable control interfaces of fine-granular functionalities and coarse-granular skills. Low-level functionalities are executed by automation components, and high-level skills are performed by automation modules. Subsequently, a digital twin system is developed, registering these interfaces and containing additional descriptive information about the production system. Based on the retrofitted automation system and the created digital twins, LLM-agents are designed to interpret descriptive information in the digital twins and control the physical system through service interfaces. These LLM-agents serve as intelligent agents on different levels within an automation system, enabling autonomous planning and control of flexible production. Given a task instruction as input, the LLM-agents orchestrate a sequence of atomic functionalities and skills to accomplish the task. We demonstrate how our implemented prototype can handle un-predefined tasks, plan a production process, and execute the operations. This research highlights the potential of integrating LLMs into industrial automation systems in the context of smart factory for more agile, flexible, and adaptive production processes, while it also underscores the critical insights and limitations for future work. Demos at: this https URL



### Cross-Agent Collaboration
- **[2025-02] SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software
  Engineering** [[arXiv](http://arxiv.org/abs/2502.06994v1)]
  *Xuehang Guo, Xingyao Wang, Yangyi Chen, Sha Li, Chi Han, Manling Li, Heng Ji*
  Abstract: Software engineering (SE) is increasingly collaborative, with developers
working together on shared complex codebases. Effective collaboration in shared
environments requires participants -- whether humans or AI agents -- to stay on
the same page as their environment evolves. When a collaborator's understanding
diverges from the current state -- what we term the out-of-sync challenge --
the collaborator's actions may fail, leading to integration issues. In this
work, we introduce SyncMind, a framework that systematically defines the
out-of-sync problem faced by large language model (LLM) agents in collaborative
software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark
featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE
derived from 21 popular GitHub repositories with executable verification tests.
Experiments on SyncBench uncover critical insights into existing LLM agents'
capabilities and limitations. Besides substantial performance gaps among agents
(from Llama-3.1 agent <= 3.33% to Claude-3.5-Sonnet >= 28.18%), their
consistently low collaboration willingness (<= 4.86%) suggests fundamental
limitations of existing LLM in CSE. However, when collaboration occurs, it
positively correlates with out-of-sync recovery success. Minimal performance
differences in agents' resource-aware out-of-sync recoveries further reveal
their significant lack of resource awareness and adaptability, shedding light
on future resource-efficient collaborative systems. Code and data are openly
available on our project website: https://xhguo7.github.io/SyncMind/.

- **[2024-09] LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless
  Integration of Multi Active/Passive Core-Agents** [[arXiv](http://arxiv.org/abs/2409.11393v2)]
  *Amine Ben Hassouna, Hana Chaari, Ines Belhaj*
  Abstract: In an era where vast amounts of data are collected and processed from diverse
sources, there is a growing demand to develop sophisticated AI systems capable
of intelligently fusing and analyzing this information. To address these
challenges, researchers have turned towards integrating tools into LLM-powered
agents to enhance the overall information fusion process. However, the
conjunction of these technologies and the proposed enhancements in several
state-of-the-art works followed a non-unified software architecture resulting
in a lack of modularity and terminological inconsistencies among researchers.
To address these issues, we propose a novel LLM-based Agent Unified Modeling
Framework (LLM-Agent-UMF) that aims to establish a clear foundation for agent
development from both functional and software architectural perspectives. Our
framework distinguishes between the different components of an LLM-based agent,
setting LLMs, and tools apart from a new element, the core-agent, playing the
role of the central coordinator of the agent. This pivotal entity comprises
five modules: planning, memory, profile, action, and security - the latter
often neglected in previous works. By classifying core-agents into passive and
active types based on their authoritative natures, we propose various
multi-core agent architectures that combine unique characteristics of
distinctive agents to tackle complex tasks more efficiently. We evaluate our
framework by applying it to thirteen state-of-the-art agents, thereby
demonstrating its alignment with their functionalities and clarifying the
overlooked architectural aspects. Moreover, we thoroughly assess five of our
proposed architectures through the integration of existing agents into new
hybrid active/passive core-agents architectures. This analysis provides
insights into potential improvements and highlights challenges involved in
combining specific agents.

- **[2024-09] MOSS: Enabling Code-Driven Evolution and Context Management for AI
  Agents** [[arXiv](http://arxiv.org/abs/2409.16120v1)]
  *Ming Zhu, Yi Zhou*
  Abstract: Developing AI agents powered by large language models (LLMs) faces
significant challenges in achieving true Turing completeness and adaptive,
code-driven evolution. Current approaches often generate code independently of
its runtime context, relying heavily on the LLM's memory, which results in
inefficiencies and limits adaptability. Manual protocol development in sandbox
environments further constrains the agent's autonomous adaptability. Crucially,
achieving consistency in code and context across multi-turn interactions and
ensuring isolation of local variables within each interaction remains an
unsolved problem.
  We introduce MOSS (llM-oriented Operating System Simulation), a novel
framework that addresses these challenges by integrating code generation with a
dynamic context management system. MOSS ensures consistency and adaptability by
using a mechanism that maintains the Python context across interactions,
including isolation of local variables and preservation of runtime integrity.
At its core, the framework employs an Inversion of Control (IoC) container in
conjunction with decorators to enforce the least knowledge principle, allowing
agents to focus on abstract interfaces rather than concrete implementations.
This facilitates seamless integration of new tools and libraries, enables
runtime instance replacement, and reduces prompt complexity, providing a "what
you see is what you get" environment for the agent.
  Through a series of case studies, we show how this framework can enhance the
efficiency and capabilities of agent development and highlight its advantages
in moving towards Turing-complete agents capable of evolving through code.

- **[2024/08] Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents** [[Link](https://openreview.net/pdf?id=cKlzKs3Nnb)]
  *Kexun Zhang, Weiran Yao, Zuxin Liu, Yihao Feng, Zhiwei Liu, Rithesh Murthy, Tian Lan, Lei Li, Renze Lou, Jiacheng Xu, Bo Pang, Yingbo Zhou, Shelby Heinecke, Silvio Savarese, Huan Wang, Caiming Xiong*
Large language model (LLM) agents have shown great potential in solving real-world software engineering (SWE) problems. The most advanced open-source SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite. However, these sophisticated agent frameworks exhibit varying strengths, excelling in certain tasks while underperforming in others. To fully harness the diversity of these agents, we propose DEI (Diversity Empowered Intelligence), a framework that leverages their unique expertise. DEI functions as a meta-module atop existing SWE agent frameworks, managing agent collectives for enhanced problem-solving. Experimental results show that a DEI-guided committee of agents is able to surpass the best individual agent's performance by a large margin. For instance, a group of open-source SWE agents, with a maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3% resolve rate with DEI, making a 25% improvement and beating most closed-source solutions. Our best-performing group excels with a 55% resolve rate, securing the highest ranking on SWE-Bench Lite. Our findings contribute to the growing body of research on collaborative AI systems and their potential to solve complex software engineering challenges.

- **[2024-06] AgileCoder: Dynamic Collaborative Agents for Software Development based
  on Agile Methodology** [[arXiv](https://arxiv.org/pdf/2406.11912)]
  *Minh Huynh Nguyen, Thang Phan Chau, Phong X. Nguyen, Nghi D. Q. Bui*
  Abstract: Software agents have emerged as promising tools for addressing complex
software engineering tasks. Existing works, on the other hand, frequently
oversimplify software development workflows, despite the fact that such
workflows are typically more complex in the real world. Thus, we propose
AgileCoder, a multi agent system that integrates Agile Methodology (AM) into
the framework. This system assigns specific AM roles - such as Product Manager,
Developer, and Tester to different agents, who then collaboratively develop
software based on user inputs. AgileCoder enhances development efficiency by
organizing work into sprints, focusing on incrementally developing software
through sprints. Additionally, we introduce Dynamic Code Graph Generator, a
module that creates a Code Dependency Graph dynamically as updates are made to
the codebase. This allows agents to better comprehend the codebase, leading to
more precise code generation and modifications throughout the software
development process. AgileCoder surpasses existing benchmarks, like ChatDev and
MetaGPT, establishing a new standard and showcasing the capabilities of multi
agent systems in advanced software engineering environments.

- **[2023/08] AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation.** [[Link](https://openreview.net/pdf?id=BAakY1hNKS)]
  *Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Awadallah, Ryen W. White, Doug Burger, Chi Wang*
AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.

- **[2023/07] ChatDev: Communicative Agents for Software Development.** [[Link](https://aclanthology.org/2024.acl-long.810.pdf)]
  *Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, Maosong Sun*
Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at this https URL.

- **[2023/04] Self-collaboration Code Generation via ChatGPT.** [[Link](https://dl.acm.org/doi/10.1145/3672459)]
  *Yihong Dong, Xue Jiang, Zhi Jin, Ge Li*
Although Large Language Models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, 1) Multiple LLM agents act as distinct `experts', each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other's work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development's analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9%-47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.

- **[2023/03] CAMEL: Communicative Agents for “Mind” Exploration of Large Language Model Society.** [[Link](https://proceedings.NeurIPS.cc/paper_files/paper/2023/file/a3621ee907def47c1b952ade25c67698-Paper-Conference.pdf)]
  *Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem*
The rapid advancement of chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents, and provides insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of a society of agents, providing a valuable resource for investigating conversational language models. In particular, we conduct comprehensive studies on instruction-following cooperation in multi-agent settings. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond: this https URL.




## Software Testing, Debugging, and Quality Assurance

### Automated Testing
- **[2025-02] Otter: Generating Tests from Issues to Validate SWE Patches** [[arXiv](http://arxiv.org/abs/2502.05368v1)]
  *Toufique Ahmed, Jatin Ganhotra, Rangeet Pan, Avraham Shinnar, Saurabh Sinha, Martin Hirzel*
  Abstract: While there has been plenty of work on generating tests from existing code,
there has been limited work on generating tests from issues. A correct test
must validate the code patch that resolves the issue. In this work, we focus on
the scenario where the code patch does not exist yet. This approach supports
two major use-cases. First, it supports TDD (test-driven development), the
discipline of "test first, write code later" that has well-documented benefits
for human software engineers. Second, it also validates SWE (software
engineering) agents, which generate code patches for resolving issues. This
paper introduces Otter, an LLM-based solution for generating tests from issues.
Otter augments LLMs with rule-based analysis to check and repair their outputs,
and introduces a novel self-reflective action planning stage. Experiments show
Otter outperforming state-of-the-art systems for generating tests from issues,
in addition to enhancing systems that generate patches from issues. We hope
that Otter helps make developers more productive at resolving issues and leads
to more robust, well-tested code.

- **[2025-01] AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL** [[arXiv](http://arxiv.org/abs/2501.08600v1)]
  *Tyler Stennett, Myeongsoo Kim, Saurabh Sinha, Alessandro Orso*
  Abstract: As REST APIs have become widespread in modern web services, comprehensive
testing of these APIs has become increasingly crucial. Due to the vast search
space consisting of operations, parameters, and parameter values along with
their complex dependencies and constraints, current testing tools suffer from
low code coverage, leading to suboptimal fault detection. To address this
limitation, we present a novel tool, AutoRestTest, which integrates the
Semantic Operation Dependency Graph (SODG) with Multi-Agent Reinforcement
Learning (MARL) and large language models (LLMs) for effective REST API
testing. AutoRestTest determines operation-dependent parameters using the SODG
and employs five specialized agents (operation, parameter, value, dependency,
and header) to identify dependencies of operations and generate operation
sequences, parameter combinations, and values. AutoRestTest provides a
command-line interface and continuous telemetry on successful operation count,
unique server errors detected, and time elapsed. Upon completion, AutoRestTest
generates a detailed report highlighting errors detected and operations
exercised. In this paper, we introduce our tool and present preliminary
results.

- **[2025-01] LLM-Agents Driven Automated Simulation Testing and Analysis of small
  Uncrewed Aerial Systems** [[arXiv](http://arxiv.org/abs/2501.11864v1)]
  *Venkata Sai Aswath Duvvuru, Bohan Zhang, Michael Vierhauser, Ankit Agrawal*
  Abstract: Thorough simulation testing is crucial for validating the correct behavior of
small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including
adverse weather conditions (such as wind, and fog), diverse settings (hilly
terrain, or urban areas), and varying mission profiles (surveillance,
tracking). While various sUAS simulation tools exist to support developers, the
entire process of creating, executing, and analyzing simulation tests remains a
largely manual and cumbersome task. Developers must identify test scenarios,
set up the simulation environment, integrate the System under Test (SuT) with
simulation tools, formulate mission plans, and collect and analyze results.
These labor-intensive tasks limit the ability of developers to conduct
exhaustive testing across a wide range of scenarios. To alleviate this problem,
in this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven
framework, where multiple LLM agents collaborate to support the sUAS simulation
testing process. This includes: (1) creating test scenarios that subject the
SuT to unique environmental contexts; (2) preparing the simulation environment
as per the test scenario; (3) generating diverse sUAS missions for the SuT to
execute; and (4) analyzing simulation results and providing an interactive
analytics interface. Further, the design of the framework is flexible for
creating and testing scenarios for a variety of sUAS use cases, simulation
tools, and SuT input requirements. We evaluated our approach by (a) conducting
simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b)
analyzing the performance of each agent, and (c) gathering feedback from sUAS
developers. Our findings indicate that AutoSimTest significantly improves the
efficiency and scope of the sUAS testing process, allowing for more
comprehensive and varied scenario evaluations while reducing the manual effort.

- **[2025-01] Mutation-Guided LLM-based Test Generation at Meta** [[arXiv](http://arxiv.org/abs/2501.12862v1)]
  *Christopher Foster, Abhishek Gulati, Mark Harman, Inna Harper, Ke Mao, Jillian Ritchey, Hervé Robert, Shubho Sengupta*
  Abstract: This paper describes Meta's ACH system for mutation-guided LLM-based test
generation. ACH generates relatively few mutants (aka simulated faults),
compared to traditional mutation testing. Instead, it focuses on generating
currently undetected faults that are specific to an issue of concern. From
these currently uncaught faults, ACH generates tests that can catch them,
thereby `killing' the mutants and consequently hardening the platform against
regressions. We use privacy concerns to illustrate our approach, but ACH can
harden code against {\em any} type of regression. In total, ACH was applied to
10,795 Android Kotlin classes in 7 software platforms deployed by Meta, from
which it generated 9,095 mutants and 571 privacy-hardening test cases. ACH also
deploys an LLM-based equivalent mutant detection agent that achieves a
precision of 0.79 and a recall of 0.47 (rising to 0.95 and 0.96 with simple
pre-processing). ACH was used by Messenger and WhatsApp test-a-thons where
engineers accepted 73% of its tests, judging 36% to privacy relevant. We
conclude that ACH hardens code against specific concerns and that, even when
its tests do not directly tackle the specific concern, engineers find them
useful for their other benefits.

- **[2024-12] Automated Soap Opera Testing Directed by LLMs and Scenario Knowledge:
  Feasibility, Challenges, and Road Ahead** [[arXiv](http://arxiv.org/abs/2412.08581v1)]
  *Yanqi Su, Zhenchang Xing, Chong Wang, Chunyang Chen, Xiwei Xu, Qinghua Lu, Liming Zhu*
  Abstract: Exploratory testing (ET) harnesses tester's knowledge, creativity, and
experience to create varying tests that uncover unexpected bugs from the
end-user's perspective. Although ET has proven effective in system-level
testing of interactive systems, the need for manual execution has hindered
large-scale adoption. In this work, we explore the feasibility, challenges and
road ahead of automated scenario-based ET (a.k.a soap opera testing). We
conduct a formative study, identifying key insights for effective manual soap
opera testing and challenges in automating the process. We then develop a
multi-agent system leveraging LLMs and a Scenario Knowledge Graph (SKG) to
automate soap opera testing. The system consists of three multi-modal agents,
Planner, Player, and Detector that collaborate to execute tests and identify
potential bugs. Experimental results demonstrate the potential of automated
soap opera testing, but there remains a significant gap compared to manual
execution, especially under-explored scenario boundaries and incorrectly
identified bugs. Based on the observation, we envision road ahead for the
future of automated soap opera testing, focusing on three key aspects: the
synergy of neural and symbolic approaches, human-AI co-learning, and the
integration of soap opera testing with broader software engineering practices.
These insights aim to guide and inspire the future research.

- **[2024-12] You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary
  Projects** [[arXiv](http://arxiv.org/abs/2412.10133v1)]
  *Islem Bouzenia, Michael Pradel*
  Abstract: The ability to execute the test suite of a project is essential in many
scenarios, e.g., to assess code quality and code coverage, to validate code
changes made by developers or automated tools, and to ensure compatibility with
dependencies. Despite its importance, executing the test suite of a project can
be challenging in practice because different projects use different programming
languages, software ecosystems, build systems, testing frameworks, and other
tools. These challenges make it difficult to create a reliable, universal test
execution method that works across different projects. This paper presents
ExecutionAgent, an automated technique that installs arbitrary projects,
configures them to run test cases, and produces project-specific scripts to
reproduce the setup. Inspired by the way a human developer would address this
task, our approach is a large language model-based agent that autonomously
executes commands and interacts with the host system. The agent uses
meta-prompting to gather guidelines on the latest technologies related to the
given project, and it iteratively refines its process based on feedback from
the previous steps. Our evaluation applies ExecutionAgent to 50 open-source
projects that use 14 different programming languages and many different build
and testing tools. The approach successfully executes the test suites of 33/55
projects, while matching the test results of ground truth test suite executions
with a deviation of only 7.5\%. These results improve over the best previously
available technique by 6.6x. The costs imposed by the approach are reasonable,
with an execution time of 74 minutes and LLM costs of 0.16 dollars, on average
per project. We envision ExecutionAgent to serve as a valuable tool for
developers, automated programming tools, and researchers that need to execute
tests across a wide variety of projects.

- **[2024-12] The Potential of LLMs in Automating Software Testing: From Generation to
  Reporting** [[arXiv](http://arxiv.org/abs/2501.00217v1)]
  *Betim Sherifi, Khaled Slhoub, Fitzroy Nembhard*
  Abstract: Having a high quality software is essential in software engineering, which
requires robust validation and verification processes during testing
activities. Manual testing, while effective, can be time consuming and costly,
leading to an increased demand for automated methods. Recent advancements in
Large Language Models (LLMs) have significantly influenced software
engineering, particularly in areas like requirements analysis, test automation,
and debugging. This paper explores an agent-oriented approach to automated
software testing, using LLMs to reduce human intervention and enhance testing
efficiency. The proposed framework integrates LLMs to generate unit tests,
visualize call graphs, and automate test execution and reporting. Evaluations
across multiple applications in Python and Java demonstrate the system's high
test coverage and efficient operation. This research underscores the potential
of LLM-powered agents to streamline software testing workflows while addressing
challenges in scalability and accuracy.

- **[2024-11] A Multi-Agent Approach for REST API Testing with Semantic Graphs and
  LLM-Driven Inputs** [[arXiv](http://arxiv.org/abs/2411.07098v2)]
  *Myeongsoo Kim, Tyler Stennett, Saurabh Sinha, Alessandro Orso*
  Abstract: As modern web services increasingly rely on REST APIs, their thorough testing
has become crucial. Furthermore, the advent of REST API documentation
languages, such as the OpenAPI Specification, has led to the emergence of many
black-box REST API testing tools. However, these tools often focus on
individual test elements in isolation (e.g., APIs, parameters, values),
resulting in lower coverage and less effectiveness in fault detection. To
address these limitations, we present AutoRestTest, the first black-box tool to
adopt a dependency-embedded multi-agent approach for REST API testing that
integrates multi-agent reinforcement learning (MARL) with a semantic property
dependency graph (SPDG) and Large Language Models (LLMs). Our approach treats
REST API testing as a separable problem, where four agents -- API, dependency,
parameter, and value agents -- collaborate to optimize API exploration. LLMs
handle domain-specific value generation, the SPDG model simplifies the search
space for dependencies using a similarity score between API operations, and
MARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest
on 12 real-world REST services shows that it outperforms the four leading
black-box REST API testing tools, including those assisted by RESTGPT (which
generates realistic test inputs using LLMs), in terms of code coverage,
operation coverage, and fault detection. Notably, AutoRestTest is the only tool
able to trigger an internal server error in the Spotify service. Our ablation
study illustrates that each component of AutoRestTest -- the SPDG, the LLM, and
the agent-learning mechanism -- contributes to its overall effectiveness.

- **[2024-11] CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge
  Graph** [[arXiv](http://arxiv.org/abs/2411.11532v3)]
  *Hanxiang Xu, Wei Ma, Ting Zhou, Yanjie Zhao, Kai Chen, Qiang Hu, Yang Liu, Haoyu Wang*
  Abstract: In recent years, the programming capabilities of large language models (LLMs)
have garnered significant attention. Fuzz testing, a highly effective
technique, plays a key role in enhancing software reliability and detecting
vulnerabilities. However, traditional fuzz testing tools rely on manually
crafted fuzz drivers, which can limit both testing efficiency and
effectiveness. To address this challenge, we propose an automated fuzz testing
method driven by a code knowledge graph and powered by an LLM-based intelligent
agent system, referred to as CKGFuzzer. We approach fuzz driver creation as a
code generation task, leveraging the knowledge graph of the code repository to
automate the generation process within the fuzzing loop, while continuously
refining both the fuzz driver and input seeds. The code knowledge graph is
constructed through interprocedural program analysis, where each node in the
graph represents a code entity, such as a function or a file. The knowledge
graph-enhanced CKGFuzzer not only effectively resolves compilation errors in
fuzz drivers and generates input seeds tailored to specific API usage
scenarios, but also analyzes fuzz driver crash reports, assisting developers in
improving code quality. By querying the knowledge graph of the code repository
and learning from API usage scenarios, we can better identify testing targets
and understand the specific purpose of each fuzz driver. We evaluated our
approach using eight open-source software projects. The experimental results
indicate that CKGFuzzer achieved an average improvement of 8.73% in code
coverage compared to state-of-the-art techniques. Additionally, CKGFuzzer
reduced the manual review workload in crash case analysis by 84.4% and
successfully detected 11 real bugs (including nine previously unreported bugs)
across the tested libraries.

- **[2024-10] Agents4PLC: Automating Closed-loop PLC Code Generation and Verification
  in Industrial Control Systems using LLM-based Agents** [[arXiv](http://arxiv.org/abs/2410.14209v2)]
  *Zihan Liu, Ruinan Zeng, Dongxia Wang, Gengyun Peng, Jingyi Wang, Qiang Liu, Peiyu Liu, Wenhai Wang*
  Abstract: In industrial control systems, the generation and verification of
Programmable Logic Controller (PLC) code are critical for ensuring operational
efficiency and safety. While Large Language Models (LLMs) have made strides in
automated code generation, they often fall short in providing correctness
guarantees and specialized support for PLC programming. To address these
challenges, this paper introduces Agents4PLC, a novel framework that not only
automates PLC code generation but also includes code-level verification through
an LLM-based multi-agent system. We first establish a comprehensive benchmark
for verifiable PLC code generation area, transitioning from natural language
requirements to human-written-verified formal specifications and reference PLC
code. We further enhance our `agents' specifically for industrial control
systems by incorporating Retrieval-Augmented Generation (RAG), advanced prompt
engineering techniques, and Chain-of-Thought strategies. Evaluation against the
benchmark demonstrates that Agents4PLC significantly outperforms previous
methods, achieving superior results across a series of increasingly rigorous
metrics. This research not only addresses the critical challenges in PLC
programming but also highlights the potential of our framework to generate
verifiable code applicable to real-world industrial applications.

- **[2024-09] Exploring the Integration of Large Language Models in Industrial Test
  Maintenance Processes** [[arXiv](http://arxiv.org/abs/2409.06416v1)]
  *Ludvig Lemner, Linnea Wahlgren, Gregory Gay, Nasser Mohammadiha, Jingxiong Liu, Joakim Wennerberg*
  Abstract: Much of the cost and effort required during the software testing process is
invested in performing test maintenance - the addition, removal, or
modification of test cases to keep the test suite in sync with the
system-under-test or to otherwise improve its quality. Tool support could
reduce the cost - and improve the quality - of test maintenance by automating
aspects of the process or by providing guidance and support to developers.
  In this study, we explore the capabilities and applications of large language
models (LLMs) - complex machine learning models adapted to textual analysis -
to support test maintenance. We conducted a case study at Ericsson AB where we
explored the triggers that indicate the need for test maintenance, the actions
that LLMs can take, and the considerations that must be made when deploying
LLMs in an industrial setting. We also proposed and demonstrated
implementations of two multi-agent architectures that can predict which test
cases require maintenance following a change to the source code. Collectively,
these contributions advance our theoretical and practical understanding of how
LLMs can be deployed to benefit industrial test maintenance processes.

- **[2024-07] Seeing is Believing: Vision-driven Non-crash Functional Bug Detection
  for Mobile Apps** [[arXiv](https://arxiv.org/pdf/2407.03037)]
  *Zhe Liu, Cheng Li, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Yawen Wang, Jun Hu, Qing Wang*
  Abstract: Mobile app GUI (Graphical User Interface) pages now contain rich visual
information, with the visual semantics of each page helping users understand
the application logic. However, these complex visual and functional logic
present new challenges to software testing. Existing automated GUI testing
methods, constrained by the lack of reliable testing oracles, are limited to
detecting crash bugs with obvious abnormal signals. Consequently, many
non-crash functional bugs, ranging from unexpected behaviors to logical errors,
often evade detection by current techniques. While these non-crash functional
bugs can exhibit visual cues that serve as potential testing oracles, they
often entail a sequence of screenshots, and detecting them necessitates an
understanding of the operational logic among GUI page transitions, which is
challenging traditional techniques. Considering the remarkable performance of
Multimodal Large Language Models (MLLM) in visual and language understanding,
this paper proposes Trident, a novel vision-driven, multi-agent collaborative
automated GUI testing approach for detecting non-crash functional bugs. It
comprises three agents: Explorer, Monitor, and Detector, to guide the
exploration, oversee the testing progress, and spot issues. We also address
several challenges, i.e., align visual and textual information for MLLM input,
achieve functionality-oriented exploration, and infer test oracles for
non-crash bugs, to enhance the performance of functionality bug detection. We
evaluate Trident on 590 non-crash bugs and compare it with 12 baselines, it can
achieve more than 14%-112% and 108%-147% boost in average recall and precision
compared with the best baseline. The ablation study further proves the
contribution of each module. Moreover, Trident identifies 43 new bugs on Google
Play, of which 31 have been fixed.

- **[2024-06] A Tool for Test Case Scenarios Generation Using Large Language Models** [[arXiv](http://arxiv.org/abs/2406.07021v1)]
  *Abdul Malik Sami, Zeeshan Rasheed, Muhammad Waseem, Zheying Zhang, Herda Tomas, Pekka Abrahamsson*
  Abstract: Large Language Models (LLMs) are widely used in Software Engineering (SE) for
various tasks, including generating code, designing and documenting software,
adding code comments, reviewing code, and writing test scripts. However,
creating test scripts or automating test cases demands test suite documentation
that comprehensively covers functional requirements. Such documentation must
enable thorough testing within a constrained scope and timeframe, particularly
as requirements and user demands evolve. This article centers on generating
user requirements as epics and high-level user stories and crafting test case
scenarios based on these stories. It introduces a web-based software tool that
employs an LLM-based agent and prompt engineering to automate the generation of
test case scenarios against user requirements.

- **[2024-04] Large Language Models as Test Case Generators: Performance Evaluation
  and Enhancement** [[arXiv](http://arxiv.org/abs/2404.13340v1)]
  *Kefan Li, Yuan Yuan*
  Abstract: Code generation with Large Language Models (LLMs) has been extensively
studied and achieved remarkable progress. As a complementary aspect to code
generation, test case generation is of crucial importance in ensuring the
quality and reliability of code. However, using LLMs as test case generators
has been much less explored. Current research along this line primarily focuses
on enhancing code generation with assistance from test cases generated by LLMs,
while the performance of LLMs in test case generation alone has not been
comprehensively examined. To bridge this gap, we conduct extensive experiments
to study how well LLMs can generate high-quality test cases. We find that as
the problem difficulty increases, state-of-the-art LLMs struggle to generate
correct test cases, largely due to their inherent limitations in computation
and reasoning. To mitigate this issue, we further propose a multi-agent
framework called \emph{TestChain} that decouples the generation of test inputs
and test outputs. Notably, TestChain uses a ReAct format conversation chain for
LLMs to interact with a Python interpreter in order to provide more accurate
test outputs. Our results indicate that TestChain outperforms the baseline by a
large margin. Particularly, in terms of the accuracy of test cases, TestChain
using GPT-4 as the backbone achieves a 13.84\% improvement over the baseline on
the LeetCode-hard dataset.

- **[2024-01] XUAT-Copilot: Multi-Agent Collaborative System for Automated User
  Acceptance Testing with Large Language Model** [[arXiv](https://arxiv.org/pdf/2401.02705)]
  *Zhitao Wang, Wei Wang, Zirao Li, Long Wang, Can Yi, Xinjie Xu, Luyang Cao, Hanjing Su, Shouzhi Chen, Jun Zhou*
  Abstract: In past years, we have been dedicated to automating user acceptance testing
(UAT) process of WeChat Pay, one of the most influential mobile payment
applications in China. A system titled XUAT has been developed for this
purpose. However, there is still a human-labor-intensive stage, i.e, test
scripts generation, in the current system. Therefore, in this paper, we
concentrate on methods of boosting the automation level of the current system,
particularly the stage of test scripts generation. With recent notable
successes, large language models (LLMs) demonstrate significant potential in
attaining human-like intelligence and there has been a growing research area
that employs LLMs as autonomous agents to obtain human-like decision-making
capabilities. Inspired by these works, we propose an LLM-powered multi-agent
collaborative system, named XUAT-Copilot, for automated UAT. The proposed
system mainly consists of three LLM-based agents responsible for action
planning, state checking and parameter selecting, respectively, and two
additional modules for state sensing and case rewriting. The agents interact
with testing device, make human-like decision and generate action command in a
collaborative way. The proposed multi-agent system achieves a close
effectiveness to human testers in our experimental studies and gains a
significant improvement of Pass@1 accuracy compared with single-agent
architecture. More importantly, the proposed system has launched in the formal
testing environment of WeChat Pay mobile app, which saves a considerable amount
of manpower in the daily development work.

- **[2023/11] Intent-Driven Mobile GUI Testing with Autonomous Large Language Model Agents.** [[Link](https://ieeexplore.ieee.org/abstract/document/10638557)]
  *Juyeon Yoon, Robert Feldt, Shin Yoo*
GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51% for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 374 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.

- **[2023/10] AXNav: Replaying Accessibility Tests from Natural Language.** [[Link](https://dl.acm.org/doi/pdf/10.1145/3613904.3642777)]
  *Maryam Taeb, Amanda Swearngin, Eldon Schoop, Ruijia Cheng, Yue Jiang, Jeffrey Nichols*
Developers and quality assurance testers often rely on manual testing to test accessibility features throughout the product lifecycle. Unfortunately, manual testing can be tedious, often has an overwhelming scope, and can be difficult to schedule amongst other development milestones. Recently, Large Language Models (LLMs) have been used for a variety of tasks including automation of UIs. However, to our knowledge, no one has yet explored the use of LLMs in controlling assistive technologies for the purposes of supporting accessibility testing. In this paper, we explore the requirements of a natural language based accessibility testing workflow, starting with a formative study. From this we build a system that takes a manual accessibility test instruction in natural language (e.g., “Search for a show in VoiceOver”) as input and uses an LLM combined with pixel-based UI Understanding models to execute the test and produce a chaptered, navigable video. In each video, to help QA testers, we apply heuristics to detect and flag accessibility issues (e.g., Text size not increasing with Large Text enabled, VoiceOver navigation loops). We evaluate this system through a 10-participant user study with accessibility QA professionals who indicated that the tool would be very useful in their current work and performed tests similarly to how they would manually test the features. The study also reveals insights for future work on using LLMs for accessibility testing.

- **[2023/10] WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models** [[Link](https://dl.acm.org/doi/pdf/10.1145/3689736)]
  *Chenyuan Yang, Yinlin Deng, Runyu Lu, Jiayi Yao, Jiawei Liu, Reyhaneh Jabbarvand, Lingming Zhang*
Compiler correctness is crucial, as miscompilation can falsify program behaviors, leading to serious consequences over the software supply chain. In the literature, fuzzing has been extensively studied to uncover compiler defects. However, compiler fuzzing remains challenging: Existing arts focus on black- and grey-box fuzzing, which generates test programs without sufficient understanding of internal compiler behaviors. As such, they often fail to construct test programs to exercise intricate optimizations. Meanwhile, traditional white-box techniques, such as symbolic execution, are computationally inapplicable to the giant codebase of compiler systems. Recent advances demonstrate that Large Language Models (LLMs) excel in code generation/understanding tasks and even have achieved state-of-the-art performance in black-box fuzzing. Nonetheless, guiding LLMs with compiler source-code information remains a missing piece of research in compiler testing. To this end, we propose WhiteFox, the first white-box compiler fuzzer using LLMs with source-code information to test compiler optimization, with a spotlight on detecting deep logic bugs in the emerging deep learning (DL) compilers. WhiteFox adopts a multi-agent framework: (i) an LLM-based analysis agent examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization; (ii) an LLM-based generation agent produces test programs based on the summarized requirements. Additionally, optimization-triggering tests are also used as feedback to further enhance the test generation prompt on the fly. Our evaluation on the three most popular DL compilers (i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows that WhiteFox can generate high-quality test programs to exercise deep optimizations requiring intricate conditions, practicing up to 8 times more optimizations than state-of-the-art fuzzers. To date, WhiteFox has found in total 101 bugs for the compilers under test, with 92 confirmed as previously unknown and 70 already fixed. Notably, WhiteFox has been recently acknowledged by the PyTorch team, and is in the process of being incorporated into its development workflow. Finally, beyond DL compilers, WhiteFox can also be adapted for compilers in different domains, such as LLVM, where WhiteFox has already found multiple bugs.

- **[2023/06] Towards Autonomous Testing Agents via Conversational Large Language Models.** [[Link](https://dl.acm.org/doi/10.1109/ASE56229.2023.00148)]
  *Robert Feldt, Sungmin Kang, Juyeon Yoon, Shin Yoo*
Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized hallucination of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.



### Bug Fixing and Fault Localization

#### Fault Localization
- **[2025-02] COSMosFL: Ensemble of Small Language Models for Fault Localisation** [[arXiv](http://arxiv.org/abs/2502.02908v1)]
  *Hyunjoon Cho, Sungmin Kang, Gabin An, Shin Yoo*
  Abstract: LLMs are rapidly being adopted to build powerful tools and agents for
software engineering, but most of them rely heavily on extremely large
closed-source models. This, in turn, can hinder wider adoption due to security
issues as well as financial cost and environmental impact. Recently, a number
of open source Small Language Models (SLMs) are being released and gaining
traction. While SLMs are smaller, more energy-efficient, and therefore easier
to locally deploy, they tend to show worse performance when compared to larger
closed LLMs. We present COSMos, a task-level LLM ensemble technique that uses
voting mechanism, to provide a broader range of choice between SLMs and LLMs.
We instantiate COSMos with an LLM-based Fault Localisation technique, AutoFL,
and report the cost-benefit trade-off between LLM accuracy and various costs
such as energy consumption, inference time, and the number of tokens used. An
empirical evaluation using Defects4J shows that COSMos can build effective
ensembles that can achieve Pareto-optimality in terms of FL accuracy and
inference cost, when compared to individual models.

- **[2025-02] OrcaLoca: An LLM Agent Framework for Software Issue Localization** [[arXiv](http://arxiv.org/abs/2502.00350v1)]
  *Zhongming Yu, Hejia Zhang, Yujie Zhao, Hanxian Huang, Matrix Yao, Ke Ding, Jishen Zhao*
  Abstract: Recent developments in Large Language Model (LLM) agents are revolutionizing
Autonomous Software Engineering (ASE), enabling automated coding, problem
fixes, and feature improvements. However, localization -- precisely identifying
software problems by navigating to relevant code sections -- remains a
significant challenge. Current approaches often yield suboptimal results due to
a lack of effective integration between LLM agents and precise code search
mechanisms. This paper introduces OrcaLoca, an LLM agent framework that
improves accuracy for software issue localization by integrating priority-based
scheduling for LLM-guided action, action decomposition with relevance scoring,
and distance-aware context pruning. Experimental results demonstrate that
OrcaLoca becomes the new open-source state-of-the-art (SOTA) in function match
rate (65.33%) on SWE-bench Lite. It also improves the final resolved rate of an
open-source framework by 6.33 percentage points through its patch generation
integration.

- **[2024-11] FlexFL: Flexible and Effective Fault Localization with Open-Source Large
  Language Models** [[arXiv](http://arxiv.org/abs/2411.10714v1)]
  *Chuyang Xu, Zhongxin Liu, Xiaoxue Ren, Gehao Zhang, Ming Liang, David Lo*
  Abstract: Due to the impressive code comprehension ability of Large Language Models
(LLMs), a few studies have proposed to leverage LLMs to locate bugs, i.e.,
LLM-based FL, and demonstrated promising performance. However, first, these
methods are limited in flexibility. They rely on bug-triggering test cases to
perform FL and cannot make use of other available bug-related information,
e.g., bug reports. Second, they are built upon proprietary LLMs, which are,
although powerful, confronted with risks in data privacy. To address these
limitations, we propose a novel LLM-based FL framework named FlexFL, which can
flexibly leverage different types of bug-related information and effectively
work with open-source LLMs. FlexFL is composed of two stages. In the first
stage, FlexFL reduces the search space of buggy code using state-of-the-art FL
techniques of different families and provides a candidate list of bug-related
methods. In the second stage, FlexFL leverages LLMs to delve deeper to
double-check the code snippets of methods suggested by the first stage and
refine fault localization results. In each stage, FlexFL constructs agents
based on open-source LLMs, which share the same pipeline that does not
postulate any type of bug-related information and can interact with function
calls without the out-of-the-box capability. Extensive experimental results on
Defects4J demonstrate that FlexFL outperforms the baselines and can work with
different open-source LLMs. Specifically, FlexFL with a lightweight open-source
LLM Llama3-8B can locate 42 and 63 more bugs than two state-of-the-art
LLM-based FL approaches AutoFL and AgentFL that both use GPT-3.5.

Error fetching details for http://arxiv.org/abs/2410.09117v1: 502 Server Error: Bad Gateway for url: http://export.arxiv.org/api/query?id_list=2410.09117v1
- **[2024/10] REDO: Execution-Free Runtime Error Detection for COding Agents.** [[Link](http://arxiv.org/abs/2410.09117v1)]
  *Shou Li, Andrey Kan, Laurent Callot, Bhavana Bhasker, Muhammad Shihab Rashid, Timothy B Esler*
As LLM-based agents exhibit exceptional capabilities in addressing complex problems, there is a growing focus on developing coding agents to tackle increasingly sophisticated tasks. Despite their promising performance, these coding agents often produce programs or modifications that contain runtime errors, which can cause code failures and are difficult for static analysis tools to detect. Enhancing the ability of coding agents to statically identify such errors could significantly improve their overall performance. In this work, we introduce Execution-free Runtime Error Detection for COding Agents (REDO), a method that integrates LLMs with static analysis tools to detect runtime errors for coding agents, without code execution. Additionally, we propose a benchmark task, SWE-Bench-Error-Detection (SWEDE), based on SWE-Bench (lite), to evaluate error detection in repository-level problems with complex external dependencies. Finally, through both quantitative and qualitative analyses across various error detection tasks, we demonstrate that REDO outperforms current state-of-the-art methods by achieving a 11.0% higher accuracy and 9.1% higher weighted F1 score; and provide insights into the advantages of incorporating LLMs for error detection.

- **[2024-09] Enhancing Fault Localization Through Ordered Code Analysis with LLM
  Agents and Self-Reflection** [[arXiv](http://arxiv.org/abs/2409.13642v1)]
  *Md Nakhla Rafi, Dong Jae Kim, Tse-Hsun Chen, Shaowei Wang*
  Abstract: Locating and fixing software faults is a time-consuming and
resource-intensive task in software development. Traditional fault localization
methods, such as Spectrum-Based Fault Localization (SBFL), rely on statistical
analysis of test coverage data but often suffer from lower accuracy.
Learning-based techniques, while more effective, require extensive training
data and can be computationally expensive. Recent advancements in Large
Language Models (LLMs) offer promising improvements in fault localization by
enhancing code comprehension and reasoning. However, these LLM-based techniques
still face challenges, including token limitations, degraded performance with
long inputs, and difficulties managing large-scale projects with complex
systems involving multiple interacting components. To address these issues, we
introduce LLM4FL, a novel LLM-agent-based fault localization approach that
integrates SBFL rankings with a divide-and-conquer strategy. By dividing large
coverage data into manageable groups and employing multiple LLM agents through
prompt chaining, LLM4FL navigates the codebase and localizes faults more
effectively. The approach also incorporates self-reflection and
chain-of-thought reasoning, enabling agents to iteratively generate fixes and
re-rank suspicious methods. We evaluated LLM4FL on the Defects4J (V2.0.0)
benchmark, comprising 675 real-world faults from 14 open-source Java projects.
Our results demonstrate that LLM4FL outperforms AutoFL by 19.27% in Top-1
accuracy and surpasses state-of-the-art supervised techniques such as DeepFL
and Grace, all without task-specific training. Additionally, we highlight the
impact of coverage splitting and prompt chaining on fault localization
performance and show that different method ordering can improve Top-1 accuracy
by up to 22%.

- **[2024-03] AgentFL: Scaling LLM-based Fault Localization to Project-Level Context** [[arXiv](https://arxiv.org/pdf/2403.16362v1)]
  *Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin Wang, Xiaoling Li, Xiaoguang Mao*
  Abstract: Fault Localization (FL) is an essential step during the debugging process.
With the strong capabilities of code comprehension, the recent Large Language
Models (LLMs) have demonstrated promising performance in diagnosing bugs in the
code. Nevertheless, due to LLMs' limited performance in handling long contexts,
existing LLM-based fault localization remains on localizing bugs within a small
code scope (i.e., a method or a class), which struggles to diagnose bugs for a
large code scope (i.e., an entire software system). To address the limitation,
this paper presents AgentFL, a multi-agent system based on ChatGPT for
automated fault localization. By simulating the behavior of a human developer,
AgentFL models the FL task as a three-step process, which involves
comprehension, navigation, and confirmation. Within each step, AgentFL hires
agents with diversified expertise, each of which utilizes different tools to
handle specific tasks. Particularly, we adopt a series of auxiliary strategies
such as Test Behavior Tracking, Document-Guided Search, and Multi-Round
Dialogue to overcome the challenges in each step. The evaluation on the widely
used Defects4J-V1.2.0 benchmark shows that AgentFL can localize 157 out of 395
bugs within Top-1, which outperforms the other LLM-based approaches and
exhibits complementarity to the state-of-the-art learning-based techniques.
Additionally, we confirm the indispensability of the components in AgentFL with
the ablation study and demonstrate the usability of AgentFL through a user
study. Finally, the cost analysis shows that AgentFL spends an average of only
0.074 dollars and 97 seconds for a single bug.

- **[2024/03] Exploring LLM-based Agents for Root Cause Analysis.** [[Link](https://dl.acm.org/doi/abs/10.1145/3663529.3663841)]
  *Devjeet Roy, Xuchao Zhang, Rashi Bhave, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, Saravan Rajmohan*
The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team's specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of LLM based agents for RCA to address this limitation. We present a thorough empirical evaluation of a ReAct agent equipped with retrieval tools, on an out-of-distribution dataset of production incidents collected at Microsoft. Results show that ReAct performs competitively with strong retrieval and reasoning baselines, but with highly increased factual accuracy. We then extend this evaluation by incorporating discussions associated with incident reports as additional inputs for the models, which surprisingly does not yield significant performance improvements. Lastly, we conduct a case study with a team at Microsoft to equip the ReAct agent with tools that give it access to external diagnostic services that are used by the team for manual RCA. Our results show how agents can overcome the limitations of prior work, and practical considerations for implementing such a system in practice.

#### Automated Bug Fixing
- **[2025-02] Otter: Generating Tests from Issues to Validate SWE Patches** [[arXiv](http://arxiv.org/abs/2502.05368v1)]
  *Toufique Ahmed, Jatin Ganhotra, Rangeet Pan, Avraham Shinnar, Saurabh Sinha, Martin Hirzel*
  Abstract: While there has been plenty of work on generating tests from existing code,
there has been limited work on generating tests from issues. A correct test
must validate the code patch that resolves the issue. In this work, we focus on
the scenario where the code patch does not exist yet. This approach supports
two major use-cases. First, it supports TDD (test-driven development), the
discipline of "test first, write code later" that has well-documented benefits
for human software engineers. Second, it also validates SWE (software
engineering) agents, which generate code patches for resolving issues. This
paper introduces Otter, an LLM-based solution for generating tests from issues.
Otter augments LLMs with rule-based analysis to check and repair their outputs,
and introduces a novel self-reflective action planning stage. Experiments show
Otter outperforming state-of-the-art systems for generating tests from issues,
in addition to enhancing systems that generate patches from issues. We hope
that Otter helps make developers more productive at resolving issues and leads
to more robust, well-tested code.

- **[2025-02] Agentic Bug Reproduction for Effective Automated Program Repair at
  Google** [[arXiv](http://arxiv.org/abs/2502.01821v1)]
  *Runxiang Cheng, Michele Tufano, Jürgen Cito, José Cambronero, Pat Rondon, Renyao Wei, Aaron Sun, Satish Chandra*
  Abstract: Bug reports often lack sufficient detail for developers to reproduce and fix
the underlying defects. Bug Reproduction Tests (BRTs), tests that fail when the
bug is present and pass when it has been resolved, are crucial for debugging,
but they are rarely included in bug reports, both in open-source and in
industrial settings. Thus, automatically generating BRTs from bug reports has
the potential to accelerate the debugging process and lower time to repair.
This paper investigates automated BRT generation within an industry setting,
specifically at Google, focusing on the challenges of a large-scale,
proprietary codebase and considering real-world industry bugs extracted from
Google's internal issue tracker. We adapt and evaluate a state-of-the-art BRT
generation technique, LIBRO, and present our agent-based approach, BRT Agent,
which makes use of a fine-tuned Large Language Model (LLM) for code editing.
Our BRT Agent significantly outperforms LIBRO, achieving a 28% plausible BRT
generation rate, compared to 10% by LIBRO, on 80 human-reported bugs from
Google's internal issue tracker. We further investigate the practical value of
generated BRTs by integrating them with an Automated Program Repair (APR)
system at Google. Our results show that providing BRTs to the APR system
results in 30% more bugs with plausible fixes. Additionally, we introduce
Ensemble Pass Rate (EPR), a metric which leverages the generated BRTs to select
the most promising fixes from all fixes generated by APR system. Our evaluation
on EPR for Top-K and threshold-based fix selections demonstrates promising
results and trade-offs. For example, EPR correctly selects a plausible fix from
a pool of 20 candidates in 70% of cases, based on its top-1 ranking.

- **[2025-01] Evaluating Agent-based Program Repair at Google** [[arXiv](http://arxiv.org/abs/2501.07531v1)]
  *Pat Rondon, Renyao Wei, José Cambronero, Jürgen Cito, Aaron Sun, Siddhant Sanyam, Michele Tufano, Satish Chandra*
  Abstract: Agent-based program repair offers to automatically resolve complex bugs
end-to-end by combining the planning, tool use, and code generation abilities
of modern LLMs. Recent work has explored the use of agent-based repair
approaches on the popular open-source SWE-Bench, a collection of bugs from
highly-rated GitHub Python projects. In addition, various agentic approaches
such as SWE-Agent have been proposed to solve bugs in this benchmark. This
paper explores the viability of using an agentic approach to address bugs in an
enterprise context. To investigate this, we curate an evaluation set of 178
bugs drawn from Google's issue tracking system. This dataset spans both
human-reported (78) and machine-reported bugs (100).
  To establish a repair performance baseline on this benchmark, we implement
Passerine, an agent similar in spirit to SWE-Agent that can work within
Google's development environment. We show that with 20 trajectory samples and
Gemini 1.5 Pro, Passerine can produce a patch that passes bug tests (i.e.,
plausible) for 73% of machine-reported and 25.6% of human-reported bugs in our
evaluation set. After manual examination, we found that 43% of machine-reported
bugs and 17.9% of human-reported bugs have at least one patch that is
semantically equivalent to the ground-truth patch.
  These results establish a baseline on an industrially relevant benchmark,
which as we show, contains bugs drawn from a different distribution -- in terms
of language diversity, size, and spread of changes, etc. -- compared to those
in the popular SWE-Bench dataset.

- **[2024-11] An Empirical Study on LLM-based Agents for Automated Bug Fixing** [[arXiv](http://arxiv.org/abs/2411.10213v1)]
  *Xiangxin Meng, Zexiong Ma, Pengfei Gao, Chao Peng*
  Abstract: Large language models (LLMs) and LLM-based Agents have been applied to fix
bugs automatically, demonstrating the capability in addressing software defects
by engaging in development environment interaction, iterative validation and
code modification. However, systematic analysis of these agent and non-agent
systems remain limited, particularly regarding performance variations among
top-performing ones. In this paper, we examine seven proprietary and
open-source systems on the SWE-bench Lite benchmark for automated bug fixing.
We first assess each system's overall performance, noting instances solvable by
all or none of these sytems, and explore why some instances are uniquely solved
by specific system types. We also compare fault localization accuracy at file
and line levels and evaluate bug reproduction capabilities, identifying
instances solvable only through dynamic reproduction. Through analysis, we
concluded that further optimization is needed in both the LLM itself and the
design of Agentic flow to improve the effectiveness of the Agent in bug fixing.

- **[2024-11] LLMs as Continuous Learners: Improving the Reproduction of Defective
  Code in Software Issues** [[arXiv](http://arxiv.org/abs/2411.13941v1)]
  *Yalan Lin, Yingwei Ma, Rongyu Cao, Binhua Li, Fei Huang, Xiaodong Gu, Yongbin Li*
  Abstract: Reproducing buggy code is the first and crucially important step in issue
resolving, as it aids in identifying the underlying problems and validating
that generated patches resolve the problem. While numerous approaches have been
proposed for this task, they primarily address common, widespread errors and
struggle to adapt to unique, evolving errors specific to individual code
repositories. To fill this gap, we propose EvoCoder, a multi-agent continuous
learning framework for issue code reproduction. EvoCoder adopts a reflection
mechanism that allows the LLM to continuously learn from previously resolved
problems and dynamically refine its strategies to new emerging challenges. To
prevent experience bloating, EvoCoder introduces a novel hierarchical
experience pool that enables the model to adaptively update common and
repo-specific experiences. Our experimental results show a 20\% improvement in
issue reproduction rates over existing SOTA methods. Furthermore, integrating
our reproduction mechanism significantly boosts the overall accuracy of the
existing issue-resolving pipeline.

- **[2024-03] RepairAgent: An Autonomous, LLM-Based Agent for Program Repair** [[arXiv](http://arxiv.org/abs/2403.17134v2)]
  *Islem Bouzenia, Premkumar Devanbu, Michael Pradel*
  Abstract: Automated program repair has emerged as a powerful technique to mitigate the
impact of software bugs on system reliability and user experience. This paper
introduces RepairAgent, the first work to address the program repair challenge
through an autonomous agent based on a large language model (LLM). Unlike
existing deep learning-based approaches, which prompt a model with a fixed
prompt or in a fixed feedback loop, our work treats the LLM as an agent capable
of autonomously planning and executing actions to fix bugs by invoking suitable
tools. RepairAgent freely interleaves gathering information about the bug,
gathering repair ingredients, and validating fixes, while deciding which tools
to invoke based on the gathered information and feedback from previous fix
attempts. Key contributions that enable RepairAgent include a set of tools that
are useful for program repair, a dynamically updated prompt format that allows
the LLM to interact with these tools, and a finite state machine that guides
the agent in invoking the tools. Our evaluation on the popular Defects4J
dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164
bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM
imposes an average cost of 270,000 tokens per bug, which, under the current
pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To
the best of our knowledge, this work is the first to present an autonomous,
LLM-based agent for program repair, paving the way for future agent-based
techniques in software engineering.

- **[2023/07] Isolating Compiler Bugs by Generating Effective Witness Programs with Large Language Models** [[Link](https://ieeexplore.ieee.org/abstract/document/10521881)]
  *Haoxin Tu, Zhide Zhou, He Jiang, Imam Nur Bani Yusuf, Yuxian Li, Lingxiao Jiang*
Compiler bugs pose a significant threat to safety-critical applications, and promptly as well as effectively isolating these bugs is crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates the compiler bug isolation task. Existing compiler bug isolation approaches convert the problem into a test program mutation problem, but they are still limited by ineffective mutation strategies or high human effort requirements. Drawing inspiration from the recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT, in code generation, we propose a new approach named LLM4CBI to utilize LLMs to generate effective test programs for compiler bug isolation. However, using LLMs directly for test program mutation may not yield the desired results due to the challenges associated with formulating precise prompts and selecting specialized prompts. To overcome the challenges, three new components are designed in LLM4CBI. First, LLM4CBI utilizes a program complexity-guided prompt production component, which leverages data and control flow analysis to identify the most valuable variables and locations in programs for mutation. Second, LLM4CBI employs a memorized prompt selection component, which adopts reinforcement learning to select specialized prompts for mutating test programs continuously. Third, a test program validation component is proposed to select specialized feedback prompts to avoid repeating the same mistakes during the mutation process. Compared with state-of-the-art approaches over 120 real bugs from GCC and LLVM, our evaluation demonstrates the advantages of LLM4CBI: It can isolate 69.70%/21.74% and 24.44%/8.92% more bugs than DiWi and RecBi within Top-1/Top-5 ranked results. We also demonstrate that the LLMs component used in LLM4CBI can be easily replaced while still achieving reasonable results.



### Debugging Assistance
- **[2024/10] RGD: Multi-LLM Based Agent Debugger via Refinement and Generation Guidance.** [[Link](https://ieeexplore.ieee.org/abstract/document/10807454)]
  *Haolin Jin, Zechao Sun, Huaming Chen*
Large Language Models (LLMs) have shown incredible potential in code generation tasks, and recent research in prompt engineering have enhanced LLMs' understanding of textual information. However, ensuring the accuracy of generated code often requires extensive testing and validation by programmers. While LLMs can typically generate code based on task descriptions, their accuracy remains limited, especially for complex tasks that require a deeper understanding of both the problem statement and the code generation process. This limitation is primarily due to the LLMs' need to simultaneously comprehend text and generate syntactically and semantically correct code, without having the capability to automatically refine the code. In real-world software development, programmers rarely produce flawless code in a single attempt based on the task description alone, they rely on iterative feedback and debugging to refine their programs. Inspired by this process, we introduce a novel architecture of LLM-based agents for code generation and automatic debugging: Refinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based agent debugger that leverages three distinct LLM agents-Guide Agent, Debug Agent, and Feedback Agent. RGD decomposes the code generation task into multiple steps, ensuring a clearer workflow and enabling iterative code refinement based on self-reflection and feedback. Experimental results demonstrate that RGD exhibits remarkable code generation capabilities, achieving state-of-the-art performance with a 9.8% improvement on the HumanEval dataset and a 16.2% improvement on the MBPP dataset compared to the state-of-the-art approaches and traditional direct prompting approaches. We highlight the effectiveness of the RGD framework in enhancing LLMs' ability to generate and refine code autonomously.

- **[2024-10] Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent
  Approach** [[arXiv](http://arxiv.org/abs/2410.06949v2)]
  *Xuanming Zhang, Yuxuan Chen, Yuan Yuan, Minlie Huang*
  Abstract: In real world software development, improper or missing exception handling
can severely impact the robustness and reliability of code. Exception handling
mechanisms require developers to detect, capture, and manage exceptions
according to high standards, but many developers struggle with these tasks,
leading to fragile code. This problem is particularly evident in open source
projects and impacts the overall quality of the software ecosystem. To address
this challenge, we explore the use of large language models (LLMs) to improve
exception handling in code. Through extensive analysis, we identify three key
issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception
Types, and Distorted Handling Solutions. These problems are widespread across
real world repositories, suggesting that robust exception handling practices
are often overlooked or mishandled. In response, we propose Seeker, a multi
agent framework inspired by expert developer strategies for exception handling.
Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist
LLMs in detecting, capturing, and resolving exceptions more effectively. Our
work is the first systematic study on leveraging LLMs to enhance exception
handling practices, providing valuable insights for future improvements in code
reliability.

- **[2024-08] COAST: Enhancing the Code Debugging Ability of LLMs through
  Communicative Agent Based Data Synthesis** [[arXiv](http://arxiv.org/abs/2408.05006v2)]
  *Weiqing Yang, Hanbin Wang, Zhenghao Liu, Xinze Li, Yukun Yan, Shuo Wang, Yu Gu, Minghe Yu, Zhiyuan Liu, Ge Yu*
  Abstract: Code debugging is a vital stage of software development, essential for
ensuring the reliability and performance of Large Language Models (LLMs) in
code generation task. Human debugging typically follows a multi-stage process,
which includes Bug Localization, Bug Identification, Code Repair, and Code
Recognition. However, existing code debugging benchmarks predominantly focus on
the Code Repair stage, which offers only a limited perspective on evaluating
the debugging capabilities of LLMs. In this paper, we introduce DEBUGEVAL, a
comprehensive benchmark for evaluating the debugging abilities of LLMs by
emulating the multi-stage human debugging process. Through evaluating on
DEBUGEVAL, we observe that 7B-scale models consistently underperform compared
to their larger counterparts, highlighting their limitations in comprehending
code semantics. In this case, we propose the COmmunicative Agent-based data
SynThesis (COAST) framework, which employs a multi-agent system to generate
high-quality training data for supervised fine-tuning (SFT). Experimental
results demonstrate that COAST-generated data outperform human-curated and
GPT-4-generated data, enabling 7B-scale LLMs to achieve debugging performance
comparable to GPT-3.5.

- **[2024-04] A Unified Debugging Approach via LLM-Based Multi-Agent Synergy** [[arXiv](http://arxiv.org/abs/2404.17153v2)]
  *Cheryl Lee, Chunqiu Steven Xia, Longji Yang, Jen-tse Huang, Zhouruixin Zhu, Lingming Zhang, Michael R. Lyu*
  Abstract: Software debugging is a time-consuming endeavor involving a series of steps,
such as fault localization and patch generation, each requiring thorough
analysis and a deep understanding of the underlying logic. While large language
models (LLMs) demonstrate promising potential in coding tasks, their
performance in debugging remains limited. Current LLM-based methods often focus
on isolated steps and struggle with complex bugs. In this paper, we propose the
first end-to-end framework, FixAgent, for unified debugging through multi-agent
synergy. It mimics the entire cognitive processes of developers, with each
agent specialized as a particular component of this process rather than
mirroring the actions of an independent expert as in previous multi-agent
systems. Agents are coordinated through a three-level design, following a
cognitive model of debugging, allowing adaptive handling of bugs with varying
complexities. Experiments on extensive benchmarks demonstrate that FixAgent
significantly outperforms state-of-the-art repair methods, fixing 1.25$\times$
to 2.56$\times$ bugs on the repo-level benchmark, Defects4J. This performance
is achieved without requiring ground-truth root-cause code statements, unlike
the baselines. Our source code is available on
https://github.com/AcceptePapier/UniDebugger.

- **[2024-03] ChatDBG: An AI-Powered Debugging Assistant** [[arXiv](http://arxiv.org/abs/2403.16354v2)]
  *Kyla Levin, Nicolas van Kempen, Emery D. Berger, Stephen N. Freund*
  Abstract: Debugging is a critical but challenging task for programmers. This paper
proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large
language models (LLMs) to significantly enhance the capabilities and
user-friendliness of conventional debuggers. ChatDBG lets programmers engage in
a collaborative dialogue with the debugger, allowing them to pose complex
questions about program state, perform root cause analysis for crashes or
assertion failures, and explore open-ended queries like `why is x null?'. To
handle these queries, ChatDBG grants the LLM autonomy to "take the wheel": it
can act as an independent agent capable of querying and controlling the
debugger to navigate through stacks and inspect program state. It then reports
its findings and yields back control to the programmer. Our ChatDBG prototype
integrates with standard debuggers including LLDB and GDB for native code and
Pdb for Python. Our evaluation across a diverse set of code, including C/C++
code with known bugs and a suite of Python code including standalone scripts
and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root
causes, explain bugs, and generate accurate fixes for a wide range of
real-world errors. For the Python programs, a single query led to an actionable
bug fix 67% of the time; one additional follow-up query increased the success
rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded
roughly 50,000 times.

- **[2024-03] Untangling Knots: Leveraging LLM for Error Resolution in Computational
  Notebooks** [[arXiv](http://arxiv.org/abs/2405.01559v1)]
  *Konstantin Grotov, Sergey Titov, Yaroslav Zharov, Timofey Bryksin*
  Abstract: Computational notebooks became indispensable tools for research-related
development, offering unprecedented interactivity and flexibility in the
development process. However, these benefits come at the cost of
reproducibility and an increased potential for bugs. There are many tools for
bug fixing; however, they are generally targeted at the classical linear code.
With the rise of code-fluent Large Language Models, a new stream of smart
bug-fixing tools has emerged. However, the applicability of those tools is
still problematic for non-linear computational notebooks. In this paper, we
propose a potential solution for resolving errors in computational notebooks
via an iterative LLM-based agent. We discuss the questions raised by this
approach and share a novel dataset of computational notebooks containing bugs
to facilitate the research of the proposed approach.

- **[2024-02] Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides** [[arXiv](http://arxiv.org/abs/2402.17531v2)]
  *Kaikai An, Fangkai Yang, Junting Lu, Liqun Li, Zhixing Ren, Hao Huang, Lu Wang, Pu Zhao, Yu Kang, Hua Ding, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang*
  Abstract: Effective incident management is pivotal for the smooth operation of
enterprises-level cloud services. In order to expedite incident mitigation,
service teams compile troubleshooting knowledge into Troubleshooting Guides
(TSGs) accessible to on-call engineers (OCEs). While automated pipelines are
enabled to resolve the most frequent and easy incidents, there still exist
complex incidents that require OCEs' intervention. However, TSGs are often
unstructured and incomplete, which requires manual interpretation by OCEs,
leading to on-call fatigue and decreased productivity, especially among
new-hire OCEs. In this work, we propose Nissist which leverages TSGs and
incident mitigation histories to provide proactive suggestions, reducing human
intervention. Leveraging Large Language Models (LLM), Nissist extracts insights
from unstructured TSGs and historical incident mitigation discussions, forming
a comprehensive knowledge base. Its multi-agent system design enhances
proficiency in precisely discerning user queries, retrieving relevant
information, and delivering systematic plans consecutively. Through our user
case and experiment, we demonstrate that Nissist significant reduce Time to
Mitigate (TTM) in incident mitigation, alleviating operational burdens on OCEs
and improving service reliability. Our demo is available at
https://aka.ms/nissist_demo.

- **[2023-12] E&V: Prompting Large Language Models to Perform Static Analysis by
  Pseudo-code Execution and Verification** [[arXiv](https://arxiv.org/pdf/2312.08477)]
  *Yu Hao, Weiteng Chen, Ziqiao Zhou, Weidong Cui*
  Abstract: Static analysis, the process of examining code without executing it, is
crucial for identifying software issues. Yet, static analysis is hampered by
its complexity and the need for customization for different targets.
Traditional static analysis tools require extensive human effort and are often
limited to specific target programs and programming languages. Recent
advancements in Large Language Models (LLMs), such as GPT-4 and Llama, offer
new capabilities for software engineering tasks. However, their application in
static analysis, especially in understanding complex code structures, remains
under-explored. This paper introduces a novel approach named E&V , which
leverages LLMs to perform static analysis. Specifically, E&V employs LLMs to
simulate the execution of pseudo-code, effectively conducting static analysis
encoded in the pseudo-code with minimal human effort, thereby improving the
accuracy of results. E&V includes a verification process for pseudo-code
execution without needing an external oracle. This process allows E&V to
mitigate hallucinations of LLMs and enhance the accuracy of static analysis
results. We have implemented E&V in a prototype tool designed for triaging
crashes through backward taint analysis. This prototype, paired with GPT-4-32k,
has been applied to triage 170 recently fixed Linux kernel bugs across seven
bug categories. Our experiments demonstrate that the prototype correctly
identifies the blamed function in 81.2% of the cases. Additionally, we observe
that our novel verification process significantly improves the accuracy,
increasing it from 28.2% to 81.2%.

- **[2023/10] RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models** [[Link](https://dl.acm.org/doi/abs/10.1145/3627673.3680016)]
  *Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Jihong Wang, Fengbin Yin, Lunting Fan, Lingfei Wu, Qingsong Wen*
Large language model (LLM) applications in cloud root cause analysis (RCA) have been actively explored recently. However, current methods are still reliant on manual workflow settings and do not unleash LLMs' decision-making and environment interaction capabilities. We present RCAgent, a tool-augmented LLM autonomous agent framework for practical and privacy-aware industrial RCA usage. Running on an internally deployed model rather than GPT families, RCAgent is capable of free-form data collection and comprehensive analysis with tools. Our framework combines a variety of enhancements, including a unique Self-Consistency for action trajectories, and a suite of methods for context management, stabilization, and importing domain knowledge. Our experiments show RCAgent's evident and consistent superiority over ReAct across all aspects of RCA -- predicting root causes, solutions, evidence, and responsibilities -- and tasks covered or uncovered by current rules, as validated by both automated metrics and human evaluations. Furthermore, RCAgent has already been integrated into the diagnosis and issue discovery workflow of the Real-time Compute Platform for Apache Flink of Alibaba Cloud.

- **[2023/07] Extending the Frontier of ChatGPT: Code Generation and Debugging.** [[Link](https://ieeexplore.ieee.org/document/10698405)]
  *Fardin Ahsan Sakib, Saadat Hasan Khan, A. H. M. Rezaul Karim*
Large-scale language models (LLMs) have emerged as a groundbreaking innovation in the realm of question-answering and conversational agents. These models, leveraging different deep learning architectures such as Transformers, are trained on vast corpora to predict sentences based on given queries. Among these LLMs, ChatGPT, developed by OpenAI, has ushered in a new era by utilizing artificial intelligence (AI) to tackle diverse problem domains, ranging from composing essays and biographies to solving intricate mathematical integrals. The versatile applications enabled by ChatGPT offer immense value to users. However, assessing the performance of ChatGPT's output poses a challenge, particularly in scenarios where queries lack clear objective criteria for correctness. For instance, evaluating the quality of generated essays becomes arduous and relies heavily on manual labor, in stark contrast to evaluating solutions to well-defined, closed-ended questions such as mathematical problems. This research paper delves into the efficacy of ChatGPT in solving programming problems, examining both the correctness and the efficiency of its solution in terms of time and memory complexity. The research reveals a commendable overall success rate of 71.875\%, denoting the proportion of problems for which ChatGPT was able to provide correct solutions that successfully satisfied all the test cases present in Leetcode. It exhibits strengths in structured problems and shows a linear correlation between its success rate and problem acceptance rates. However, it struggles to improve solutions based on feedback, pointing to potential shortcomings in debugging tasks. These findings provide a compact yet insightful glimpse into ChatGPT's capabilities and areas for improvement.

- **[2023-06] InterCode: Standardizing and Benchmarking Interactive Coding with
  Execution Feedback** [[arXiv](https://arxiv.org/pdf/2306.14898)]
  *John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao*
  Abstract: Humans write code in a fundamentally interactive manner and rely on constant
execution feedback to correct errors, resolve ambiguities, and decompose tasks.
While LLMs have recently exhibited promising coding capabilities, current
coding benchmarks mostly consider a static instruction-to-code sequence
transduction process, which has the potential for error propagation and a
disconnect between the generated code and its final execution environment. To
address this gap, we introduce InterCode, a lightweight, flexible, and
easy-to-use framework of interactive coding as a standard reinforcement
learning (RL) environment, with code as actions and execution feedback as
observations. Our framework is language and platform agnostic, uses
self-contained Docker environments to provide safe and reproducible execution,
and is compatible out-of-the-box with traditional seq2seq coding methods, while
enabling the development of new methods for interactive code generation. We use
InterCode to create three interactive code environments with Bash, SQL, and
Python as action spaces, leveraging data from the static NL2Bash, Spider, and
MBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating
multiple state-of-the-art LLMs configured with different prompting strategies
such as ReAct and Plan & Solve. Our results showcase the benefits of
interactive code generation and demonstrate that InterCode can serve as a
challenging benchmark for advancing code understanding and generation
capabilities. InterCode is designed to be easily extensible and can even be
used to create new tasks such as Capture the Flag, a popular coding puzzle that
is inherently multi-step and involves multiple programming languages. Project
site with code and data: https://intercode-benchmark.github.io



### Code Quality Assurance
- **[2025-01] Code Readability in the Age of Large Language Models: An Industrial Case
  Study from Atlassian** [[arXiv](http://arxiv.org/abs/2501.11264v1)]
  *Wannita Takerngsaksiri, Micheal Fu, Chakkrit Tantithamthavorn, Jirat Pasuksmit, Kun Chen, Ming Wu*
  Abstract: Programmers spend a significant amount of time reading code during the
software development process. This trend is amplified by the emergence of large
language models (LLMs) that automatically generate code. However, little is
known about the readability of the LLM-generated code and whether it is still
important from practitioners' perspectives in this new era. In this paper, we
conduct a survey to explore the practitioners' perspectives on code readability
in the age of LLMs and investigate the readability of our LLM-based software
development agents framework, HULA, by comparing its generated code with
human-written code in real-world scenarios. Overall, the findings underscore
that (1) readability remains a critical aspect of software development; (2) the
readability of our LLM-generated code is comparable to human-written code,
fostering the establishment of appropriate trust and driving the broad adoption
of our LLM-powered software development platform.

- **[2025-01] RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing** [[arXiv](http://arxiv.org/abs/2501.18160v1)]
  *Jinyao Guo, Chengpeng Wang, Xiangzhe Xu, Zian Su, Xiangyu Zhang*
  Abstract: Code auditing is a code review process with the goal of finding bugs. Large
Language Models (LLMs) have shown substantial potential in this task, offering
the ability to analyze programs without compilation and enabling customized bug
detection following specified prompts. However, applying LLMs to
repository-level code auditing presents notable challenges. The inherent
context limits and hallucinations of LLMs can lead to the low quality of bug
reports. Meanwhile, the large size of software repositories introduces
substantial time and token costs, hindering efficiency and scalability in
real-world scenarios.
  This work introduces an autonomous LLM-agent, RepoAudit, designed to enable
precise and efficient repository-level code auditing. Equipped with the agent
memory, RepoAudit explores the code repository on demand, analyzing data-flow
facts along different feasible program paths in individual functions. It also
introduces the validator to check the data-flow facts for hallucination
mitigation and examine the satisfiability of path conditions of potential buggy
paths, which enables RepoAudit to discard false positives in the code auditing.
Our experiment shows that RepoAudit powered by Claude 3.5 Sonnet successfully
finds 38 true bugs in 15 real-world systems, consuming 0.44 hours and $2.54 per
project on average.

- **[2024-12] Automated Code Review In Practice** [[arXiv](http://arxiv.org/abs/2412.18531v2)]
  *Umut Cihan, Vahid Haratian, Arda İçöz, Mert Kaan Gül, Ömercan Devran, Emircan Furkan Bayendur, Baykal Mehmet Uçar, Eray Tüzün*
  Abstract: Code review is a widespread practice to improve software quality and transfer
knowledge. It is often seen as time-consuming due to the need for manual effort
and potential delays. Several AI-assisted tools, such as Qodo, GitHub Copilot,
and Coderabbit, provide automated reviews using large language models (LLMs).
The effects of such tools in the industry are yet to be examined.
  This study examines the impact of LLM-based automated code review tools in an
industrial setting. The study was conducted within a software development
environment that adopted an AI-assisted review tool (based on open-source Qodo
PR Agent). Around 238 practitioners across ten projects had access to the tool.
We focused on three projects with 4,335 pull requests, 1,568 of which underwent
automated reviews. Data collection comprised three sources: (1) a quantitative
analysis of pull request data, including comment labels indicating whether
developers acted on the automated comments, (2) surveys sent to developers
regarding their experience with reviews on individual pull requests, and (3) a
broader survey of 22 practitioners capturing their general opinions on
automated reviews.
  73.8% of automated comments were resolved. However, the average pull request
closure duration increased from five hours 52 minutes to eight hours 20
minutes, with varying trends across projects. Most practitioners reported a
minor improvement in code quality due to automated reviews.
  The LLM-based tool proved useful in software development, enhancing bug
detection, increasing awareness of code quality, and promoting best practices.
However, it also led to longer pull request closure times and introduced
drawbacks like faulty reviews, unnecessary corrections, and irrelevant
comments.

- **[2024-06] Identifying Performance-Sensitive Configurations in Software Systems
  through Code Analysis with LLM Agents** [[arXiv](http://arxiv.org/abs/2406.12806v1)]
  *Zehao Wang, Dong Jae Kim, Tse-Hsun Chen*
  Abstract: Configuration settings are essential for tailoring software behavior to meet
specific performance requirements. However, incorrect configurations are
widespread, and identifying those that impact system performance is challenging
due to the vast number and complexity of possible settings. In this work, we
present PerfSense, a lightweight framework that leverages Large Language Models
(LLMs) to efficiently identify performance-sensitive configurations with
minimal overhead. PerfSense employs LLM agents to simulate interactions between
developers and performance engineers using advanced prompting techniques such
as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of
seven open-source Java systems demonstrates that PerfSense achieves an average
accuracy of 64.77% in classifying performance-sensitive configurations,
outperforming both our LLM baseline (50.36%) and the previous state-of-the-art
method (61.75%). Notably, our prompt chaining technique improves recall by 10%
to 30% while maintaining similar precision levels. Additionally, a manual
analysis of 362 misclassifications reveals common issues, including LLMs'
misunderstandings of requirements (26.8%). In summary, PerfSense significantly
reduces manual effort in classifying performance-sensitive configurations and
offers valuable insights for future LLM-based code analysis research.

- **[2024-04] AI-powered Code Review with LLMs: Early Results** [[arXiv](http://arxiv.org/abs/2404.18496v1)]
  *Zeeshan Rasheed, Malik Abdul Sami, Muhammad Waseem, Kai-Kristian Kemell, Xiaofeng Wang, Anh Nguyen, Kari Systä, Pekka Abrahamsson*
  Abstract: In this paper, we present a novel approach to improving software quality and
efficiency through a Large Language Model (LLM)-based model designed to review
code and identify potential issues. Our proposed LLM-based AI agent model is
trained on large code repositories. This training includes code reviews, bug
reports, and documentation of best practices. It aims to detect code smells,
identify potential bugs, provide suggestions for improvement, and optimize the
code. Unlike traditional static code analysis tools, our LLM-based AI agent has
the ability to predict future potential risks in the code. This supports a dual
goal of improving code quality and enhancing developer education by encouraging
a deeper understanding of best practices and efficient coding techniques.
Furthermore, we explore the model's effectiveness in suggesting improvements
that significantly reduce post-release bugs and enhance code review processes,
as evidenced by an analysis of developer sentiment toward LLM feedback. For
future work, we aim to assess the accuracy and efficiency of LLM-generated
documentation updates in comparison to manual methods. This will involve an
empirical study focusing on manually conducted code reviews to identify code
smells and bugs, alongside an evaluation of best practice documentation,
augmented by insights from developer discussions and code reviews. Our goal is
to not only refine the accuracy of our LLM-based tool but also to underscore
its potential in streamlining the software development lifecycle through
proactive code improvement and education.

- **[2024/02] CodeAgent: Autonomous Communicative Agents for Code Review.** [[Link](https://aclanthology.org/2024.emnlp-main.632.pdf)]
  *Xunzhu Tang, Kisub Kim, Yewei Song, Cedric Lothritz, Bei Li, Saad Ezzini, Haoye Tian, Jacques Klein, Tegawende F. Bissyande*
Code review, which aims at ensuring the overall quality and reliability of software, is a cornerstone of software development. Unfortunately, while crucial, Code review is a labor-intensive process that the research community is looking to automate. Existing automated methods rely on single input-output generative models and thus generally struggle to emulate the collaborative nature of code review. This work introduces \tool{}, a novel multi-agent Large Language Model (LLM) system for code review automation. CodeAgent incorporates a supervisory agent, QA-Checker, to ensure that all the agents' contributions address the initial review question. We evaluated CodeAgent on critical code review tasks: (1) detect inconsistencies between code changes and commit messages, (2) identify vulnerability introductions, (3) validate code style adherence, and (4) suggest code revision. The results demonstrate CodeAgent's effectiveness, contributing to a new state-of-the-art in code review automation. Our data and code are publicly available (\url{this https URL}).

- **[2023-10] Static Code Analysis in the AI Era: An In-depth Exploration of the
  Concept, Function, and Potential of Intelligent Code Analysis Agents** [[arXiv](https://arxiv.org/pdf/2310.08837)]
  *Gang Fan, Xiaoheng Xie, Xunjin Zheng, Yinan Liang, Peng Di*
  Abstract: The escalating complexity of software systems and accelerating development
cycles pose a significant challenge in managing code errors and implementing
business logic. Traditional techniques, while cornerstone for software quality
assurance, exhibit limitations in handling intricate business logic and
extensive codebases. To address these challenges, we introduce the Intelligent
Code Analysis Agent (ICAA), a novel concept combining AI models, engineering
process designs, and traditional non-AI components. The ICAA employs the
capabilities of large language models (LLMs) such as GPT-3 or GPT-4 to
automatically detect and diagnose code errors and business logic
inconsistencies. In our exploration of this concept, we observed a substantial
improvement in bug detection accuracy, reducing the false-positive rate to 66\%
from the baseline's 85\%, and a promising recall rate of 60.8\%. However, the
token consumption cost associated with LLMs, particularly the average cost for
analyzing each line of code, remains a significant consideration for widespread
adoption. Despite this challenge, our findings suggest that the ICAA holds
considerable potential to revolutionize software quality assurance,
significantly enhancing the efficiency and accuracy of bug detection in the
software development process. We hope this pioneering work will inspire further
research and innovation in this field, focusing on refining the ICAA concept
and exploring ways to mitigate the associated costs.

- **[2023/07] Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures** [[Link](https://www.scitepress.org/PublicationsDetail.aspx?ID=1Ne2ZQRkAVs=&t=1)]
  *Sayed Erfan Arefin, Tasnia Ashrafi Heya, Hasan Al-Qudah, Ynes Ineza, Abdul Serwadda*
The transformative influence of Large Language Models (LLMs) is profoundly reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT distinguishes itself within these models, demonstrating remarkable performance in multi-turn conversations and exhibiting code proficiency across an array of languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges. Our focus is on the python programming language and problems centered on data structures and algorithms, two topics at the very foundations of Computer Science. We evaluate ChatGPT for its ability to generate correct solutions to the problems fed to it, its code quality, and nature of run-time errors thrown by its code. Where ChatGPT code successfully executes, but fails to solve the problem at hand, we look into patterns in the test cases passed in order to gain some insights into how wrong ChatGPT code is in these kinds of situations. To infer whether ChatGPT might have directly memorized some of the data that was used to train it, we methodically design an experiment to investigate this phenomena. Making comparisons with human performance whenever feasible, we investigate all the above questions from the context of both its underlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics within the main topics, and on problems having varying degrees of difficulty.



## Software Design and Architecture

### System and Software Design
- **[2025-02] Every Software as an Agent: Blueprint and Case Study** [[arXiv](http://arxiv.org/abs/2502.04747v1)]
  *Mengwei Xu*
  Abstract: The rise of (multimodal) large language models (LLMs) has shed light on
software agent -- where software can understand and follow user instructions in
natural language. However, existing approaches such as API-based and GUI-based
agents are far from satisfactory at accuracy and efficiency aspects. Instead,
we advocate to endow LLMs with access to the software internals (source code
and runtime context) and the permission to dynamically inject generated code
into software for execution. In such a whitebox setting, one may better
leverage the software context and the coding ability of LLMs. We then present
an overall design architecture and case studies on two popular web-based
desktop applications. We also give in-depth discussion of the challenges and
future directions. We deem that such a new paradigm has the potential to
fundamentally overturn the existing software agent design, and finally creating
a digital world in which software can comprehend, operate, collaborate, and
even think to meet complex user needs.

- **[2025-01] RTLSquad: Multi-Agent Based Interpretable RTL Design** [[arXiv](http://arxiv.org/abs/2501.05470v1)]
  *Bowei Wang, Qi Xiong, Zeqing Xiang, Lei Wang, Renzhi Chen*
  Abstract: Optimizing Register-Transfer Level (RTL) code is crucial for improving
hardware PPA performance. Large Language Models (LLMs) offer new approaches for
automatic RTL code generation and optimization. However, existing methods often
lack decision interpretability (sufficient, understandable justification for
decisions), making it difficult for hardware engineers to trust the generated
results, thus preventing these methods from being integrated into the design
process. To address this, we propose RTLSquad, a novel LLM-Based Multi-Agent
system for interpretable RTL code generation. RTLSquad divides the design
process into exploration, implementation, and verification & evaluation stages
managed by specialized agent squads, generating optimized RTL code through
inter-agent collaboration, and providing decision interpretability through the
communication process. Experiments show that RTLSquad excels in generating
functionally correct RTL code and optimizing PPA performance, while also having
the capability to provide decision paths, demonstrating the practical value of
our system.

- **[2024-11] An Evaluation-Driven Approach to Designing LLM Agents: Process and
  Architecture** [[arXiv](http://arxiv.org/abs/2411.13768v1)]
  *Boming Xia, Qinghua Lu, Liming Zhu, Zhenchang Xing, Dehai Zhao, Hao Zhang*
  Abstract: The advent of Large Language Models (LLMs) has enabled the development of LLM
agents capable of autonomously achieving under-specified goals and continuously
evolving through post-deployment improvement, sometimes without requiring code
or model updates. Conventional approaches, such as pre-defined test cases and
code/model redevelopment pipelines, are inadequate for addressing the unique
challenges of LLM agent development, particularly in terms of quality and risk
control. This paper introduces an evaluation-driven design approach, inspired
by test-driven development, to address these challenges. Through a multivocal
literature review (MLR), we synthesize existing LLM evaluation methods and
propose a novel process model and reference architecture specifically designed
for LLM agents. The proposed approach integrates online and offline evaluations
to support adaptive runtime adjustments and systematic offline redevelopment,
improving runtime pipelines, artifacts, system architecture, and LLMs by
continuously incorporating evaluation results, including fine-grained feedback
from human and AI evaluators.

- **[2024-08] CodexGraph: Bridging Large Language Models and Code Repositories via
  Code Graph Databases** [[arXiv](https://arxiv.org/pdf/2408.03910)]
  *Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Shieh, Wenmeng Zhou*
  Abstract: Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval
and MBPP, but struggle with handling entire code repositories. This challenge
has prompted research on enhancing LLM-codebase interaction at a repository
scale. Current solutions rely on similarity-based retrieval or manual tools and
APIs, each with notable drawbacks. Similarity-based retrieval often has low
recall in complex tasks, while manual tools and APIs are typically
task-specific and require expert knowledge, reducing their generalizability
across diverse code tasks and real-world applications. To mitigate these
limitations, we introduce CodexGraph, a system that integrates LLM agents with
graph database interfaces extracted from code repositories. By leveraging the
structural properties of graph databases and the flexibility of the graph query
language, CodexGraph enables the LLM agent to construct and execute queries,
allowing for precise, code structure-aware context retrieval and code
navigation. We assess CodexGraph using three benchmarks: CrossCodeEval,
SWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding
applications. With a unified graph database schema, CodexGraph demonstrates
competitive performance and potential in both academic and real-world
environments, showcasing its versatility and efficacy in software engineering.
Our application demo:
https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.

- **[2024-08] SpecRover: Code Intent Extraction via LLMs** [[arXiv](http://arxiv.org/abs/2408.02232v4)]
  *Haifeng Ruan, Yuntong Zhang, Abhik Roychoudhury*
  Abstract: Autonomous program improvement typically involves automatically producing bug
fixes and feature additions. Such program improvement can be accomplished by a
combination of large language model (LLM) and program analysis capabilities, in
the form of an LLM agent. Since program repair or program improvement typically
requires a specification of intended behavior - specification inference can be
useful for producing high quality program patches. In this work, we examine
efficient and low-cost workflows for iterative specification inference within
an LLM agent. Given a GitHub issue to be resolved in a software project, our
goal is to conduct iterative code search accompanied by specification inference
- thereby inferring intent from both the project structure and behavior. The
intent thus captured is examined by a reviewer agent with the goal of vetting
the patches as well as providing a measure of confidence in the vetted patches.
Our approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent
AutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub
issues, it shows more than 50% improvement in efficacy over AutoCodeRover.
Compared to the open-source agents available, our work shows modest cost ($0.65
per issue) in resolving an average GitHub issue in SWE-Bench lite. The
production of explanation by SpecRover allows for a better "signal" to be given
to the developer, on when the suggested patches can be accepted with
confidence. SpecRover also seeks to demonstrate the continued importance of
specification inference in automated program repair, even as program repair
technologies enter the LLM era.

- **[2024-06] How to Understand Whole Software Repository?** [[arXiv](http://arxiv.org/abs/2406.01422v1)]
  *Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, Yongbin Li*
  Abstract: Recently, Large Language Model (LLM) based agents have advanced the
significant development of Automatic Software Engineering (ASE). Although
verified effectiveness, the designs of the existing methods mainly focus on the
local information of codes, e.g., issues, classes, and functions, leading to
limitations in capturing the global context and interdependencies within the
software system. From the practical experiences of the human SE developers, we
argue that an excellent understanding of the whole repository will be the
critical path to ASE. However, understanding the whole repository raises
various challenges, e.g., the extremely long code input, the noisy code
information, the complex dependency relationships, etc. To this end, we develop
a novel ASE method named RepoUnderstander by guiding agents to comprehensively
understand the whole repositories. Specifically, we first condense the critical
information of the whole repository into the repository knowledge graph in a
top-to-down mode to decrease the complexity of repository. Subsequently, we
empower the agents the ability of understanding whole repository by proposing a
Monte Carlo tree search based repository exploration strategy. In addition, to
better utilize the repository-level knowledge, we guide the agents to
summarize, analyze, and plan. Then, they can manipulate the tools to
dynamically acquire information and generate the patches to solve the
real-world GitHub issues. Extensive experiments demonstrate the superiority and
effectiveness of the proposed RepoUnderstander. It achieved 18.5\% relative
improvement on the SWE-bench Lite benchmark compared to SWE-agent.

- **[2024/06] MASAI: Modular Architecture for Software-engineering AI Agents** [[Link](https://openreview.net/attachment?id=NSINt8lLYB&name=pdf)]
  *Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, Nagarajan Natarajan*
A common method to solve complex problems in software engineering, is to divide the problem into multiple sub-problems. Inspired by this, we propose a Modular Architecture for Software-engineering AI (MASAI) agents, where different LLM-powered sub-agents are instantiated with well-defined objectives and strategies tuned to achieve those objectives. Our modular architecture offers several advantages: (1) employing and tuning different problem-solving strategies across sub-agents, (2) enabling sub-agents to gather information from different sources scattered throughout a repository, and (3) avoiding unnecessarily long trajectories which inflate costs and add extraneous context. MASAI enabled us to achieve the highest performance (28.33% resolution rate) on the popular and highly challenging SWE-bench Lite dataset consisting of 300 GitHub issues from 11 Python repositories. We conduct a comprehensive evaluation of MASAI relative to other agentic methods and analyze the effects of our design decisions and their contribution to the success of MASAI.

- **[2024/03] Generation of Asset Administration Shell with Large Language Model Agents: Toward Semantic Interoperability in Digital Twins in the Context of Industry 4.0** [[Link](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10559483)]
  *Yuchen Xia, Zhewen Xiao, Nasser Jazdi, Michael Weyrich*
This research introduces a novel approach for achieving semantic interoperability in digital twins and assisting the creation of Asset Administration Shell (AAS) as digital twin model within the context of Industry 4.0. The foundational idea of our research is that the communication based on semantics and the generation of meaningful textual data are directly linked, and we posit that these processes are equivalent if the exchanged information can be serialized in text form. Based on this, we construct a "semantic node" data structure in our research to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process the "semantic node" and generate standardized digital twin models from raw textual data collected from datasheets describing technical assets. Our evaluation demonstrates an effective generation rate of 62-79%, indicating a substantial proportion of the information from the source text can be translated error-free to the target digital twin instance model with the generative capability of large language models. This result has a direct application in the context of Industry 4.0, and the designed system is implemented as a data model generation tool for reducing the manual effort in creating AAS model. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM systems for interpreting technical concepts and translating data. Our findings emphasize LLMs' capability to automate AAS instance creation and contribute to the broader field of semantic interoperability for digital twins in industrial applications. The prototype implementation and evaluation results are presented on our GitHub Repository: this https URL.

- **[2024/01] Exploring the Potential of Large Language Models in Self-adaptive Systems.** [[Link](https://dl.acm.org/doi/10.1145/3643915.3644088)]
  *Jialong Li, Mingyue Zhang, Nianyu Li, Danny Weyns, Zhi Jin, Kenji Tei*
Large Language Models (LLMs), with their abilities in knowledge acquisition and reasoning, can potentially enhance the various aspects of Self-adaptive Systems (SAS). Yet, the potential of LLMs in SAS remains largely unexplored and ambiguous, due to the lack of literature from flagship conferences or journals in the field, such as SEAMS and TAAS. The interdisciplinary nature of SAS suggests that drawing and integrating ideas from related fields, such as software engineering and autonomous agents, could unveil innovative research directions for LLMs within SAS. To this end, this paper reports the results of a literature review of studies in relevant fields, summarizes and classifies the studies relevant to SAS, and outlines their potential to specific aspects of SAS.

- **[2023/11] Towards Responsible Generative AI: A Reference Architecture for Designing Foundation Model based Agents** [[Link](https://ieeexplore.ieee.org/abstract/document/10628258)]
  *Qinghua Lu, Liming Zhu, Xiwei Xu, Zhenchang Xing, Stefan Harrer, Jon Whittle*
Foundation models, such as large language models (LLMs), have been widely recognised as transformative AI technologies due to their capabilities to understand and generate content, including plans with reasoning capabilities. Foundation model based agents derive their autonomy from the capabilities of foundation models, which enable them to autonomously break down a given goal into a set of manageable tasks and orchestrate task execution to meet the goal. Despite the huge efforts put into building foundation model based agents, the architecture design of the agents has not yet been systematically explored. Also, while there are significant benefits of using agents for planning and execution, there are serious considerations regarding responsible AI related software quality attributes, such as security and accountability. Therefore, this paper presents a pattern-oriented reference architecture that serves as guidance when designing foundation model based agents. We evaluate the completeness and utility of the proposed reference architecture by mapping it to the architecture of two real-world agents.

- **[2023-10] Coding by Design: GPT-4 empowers Agile Model Driven Development** [[arXiv](http://arxiv.org/abs/2310.04304v1)]
  *Ahmed R. Sadik, Sebastian Brulin, Markus Olhofer*
  Abstract: Generating code from a natural language using Large Language Models (LLMs)
such as ChatGPT, seems groundbreaking. Yet, with more extensive use, it's
evident that this approach has its own limitations. The inherent ambiguity of
natural language presents challenges for complex software designs. Accordingly,
our research offers an Agile Model-Driven Development (MDD) approach that
enhances code auto-generation using OpenAI's GPT-4. Our work emphasizes
"Agility" as a significant contribution to the current MDD method, particularly
when the model undergoes changes or needs deployment in a different programming
language. Thus, we present a case-study showcasing a multi-agent simulation
system of an Unmanned Vehicle Fleet. In the first and second layer of our
approach, we constructed a textual representation of the case-study using
Unified Model Language (UML) diagrams. In the next layer, we introduced two
sets of constraints that minimize model ambiguity. Object Constraints Language
(OCL) is applied to fine-tune the code constructions details, while FIPA
ontology is used to shape communication semantics and protocols. Ultimately,
leveraging GPT-4, our last layer auto-generates code in both Java and Python.
The Java code is deployed within the JADE framework, while the Python code is
deployed in PADE framework. Concluding our research, we engaged in a
comprehensive evaluation of the generated code. From a behavioural standpoint,
the auto-generated code aligned perfectly with the expected UML sequence
diagram. Structurally, we compared the complexity of code derived from UML
diagrams constrained solely by OCL to that influenced by both OCL and
FIPA-ontology. Results indicate that ontology-constrained model produce
inherently more intricate code, but it remains manageable and low-risk for
further testing and maintenance.



### Modeling and Design Representation
- **[2024-12] AnalogXpert: Automating Analog Topology Synthesis by Incorporating
  Circuit Design Expertise into Large Language Models** [[arXiv](http://arxiv.org/abs/2412.19824v1)]
  *Haoyi Zhang, Shizhao Sun, Yibo Lin, Runsheng Wang, Jiang Bian*
  Abstract: Analog circuits are crucial in modern electronic systems, and automating
their design has attracted significant research interest. One of major
challenges is topology synthesis, which determines circuit components and their
connections. Recent studies explore large language models (LLM) for topology
synthesis. However, the scenarios addressed by these studies do not align well
with practical applications. Specifically, existing work uses vague design
requirements as input and outputs an ideal model, but detailed structural
requirements and device-level models are more practical. Moreover, current
approaches either formulate topology synthesis as graph generation or Python
code generation, whereas practical topology design is a complex process that
demands extensive design knowledge. In this work, we propose AnalogXpert, a
LLM-based agent aiming at solving practical topology synthesis problem by
incorporating circuit design expertise into LLMs. First, we represent analog
topology as SPICE code and introduce a subcircuit library to reduce the design
space, in the same manner as experienced designers. Second, we decompose the
problem into two sub-task (i.e., block selection and block connection) through
the use of CoT and incontext learning techniques, to mimic the practical design
process. Third, we introduce a proofreading strategy that allows LLMs to
incrementally correct the errors in the initial design, akin to human designers
who iteratively check and adjust the initial topology design to ensure
accuracy. Finally, we construct a high-quality benchmark containing both real
data (30) and synthetic data (2k). AnalogXpert achieves 40% and 23% success
rates on the synthetic dataset and real dataset respectively, which is markedly
better than those of GPT-4o (3% on both the synthetic dataset and the real
dataset).

- **[2024-10] LLM as a code generator in Agile Model Driven Development** [[arXiv](http://arxiv.org/abs/2410.18489v1)]
  *Ahmed R. Sadik, Sebastian Brulin, Markus Olhofer, Antonello Ceravola, Frank Joublin*
  Abstract: Leveraging Large Language Models (LLM) like GPT4 in the auto generation of
code represents a significant advancement, yet it is not without its
challenges. The ambiguity inherent in natural language descriptions of software
poses substantial obstacles to generating deployable, structured artifacts.
This research champions Model Driven Development (MDD) as a viable strategy to
overcome these challenges, proposing an Agile Model Driven Development (AMDD)
approach that employs GPT4 as a code generator. This approach enhances the
flexibility and scalability of the code auto generation process and offers
agility that allows seamless adaptation to changes in models or deployment
environments. We illustrate this by modeling a multi agent Unmanned Vehicle
Fleet (UVF) system using the Unified Modeling Language (UML), significantly
reducing model ambiguity by integrating the Object Constraint Language (OCL)
for code structure meta modeling, and the FIPA ontology language for
communication semantics meta modeling. Applying GPT4 auto generation
capabilities yields Java and Python code that is compatible with the JADE and
PADE frameworks, respectively. Our thorough evaluation of the auto generated
code verifies its alignment with expected behaviors and identifies enhancements
in agent interactions. Structurally, we assessed the complexity of code derived
from a model constrained solely by OCL meta models, against that influenced by
both OCL and FIPA ontology meta models. The results indicate that the ontology
constrained meta model produces inherently more complex code, yet its
cyclomatic complexity remains within manageable levels, suggesting that
additional meta model constraints can be incorporated without exceeding the
high risk threshold for complexity.

- **[2024-08] Text2BIM: Generating Building Models Using a Large Language Model-based
  Multi-Agent Framework** [[arXiv](http://arxiv.org/abs/2408.08054v1)]
  *Changyu Du, Sebastian Esser, Stavros Nousias, André Borrmann*
  Abstract: The conventional BIM authoring process typically requires designers to master
complex and tedious modeling commands in order to materialize their design
intentions within BIM authoring tools. This additional cognitive burden
complicates the design process and hinders the adoption of BIM and model-based
design in the AEC (Architecture, Engineering, and Construction) industry. To
facilitate the expression of design intentions more intuitively, we propose
Text2BIM, an LLM-based multi-agent framework that can generate 3D building
models from natural language instructions. This framework orchestrates multiple
LLM agents to collaborate and reason, transforming textual user input into
imperative code that invokes the BIM authoring tool's APIs, thereby generating
editable BIM models with internal layouts, external envelopes, and semantic
information directly in the software. Furthermore, a rule-based model checker
is introduced into the agentic workflow, utilizing predefined domain knowledge
to guide the LLM agents in resolving issues within the generated models and
iteratively improving model quality. Extensive experiments were conducted to
compare and analyze the performance of three different LLMs under the proposed
framework. The evaluation results demonstrate that our approach can effectively
generate high-quality, structurally rational building models that are aligned
with the abstract concepts specified by user input. Finally, an interactive
software prototype was developed to integrate the framework into the BIM
authoring software Vectorworks, showcasing the potential of modeling by
chatting.


### Requirements Engineering
- **[2024-08] AI based Multiagent Approach for Requirements Elicitation and Analysis** [[arXiv](http://arxiv.org/abs/2409.00038v1)]
  *Malik Abdul Sami, Muhammad Waseem, Zheying Zhang, Zeeshan Rasheed, Kari Systä, Pekka Abrahamsson*
  Abstract: Requirements Engineering (RE) plays a pivotal role in software development,
encompassing tasks such as requirements elicitation, analysis, specification,
and change management. Despite its critical importance, RE faces challenges
including communication complexities, early-stage uncertainties, and accurate
resource estimation. This study empirically investigates the effectiveness of
utilizing Large Language Models (LLMs) to automate requirements analysis tasks.
We implemented a multi-agent system that deploys AI models as agents to
generate user stories from initial requirements, assess and improve their
quality, and prioritize them using a selected technique. In our implementation,
we deployed four models, namely GPT-3.5, GPT-4 Omni, LLaMA3-70, and Mixtral-8B,
and conducted experiments to analyze requirements on four real-world projects.
We evaluated the results by analyzing the semantic similarity and API
performance of different models, as well as their effectiveness and efficiency
in requirements analysis, gathering users' feedback on their experiences.
Preliminary results indicate notable variations in task completion among the
models. Mixtral-8B provided the quickest responses, while GPT-3.5 performed
exceptionally well when processing complex user stories with a higher
similarity score, demonstrating its capability in deriving accurate user
stories from project descriptions. Feedback and suggestions from the four
project members further corroborate the effectiveness of LLMs in improving and
streamlining RE phases.

- **[2024-05] MARE: Multi-Agents Collaboration Framework for Requirements Engineering** [[arXiv](http://arxiv.org/abs/2405.03256v1)]
  *Dongming Jin, Zhi Jin, Xiaohong Chen, Chunhui Wang*
  Abstract: Requirements Engineering (RE) is a critical phase in the software development
process that generates requirements specifications from stakeholders' needs.
Recently, deep learning techniques have been successful in several RE tasks.
However, obtaining high-quality requirements specifications requires
collaboration across multiple tasks and roles. In this paper, we propose an
innovative framework called MARE, which leverages collaboration among large
language models (LLMs) throughout the entire RE process. MARE divides the RE
process into four tasks: elicitation, modeling, verification, and
specification. Each task is conducted by engaging one or two specific agents
and each agent can conduct several actions. MARE has five agents and nine
actions. To facilitate collaboration between agents, MARE has designed a
workspace for agents to upload their generated intermediate requirements
artifacts and obtain the information they need. We conduct experiments on five
public cases, one dataset, and four new cases created by this work. We compared
MARE with three baselines using three widely used metrics for the generated
requirements models. Experimental results show that MARE can generate more
correct requirements models and outperform the state-of-the-art approaches by
15.4%. For the generated requirements specifications, we conduct a human
evaluation in three aspects and provide insights about the quality

- **[2024-05] Semantic API Alignment: Linking High-level User Goals to APIs** [[arXiv](http://arxiv.org/abs/2405.04236v1)]
  *Robert Feldt, Riccardo Coppola*
  Abstract: Large Language Models (LLMs) are becoming key in automating and assisting
various software development tasks, including text-based tasks in requirements
engineering but also in coding. Typically, these models are used to automate
small portions of existing tasks, but we present a broader vision to span
multiple steps from requirements engineering to implementation using existing
libraries. This approach, which we call Semantic API Alignment (SEAL), aims to
bridge the gap between a user's high-level goals and the specific functions of
one or more APIs.
  In this position paper, we propose a system architecture where a set of
LLM-powered ``agents'' match such high-level objectives with appropriate API
calls. This system could facilitate automated programming by finding matching
links or, alternatively, explaining mismatches to guide manual intervention or
further development.
  As an initial pilot, our paper demonstrates this concept by applying LLMs to
Goal-Oriented Requirements Engineering (GORE), via sub-goal analysis, for
aligning with REST API specifications, specifically through a case study
involving a GitHub statistics API. We discuss the potential of our approach to
enhance complex tasks in software development and requirements engineering and
outline future directions for research.

- **[2024-04] Elicitron: An LLM Agent-Based Simulation Framework for Design
  Requirements Elicitation** [[arXiv](https://arxiv.org/pdf/2404.16045)]
  *Mohammadmehdi Ataei, Hyunmin Cheong, Daniele Grandi, Ye Wang, Nigel Morris, Alexander Tessier*
  Abstract: Requirements elicitation, a critical, yet time-consuming and challenging step
in product development, often fails to capture the full spectrum of user needs.
This may lead to products that fall short of expectations. This paper
introduces a novel framework that leverages Large Language Models (LLMs) to
automate and enhance the requirements elicitation process. LLMs are used to
generate a vast array of simulated users (LLM agents), enabling the exploration
of a much broader range of user needs and unforeseen use cases. These agents
engage in product experience scenarios, through explaining their actions,
observations, and challenges. Subsequent agent interviews and analysis uncover
valuable user needs, including latent ones. We validate our framework with
three experiments. First, we explore different methodologies for diverse agent
generation, discussing their advantages and shortcomings. We measure the
diversity of identified user needs and demonstrate that context-aware agent
generation leads to greater diversity. Second, we show how our framework
effectively mimics empathic lead user interviews, identifying a greater number
of latent needs than conventional human interviews. Third, we showcase that
LLMs can be used to analyze interviews, capture needs, and classify them as
latent or not. Our work highlights the potential of using LLM agents to
accelerate early-stage product development, reduce costs, and increase
innovation.

- **[2024-04] Prioritizing Software Requirements Using Large Language Models** [[arXiv](http://arxiv.org/abs/2405.01564v1)]
  *Malik Abdul Sami, Zeeshan Rasheed, Muhammad Waseem, Zheying Zhang, Tomas Herda, Pekka Abrahamsson*
  Abstract: Large Language Models (LLMs) are revolutionizing Software Engineering (SE) by
introducing innovative methods for tasks such as collecting requirements,
designing software, generating code, and creating test cases, among others.
This article focuses on requirements engineering, typically seen as the initial
phase of software development that involves multiple system stakeholders.
Despite its key role, the challenge of identifying requirements and satisfying
all stakeholders within time and budget constraints remains significant. To
address the challenges in requirements engineering, this study introduces a
web-based software tool utilizing AI agents and prompt engineering to automate
task prioritization and apply diverse prioritization techniques, aimed at
enhancing project management within the agile framework. This approach seeks to
transform the prioritization of agile requirements, tackling the substantial
challenge of meeting stakeholder needs within set time and budget limits.
Furthermore, the source code of our developed prototype is available on GitHub,
allowing for further experimentation and prioritization of requirements,
facilitating research and practical application.

- **[2024/03] LLM-Based Agents for Automating the Enhancement of User Story Quality: An Early Report.** [[Link](https://link.springer.com/chapter/10.1007/978-3-031-61154-4_8)]
  *Zheying Zhang, Maruf Rayhan, Tomas Herda, Manuel Goisauf, Pekka Abrahamsson*
In agile software development, maintaining high-quality user stories is crucial, but also challenging. This study explores the use of large language models to automatically improve the user story quality in Austrian Post Group IT agile teams. We developed a reference model for an Autonomous LLM-based Agent System and implemented it at the company. The quality of user stories in the study and the effectiveness of these agents for user story quality improvement was assessed by 11 participants across six agile teams. Our findings demonstrate the potential of LLMs in improving user story quality, contributing to the research on AI role in agile development, and providing a practical example of the transformative impact of AI in an industry setting.

- **[2024-02] Effort and Size Estimation in Software Projects with Large Language
  Model-based Intelligent Interfaces** [[arXiv](http://arxiv.org/abs/2402.07158v2)]
  *Claudionor N. Coelho Jr, Hanchen Xiong, Tushar Karayil, Sree Koratala, Rex Shang, Jacob Bollinger, Mohamed Shabar, Syam Nair*
  Abstract: The advancement of Large Language Models (LLM) has also resulted in an
equivalent proliferation in its applications. Software design, being one, has
gained tremendous benefits in using LLMs as an interface component that extends
fixed user stories. However, inclusion of LLM-based AI agents in software
design often poses unexpected challenges, especially in the estimation of
development efforts. Through the example of UI-based user stories, we provide a
comparison against traditional methods and propose a new way to enhance
specifications of natural language-based questions that allows for the
estimation of development effort by taking into account data sources,
interfaces and algorithms.



## Human-AI Collaboration and Interaction

### Human-in-the-Loop Systems
- **[2025-02] TableTalk: Scaffolding Spreadsheet Development with a Language Agent** [[arXiv](http://arxiv.org/abs/2502.09787v1)]
  *Jenny T. Liang, Aayush Kumar, Yasharth Bajpai, Sumit Gulwani, Vu Le, Chris Parnin, Arjun Radhakrishna, Ashish Tiwari, Emerson Murphy-Hill, Guastavo Soares*
  Abstract: Despite its ubiquity in the workforce, spreadsheet programming remains
challenging as programmers need both spreadsheet-specific knowledge (e.g., APIs
to write formulas) and problem-solving skills to create complex spreadsheets.
Large language models (LLMs) can help automate aspects of this process, and
recent advances in planning and reasoning have enabled language agents, which
dynamically plan, use tools, and take iterative actions to complete complex
tasks. These agents observe, plan, and act, making them well-suited to scaffold
spreadsheet programming by following expert processes.
  We present TableTalk, a language agent that helps programmers build
spreadsheets conversationally. Its design reifies three design principles --
scaffolding, flexibility, and incrementality -- which we derived from two
studies of seven programmers and 62 Excel templates. TableTalk structures
spreadsheet development by generating step-by-step plans and suggesting three
next steps users can choose from. It also integrates tools that enable
incremental spreadsheet construction. A user study with 20 programmers shows
that TableTalk produces spreadsheets 2.3 times more likely to be preferred over
a baseline agent, while reducing cognitive load and time spent reasoning about
spreadsheet actions by 12.6%. TableTalk's approach has implications for
human-agent collaboration. This includes providing persistent direct
manipulation interfaces for stopping or undoing agent actions, while ensuring
that such interfaces for accepting actions can be deactivated.

- **[2025-02] VTutor: An Open-Source SDK for Generative AI-Powered Animated
  Pedagogical Agents with Multi-Media Output** [[arXiv](http://arxiv.org/abs/2502.04103v1)]
  *Eason Chen, Chengyu Lin, Xinyi Tang, Aprille Xi, Canwen Wang, Jionghao Lin, Kenneth R Koedinger*
  Abstract: The rapid evolution of large language models (LLMs) has transformed
human-computer interaction (HCI), but the interaction with LLMs is currently
mainly focused on text-based interactions, while other multi-model approaches
remain under-explored. This paper introduces VTutor, an open-source Software
Development Kit (SDK) that combines generative AI with advanced animation
technologies to create engaging, adaptable, and realistic APAs for human-AI
multi-media interactions. VTutor leverages LLMs for real-time personalized
feedback, advanced lip synchronization for natural speech alignment, and WebGL
rendering for seamless web integration. Supporting various 2D and 3D character
models, VTutor enables researchers and developers to design emotionally
resonant, contextually adaptive learning agents. This toolkit enhances learner
engagement, feedback receptivity, and human-AI interaction while promoting
trustworthy AI principles in education. VTutor sets a new standard for
next-generation APAs, offering an accessible, scalable solution for fostering
meaningful and immersive human-AI interaction experiences. The VTutor project
is open-sourced and welcomes community-driven contributions and showcases.

- **[2025-02] Leveraging LLMs for Dynamic IoT Systems Generation through
  Mixed-Initiative Interaction** [[arXiv](http://arxiv.org/abs/2502.00689v1)]
  *Bassam Adnan, Sathvika Miryala, Aneesh Sambu, Karthik Vaidhyanathan, Martina De Sanctis, Romina Spalazzese*
  Abstract: IoT systems face significant challenges in adapting to user needs, which are
often under-specified and evolve with changing environmental contexts. To
address these complexities, users should be able to explore possibilities,
while IoT systems must learn and support users in the process of providing
proper services, e.g., to serve novel experiences. The IoT-Together paradigm
aims to meet this demand through the Mixed-Initiative Interaction (MII)
paradigm that facilitates a collaborative synergy between users and IoT
systems, enabling the co-creation of intelligent and adaptive solutions that
are precisely aligned with user-defined goals. This work advances IoT-Together
by integrating Large Language Models (LLMs) into its architecture. Our approach
enables intelligent goal interpretation through a multi-pass dialogue framework
and dynamic service generation at runtime according to user needs. To
demonstrate the efficacy of our methodology, we design and implement the system
in the context of a smart city tourism case study. We evaluate the system's
performance using agent-based simulation and user studies. Results indicate
efficient and accurate service identification and high adaptation quality. The
empirical evidence indicates that the integration of Large Language Models
(LLMs) into IoT architectures can significantly enhance the architectural
adaptability of the system while ensuring real-world usability.

- **[2024-12] Towards Modeling Human-Agentic Collaborative Workflows: A BPMN Extension** [[arXiv](http://arxiv.org/abs/2412.05958v2)]
  *Adem Ait, Javier Luis Cánovas Izquierdo, Jordi Cabot*
  Abstract: Large Language Models (LLMs) have facilitated the definition of autonomous
intelligent agents. Such agents have already demonstrated their potential in
solving complex tasks in different domains. And they can further increase their
performance when collaborating with other agents in a multi-agent system.
However, the orchestration and coordination of these agents is still
challenging, especially when they need to interact with humans as part of
human-agentic collaborative workflows. These kinds of workflows need to be
precisely specified so that it is clear whose responsible for each task, what
strategies agents can follow to complete individual tasks or how decisions will
be taken when different alternatives are proposed, among others. Current
business process modeling languages fall short when it comes to specifying
these new mixed collaborative scenarios. In this exploratory paper, we extend a
well-known process modeling language (i.e., BPMN) to enable the definition of
this new type of workflow. Our extension covers both the formalization of the
new metamodeling concepts required and the proposal of a BPMN-like graphical
notation to facilitate the definition of these workflows. Our extension has
been implemented and is available as an open-source human-agentic workflow
modeling editor on GitHub.

- **[2024-11] PyGen: A Collaborative Human-AI Approach to Python Package Creation** [[arXiv](http://arxiv.org/abs/2411.08932v1)]
  *Saikat Barua, Mostafizur Rahman, Md Jafor Sadek, Rafiul Islam, Shehnaz Khaled, Md. Shohrab Hossain*
  Abstract: The principles of automation and innovation serve as foundational elements
for advancement in contemporary science and technology. Here, we introduce
Pygen, an automation platform designed to empower researchers, technologists,
and hobbyists to bring abstract ideas to life as core, usable software tools
written in Python. Pygen leverages the immense power of autoregressive large
language models to augment human creativity during the ideation, iteration, and
innovation process. By combining state-of-the-art language models with
open-source code generation technologies, Pygen has significantly reduced the
manual overhead of tool development. From a user prompt, Pygen automatically
generates Python packages for a complete workflow from concept to package
generation and documentation. The findings of our work show that Pygen
considerably enhances the researcher's productivity by enabling the creation of
resilient, modular, and well-documented packages for various specialized
purposes. We employ a prompt enhancement approach to distill the user's package
description into increasingly specific and actionable. While being inherently
an open-ended task, we have evaluated the generated packages and the
documentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with
detailed results in the results section. Furthermore, we documented our
results, analyzed the limitations, and suggested strategies to alleviate them.
Pygen is our vision of ethical automation, a framework that promotes
inclusivity, accessibility, and collaborative development. This project marks
the beginning of a large-scale effort towards creating tools where intelligent
agents collaborate with humans to improve scientific and technological
development substantially.
  Our code and generated examples are open-sourced at
[https://github.com/GitsSaikat/Pygen]

- **[2024-11] Developer Challenges on Large Language Models: A Study of Stack Overflow
  and OpenAI Developer Forum Posts** [[arXiv](http://arxiv.org/abs/2411.10873v2)]
  *Khairul Alam, Kartik Mittal, Banani Roy, Chanchal Roy*
  Abstract: Large Language Models (LLMs) have gained widespread popularity due to their
exceptional capabilities across various domains, including chatbots,
healthcare, education, content generation, and automated support systems.
However, developers encounter numerous challenges when implementing,
fine-tuning, and integrating these models into real-world applications. This
study investigates LLM developers' challenges by analyzing community
interactions on Stack Overflow and OpenAI Developer Forum, employing BERTopic
modeling to identify and categorize developer discussions. Our analysis yields
nine challenges on Stack Overflow (e.g., LLM Ecosystem and Challenges, API
Usage, LLM Training with Frameworks) and 17 on the OpenAI Developer Forum
(e.g., API Usage and Error Handling, Fine-Tuning and Dataset Management).
Results indicate that developers frequently turn to Stack Overflow for
implementation guidance, while OpenAI's forum focuses on troubleshooting.
Notably, API and functionality issues dominate discussions on the OpenAI forum,
with many posts requiring multiple responses, reflecting the complexity of
LLM-related problems. We find that LLM-related queries often exhibit great
difficulty, with a substantial percentage of unresolved posts (e.g., 79.03\% on
Stack Overflow) and prolonged response times, particularly for complex topics
like 'Llama Indexing and GPU Utilization' and 'Agents and Tool Interactions'.
In contrast, established fields like Mobile Development and Security enjoy
quicker resolutions and stronger community engagement. These findings highlight
the need for improved community support and targeted resources to assist LLM
developers in overcoming the evolving challenges of this rapidly growing field.
This study provides insights into areas of difficulty, paving the way for
future research and tool development to better support the LLM developer
community.

- **[2024-11] Human-In-the-Loop Software Development Agents** [[arXiv](http://arxiv.org/abs/2411.12924v2)]
  *Wannita Takerngsaksiri, Jirat Pasuksmit, Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Ruixiong Zhang, Fan Jiang, Jing Li, Evan Cook, Kun Chen, Ming Wu*
  Abstract: Recently, Large Language Models (LLMs)-based multi-agent paradigms for
software engineering are introduced to automatically resolve software
development tasks (e.g., from a given issue to source code). However, existing
work is evaluated based on historical benchmark datasets, rarely considers
human feedback at each stage of the automated software development process, and
has not been deployed in practice. In this paper, we introduce a
Human-in-the-loop LLM-based Agents framework (HULA) for software development
that allows software engineers to refine and guide LLMs when generating coding
plans and source code for a given task. We design, implement, and deploy the
HULA framework into Atlassian JIRA for internal uses. Through a multi-stage
evaluation of the HULA framework, Atlassian software engineers perceive that
HULA can minimize the overall development time and effort, especially in
initiating a coding plan and writing code for straightforward tasks. On the
other hand, challenges around code quality remain a concern in some cases. We
draw lessons learned and discuss opportunities for future work, which will pave
the way for the advancement of LLM-based agents in software development.

- **[2024/09] A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement.** [[Link](https://dl.acm.org/doi/10.1145/3691620.3695506)]
  *Huan Zhang, Wei Cheng, Yuhan Wu, Wei Hu*
Large language models (LLMs) have achieved impressive performance on code generation. Although prior studies enhanced LLMs with prompting techniques and code refinement, they still struggle with complex programming problems due to rigid solution plans. In this paper, we draw on pair programming practices to propose PairCoder, a novel LLM-based framework for code generation. PairCoder incorporates two collaborative LLM agents, namely a Navigator agent for high-level planning and a Driver agent for specific implementation. The Navigator is responsible for proposing promising solution plans, selecting the current optimal plan, and directing the next iteration round based on execution feedback. The Driver follows the guidance of Navigator to undertake initial code generation, code testing, and refinement. This interleaved and iterative workflow involves multi-plan exploration and feedback-based refinement, which mimics the collaboration of pair programmers. We evaluate PairCoder with both open-source and closed-source LLMs on various code generation benchmarks. Extensive experimental results demonstrate the superior accuracy of PairCoder, achieving relative pass@1 improvements of 12.00%-162.43% compared to prompting LLMs directly.

- **[2024-08] Future of Artificial Intelligence in Agile Software Development** [[arXiv](http://arxiv.org/abs/2408.00703v1)]
  *Mariyam Mahboob, Mohammed Rayyan Uddin Ahmed, Zoiba Zia, Mariam Shakeel Ali, Ayman Khaleel Ahmed*
  Abstract: The advent of Artificial intelligence has promising advantages that can be
utilized to transform the landscape of software project development. The
Software process framework consists of activities that constantly require
routine human interaction, leading to the possibility of errors and
uncertainties. AI can assist software development managers, software testers,
and other team members by leveraging LLMs, GenAI models, and AI agents to
perform routine tasks, risk analysis and prediction, strategy recommendations,
and support decision making. AI has the potential to increase efficiency and
reduce the risks encountered by the project management team while increasing
the project success rates. Additionally, it can also break down complex notions
and development processes for stakeholders to make informed decisions. In this
paper, we propose an approach in which AI tools and technologies can be
utilized to bestow maximum assistance for agile software projects, which have
become increasingly favored in the industry in recent years.

- **[2024/07] OpenHands: An Open Platform for AI Software Developers as Generalist Agents** [[Link](https://openreview.net/pdf?id=OJd3ayDDoF)]
  *Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, Graham Neubig*
Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. In this paper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. We describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on our currently incorporated benchmarks, we perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others. Released under the permissive MIT license, OpenHands is a community project spanning academia and industry with more than 2.1K contributions from over 188 contributors.

- **[2024/05] SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering** [[Link](https://openreview.net/pdf?id=mXpq6ut8J3)]
  *John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, Ofir Press*
Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. We investigate how interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, we provide insight on how the design of the ACI can impact agents' behavior and performance.

- **[2024-03] AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large
  Language Models** [[arXiv](http://arxiv.org/abs/2403.15157v2)]
  *Chaoyun Zhang, Zicheng Ma, Yuhao Wu, Shilin He, Si Qin, Minghua Ma, Xiaoting Qin, Yu Kang, Yuyi Liang, Xiaoyu Gou, Yajie Xue, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang*
  Abstract: Verbatim feedback constitutes a valuable repository of user experiences,
opinions, and requirements essential for software development. Effectively and
efficiently extracting valuable insights from such data poses a challenging
task. This paper introduces Allhands , an innovative analytic framework
designed for large-scale feedback analysis through a natural language
interface, leveraging large language models (LLMs). Allhands adheres to a
conventional feedback analytic workflow, initially conducting classification
and topic modeling on the feedback to convert them into a structurally
augmented format, incorporating LLMs to enhance accuracy, robustness,
generalization, and user-friendliness. Subsequently, an LLM agent is employed
to interpret users' diverse questions in natural language on feedback,
translating them into Python code for execution, and delivering comprehensive
multi-modal responses, including text, code, tables, and images.
  We evaluate Allhands across three diverse feedback datasets. The experiments
demonstrate that Allhands achieves superior efficacy at all stages of analysis,
including classification and topic modeling, eventually providing users with an
"ask me anything" experience with comprehensive, correct and human-readable
response. To the best of our knowledge, Allhands stands as the first
comprehensive feedback analysis framework that supports diverse and customized
requirements for insight extraction through a natural language interface.

- **[2023/05] SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models.** [[Link](https://openreview.net/pdf?id=tfyr2zRVoK)]
  *Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, Zhaoxiang Zhang*
Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page:this https URL.




### Interactive Coding
- **[2024-12] DialogAgent: An Auto-engagement Agent for Code Question Answering Data
  Production** [[arXiv](http://arxiv.org/abs/2412.08069v1)]
  *Xiaoyun Liang, Jingyi Ren, Jiayi Qi, Chao Peng, Bo Jiang*
  Abstract: Large Language Models (LLMs) have become increasingly integral to enhancing
developer productivity, particularly in code generation, comprehension, and
repair tasks. However, fine-tuning these models with high-quality, real-world
data is challenging due to privacy concerns and the lack of accessible, labeled
datasets. In this paper, we present DialogAgent, an automated tool for
generating synthetic training data that closely mimics real developer
interactions within Integrated Development Environments (IDEs). DialogAgent
enables the production of diverse, high-fidelity query-response pairs by
simulating multi-turn dialogues and contextual behaviors observed in real-world
programming scenarios. The tool significantly reduces the reliance on manual
data generation, increasing efficiency by 4.8 times compared to traditional
methods. Our experiments and online deployment demonstrate substantial
improvements in model performance for code-related question-answering tasks:
the acceptance rate of responses generated by our in-house model is improved by
33%, after training on synthesized data generated by DialogAgent.

- **[2024/09] A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement.** [[Link](https://dl.acm.org/doi/10.1145/3691620.3695506)]
  *Huan Zhang, Wei Cheng, Yuhan Wu, Wei Hu*
Large language models (LLMs) have achieved impressive performance on code generation. Although prior studies enhanced LLMs with prompting techniques and code refinement, they still struggle with complex programming problems due to rigid solution plans. In this paper, we draw on pair programming practices to propose PairCoder, a novel LLM-based framework for code generation. PairCoder incorporates two collaborative LLM agents, namely a Navigator agent for high-level planning and a Driver agent for specific implementation. The Navigator is responsible for proposing promising solution plans, selecting the current optimal plan, and directing the next iteration round based on execution feedback. The Driver follows the guidance of Navigator to undertake initial code generation, code testing, and refinement. This interleaved and iterative workflow involves multi-plan exploration and feedback-based refinement, which mimics the collaboration of pair programmers. We evaluate PairCoder with both open-source and closed-source LLMs on various code generation benchmarks. Extensive experimental results demonstrate the superior accuracy of PairCoder, achieving relative pass@1 improvements of 12.00%-162.43% compared to prompting LLMs directly.

- **[2024/07] AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents** [[Link](https://aclanthology.org/2024.acl-long.850.pdf)]
  *Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, Niranjan Balasubramanian*
Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls.
To remedy this gap, we built AppWorld Engine, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our 'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least 16% fewer. This highlights the benchmark's difficulty and AppWorld's potential to push the frontiers of interactive coding agents. The project website is available at this https URL.

- **[2024/05] HumanEvalComm: Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent** [[Link](https://dl.acm.org/doi/abs/10.1145/3715109)]
  *Jie JW Wu, Fatemeh H Fard*
Large language models (LLMs) have significantly improved their ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks.
In this work, we conducted an empirical study on the benchmark and analysis of the communication skills of LLMs for code generation. We define communication skills of LLMs as ``being able to ask clarifying questions when the description of the code generation problem has issues''. We created a new benchmark, HumanEvalComm, by modifying problem descriptions according to three issues: inconsistency, ambiguity, incompleteness. We defined new evaluation metrics such as Communication Rate and Good Question Rate, and then experimented on HumanEvalComm with different Code LLMs, and a new LLM agent approach, Okanagan, to identify and ask questions in ambiguous parts from code and descriptions for further refining the generated code. Finally, we discussed evaluation results by comparing Code LLMs and Okanagan with our findings.

- **[2024-02] An Empirical Study on Low Code Programming using Traditional vs Large
  Language Model Support** [[arXiv](http://arxiv.org/abs/2402.01156v2)]
  *Yongkun Liu, Jiachi Chen, Tingting Bi, John Grundy, Yanlin Wang, Jianxing Yu, Ting Chen, Yutian Tang, Zibin Zheng*
  Abstract: Low-code programming (LCP) refers to programming using models at higher
levels of abstraction, resulting in less manual and more efficient programming,
and reduced learning effort for amateur developers. Many LCP tools have rapidly
evolved and have benefited from the concepts of visual programming languages
(VPLs) and programming by demonstration (PBD). With huge increase in interest
in using large language models (LLMs) in software engineering, LLM-based LCP
has began to become increasingly important. However, the technical principles
and application scenarios of traditional approaches to LCP and LLM-based LCP
are significantly different. Understanding these key differences and
characteristics in the application of the two approaches to LCP by users is
crucial for LCP providers in improving existing and developing new LCP tools,
and in better assisting users in choosing the appropriate LCP technology. We
conducted an empirical study of both traditional LCP and LLM-based LCP. We
analyzed developers' discussions on Stack Overflow (SO) over the past three
years and then explored the similarities and differences between traditional
LCP and LLM-based LCP features and developer feedback. Our findings reveal that
while traditional LCP and LLM-based LCP share common primary usage scenarios,
they significantly differ in scope, limitations and usage throughout the
software development lifecycle, particularly during the implementation phase.
We also examine how LLMs impact and integrate with LCP, discussing the latest
technological developments in LLM-based LCP, such as its integration with VPLs
and the application of LLM Agents in software engineering.

- **[2023-08] ChatLogo: A Large Language Model-Driven Hybrid Natural-Programming
  Language Interface for Agent-based Modeling and Programming** [[arXiv](http://arxiv.org/abs/2308.08102v1)]
  *John Chen, Uri Wilensky*
  Abstract: Building on Papert (1980)'s idea of children talking to computers, we propose
ChatLogo, a hybrid natural-programming language interface for agent-based
modeling and programming. We build upon previous efforts to scaffold ABM & P
learning and recent development in leveraging large language models (LLMs) to
support the learning of computational programming. ChatLogo aims to support
conversations with computers in a mix of natural and programming languages,
provide a more user-friendly interface for novice learners, and keep the
technical system from over-reliance on any single LLM. We introduced the main
elements of our design: an intelligent command center, and a conversational
interface to support creative expression. We discussed the presentation format
and future work. Responding to the challenges of supporting open-ended
constructionist learning of ABM & P and leveraging LLMs for educational
purposes, we contribute to the field by proposing the first constructionist
LLM-driven interface to support computational and complex systems thinking.

- **[2023/07] ChatDev: Communicative Agents for Software Development** [[Link](https://aclanthology.org/2024.acl-long.810.pdf)]
  *Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, Maosong Sun*
Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at this https URL.

- **[2023-06] InterCode: Standardizing and Benchmarking Interactive Coding with
  Execution Feedback** [[arXiv](https://arxiv.org/pdf/2306.14898)]
  *John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao*
  Abstract: Humans write code in a fundamentally interactive manner and rely on constant
execution feedback to correct errors, resolve ambiguities, and decompose tasks.
While LLMs have recently exhibited promising coding capabilities, current
coding benchmarks mostly consider a static instruction-to-code sequence
transduction process, which has the potential for error propagation and a
disconnect between the generated code and its final execution environment. To
address this gap, we introduce InterCode, a lightweight, flexible, and
easy-to-use framework of interactive coding as a standard reinforcement
learning (RL) environment, with code as actions and execution feedback as
observations. Our framework is language and platform agnostic, uses
self-contained Docker environments to provide safe and reproducible execution,
and is compatible out-of-the-box with traditional seq2seq coding methods, while
enabling the development of new methods for interactive code generation. We use
InterCode to create three interactive code environments with Bash, SQL, and
Python as action spaces, leveraging data from the static NL2Bash, Spider, and
MBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating
multiple state-of-the-art LLMs configured with different prompting strategies
such as ReAct and Plan & Solve. Our results showcase the benefits of
interactive code generation and demonstrate that InterCode can serve as a
challenging benchmark for advancing code understanding and generation
capabilities. InterCode is designed to be easily extensible and can even be
used to create new tasks such as Capture the Flag, a popular coding puzzle that
is inherently multi-step and involves multiple programming languages. Project
site with code and data: https://intercode-benchmark.github.io


### Personalized and Explainable Systems
- **[2024-12] From Critique to Clarity: A Pathway to Faithful and Personalized Code
  Explanations with Large Language Models** [[arXiv](http://arxiv.org/abs/2501.14731v1)]
  *Zexing Xu, Zhuang Luo, Yichuan Li, Kyumin Lee, S. Rasoul Etesami*
  Abstract: In the realm of software development, providing accurate and personalized
code explanations is crucial for both technical professionals and business
stakeholders. Technical professionals benefit from enhanced understanding and
improved problem-solving skills, while business stakeholders gain insights into
project alignments and transparency. Despite the potential, generating such
explanations is often time-consuming and challenging. This paper presents an
innovative approach that leverages the advanced capabilities of large language
models (LLMs) to generate faithful and personalized code explanations. Our
methodology integrates prompt enhancement, self-correction mechanisms,
personalized content customization, and interaction with external tools,
facilitated by collaboration among multiple LLM agents. We evaluate our
approach using both automatic and human assessments, demonstrating that our
method not only produces accurate explanations but also tailors them to
individual user preferences. Our findings suggest that this approach
significantly improves the quality and relevance of code explanations, offering
a valuable tool for developers and stakeholders alike.

- **[2023-12] A Prompt Learning Framework for Source Code Summarization** [[arXiv](http://arxiv.org/abs/2312.16066v2)]
  *Tingting Xu, Yun Miao, Chunrong Fang, Hanwei Qian, Xia Feng, Zhenpeng Chen, Chong Wang, Jian Zhang, Weisong Sun, Zhenyu Chen, Yang Liu*
  Abstract: (Source) code summarization is the task of automatically generating natural
language summaries (also called comments) for given code snippets. Recently,
with the successful application of large language models (LLMs) in numerous
fields, software engineering researchers have also attempted to adapt LLMs to
solve code summarization tasks. The main adaptation schemes include instruction
prompting, task-oriented (full-parameter) fine-tuning, and parameter-efficient
fine-tuning (PEFT). However, instruction prompting involves designing crafted
prompts and requires users to have professional domain knowledge, while
task-oriented fine-tuning requires high training costs, and effective, tailored
PEFT methods for code summarization are still lacking.
  This paper proposes an effective prompt learning framework for code
summarization called PromptCS. It no longer requires users to rack their brains
to design effective prompts. Instead, PromptCS trains a prompt agent that can
generate continuous prompts to unleash the potential for LLMs in code
summarization. Compared to the human-written discrete prompt, the continuous
prompts are produced under the guidance of LLMs and are therefore easier to
understand by LLMs. PromptCS is non-invasive to LLMs and freezes the parameters
of LLMs when training the prompt agent, which can greatly reduce the
requirements for training resources. Our comprehensive experimental results
show that PromptCS significantly outperforms instruction prompting schemes
(including zero-shot learning and few-shot learning) on all four widely used
metrics, and is comparable to the task-oriented fine-tuning scheme. In some
base LLMs, e.g., StarCoderBase-1B and -3B, PromptCS even outperforms the
task-oriented fine-tuning scheme. More importantly, the training efficiency of
PromptCS is faster than the task-oriented fine-tuning scheme, with a more
pronounced advantage on larger LLMs.

- **[2023/10] How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging** [[Link](https://dl.acm.org/doi/10.1007/978-3-031-64302-6_19)]
  *Qianou Ma, Hua Shen, Kenneth Koedinger, Tongshuang Wu*
Large Language Models (LLMs) now excel at generative skills and can create content at impeccable speeds. However, they are imperfect and still make various mistakes. In a Computer Science education context, as these models are widely recognized as "AI pair programmers," it becomes increasingly important to train students on evaluating and debugging the LLM-generated code. In this work, we introduce HypoCompass, a novel system to facilitate deliberate practice on debugging, where human novices play the role of Teaching Assistants and help LLM-powered teachable agents debug code. We enable effective task delegation between students and LLMs in this learning-by-teaching environment: students focus on hypothesizing the cause of code errors, while adjacent skills like code completion are offloaded to LLM-agents. Our evaluations demonstrate that HypoCompass generates high-quality training materials (e.g., bugs and fixes), outperforming human counterparts fourfold in efficiency, and significantly improves student performance on debugging by 12% in the pre-to-post test.




## Tool Integration and Automation

### Tool-Augmented Development
- **[2025-01] Leveraging LLM Agents for Translating Network Configurations** [[arXiv](http://arxiv.org/abs/2501.08760v1)]
  *Yunze Wei, Xiaohui Xie, Yiwei Zuo, Tianshuo Hu, Xinyi Chen, Kaiwen Chi, Yong Cui*
  Abstract: Configuration translation is a critical and frequent task in network
operations. When a network device is damaged or outdated, administrators need
to replace it to maintain service continuity. The replacement devices may
originate from different vendors, necessitating configuration translation to
ensure seamless network operation. However, translating configurations manually
is a labor-intensive and error-prone process. In this paper, we propose an
intent-based framework for translating network configuration with Large
Language Model (LLM) Agents. The core of our approach is an Intent-based
Retrieval Augmented Generation (IRAG) module that systematically splits a
configuration file into fragments, extracts intents, and generates accurate
translations. We also design a two-stage verification method to validate the
syntax and semantics correctness of the translated configurations. We implement
and evaluate the proposed method on real-world network configurations.
Experimental results show that our method achieves 97.74% syntax correctness,
outperforming state-of-the-art methods in translation accuracy.

- **[2025-01] ToolFactory: Automating Tool Generation by Leveraging LLM to Understand
  REST API Documentations** [[arXiv](http://arxiv.org/abs/2501.16945v1)]
  *Xinyi Ni, Qiuyang Wang, Yukun Zhang, Pengyu Hong*
  Abstract: LLM-based tool agents offer natural language interfaces, enabling users to
seamlessly interact with computing services. While REST APIs are valuable
resources for building such agents, they must first be transformed into
AI-compatible tools. Automatically generating AI-compatible tools from REST API
documents can greatly streamline tool agent development and minimize user
learning curves. However, API documentation often suffers from a lack of
standardization, inconsistent schemas, and incomplete information. To address
these issues, we developed \textbf{ToolFactory}, an open-source pipeline for
automating tool generation from unstructured API documents. To enhance the
reliability of the developed tools, we implemented an evaluation method to
diagnose errors. Furthermore, we built a knowledge base of verified tools,
which we leveraged to infer missing information from poorly documented APIs. We
developed the API Extraction Benchmark, comprising 167 API documents and 744
endpoints in various formats, and designed a JSON schema to annotate them. This
annotated dataset was utilized to train and validate ToolFactory. The
experimental results highlight the effectiveness of ToolFactory. We also
demonstrated ToolFactory by creating a domain-specific AI agent for
glycomaterials research. ToolFactory exhibits significant potential for
facilitating the seamless integration of scientific REST APIs into AI
workflows.

- **[2024-12] Creating an LLM-based AI-agent: A high-level methodology towards
  enhancing LLMs with APIs** [[arXiv](http://arxiv.org/abs/2412.13233v2)]
  *Ioannis Tzachristas*
  Abstract: Large Language Models (LLMs) have revolutionized various aspects of
engineering and science. Their utility is often bottlenecked by the lack of
interaction with the external digital environment. To overcome this limitation
and achieve integration of LLMs and Artificial Intelligence (AI) into
real-world applications, customized AI agents are being constructed. Based on
the technological trends and techniques, we extract a high-level approach for
constructing these AI agents, focusing on their underlying architecture. This
thesis serves as a comprehensive guide that elucidates a multi-faceted approach
for empowering LLMs with the capability to leverage Application Programming
Interfaces (APIs). We present a 7-step methodology that begins with the
selection of suitable LLMs and the task decomposition that is necessary for
complex problem-solving. This methodology includes techniques for generating
training data for API interactions and heuristics for selecting the appropriate
API among a plethora of options. These steps eventually lead to the generation
of API calls that are both syntactically and semantically aligned with the
LLM's understanding of a given task. Moreover, we review existing frameworks
and tools that facilitate these processes and highlight the gaps in current
attempts. In this direction, we propose an on-device architecture that aims to
exploit the functionality of carry-on devices by using small models from the
Hugging Face community. We examine the effectiveness of these approaches on
real-world applications of various domains, including the generation of a piano
sheet. Through an extensive analysis of the literature and available
technologies, this thesis aims to set a compass for researchers and
practitioners to harness the full potential of LLMs augmented with external
tool capabilities, thus paving the way for more autonomous, robust, and
context-aware AI agents.

- **[2024-11] GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis** [[arXiv](http://arxiv.org/abs/2411.03205v4)]
  *Temitope Akinboyewa, Zhenlong Li, Huan Ning, M. Naser Lessani*
  Abstract: Recent advancements in Generative AI offer promising capabilities for spatial
analysis. Despite their potential, the integration of generative AI with
established GIS platforms remains underexplored. In this study, we propose a
framework for integrating LLMs directly into existing GIS platforms, using QGIS
as an example. Our approach leverages the reasoning and programming
capabilities of LLMs to autonomously generate spatial analysis workflows and
code through an informed agent that has comprehensive documentation of key GIS
tools and parameters. The implementation of this framework resulted in the
development of a "GIS Copilot" that allows GIS users to interact with QGIS
using natural language commands for spatial analysis. The GIS Copilot was
evaluated with over 100 spatial analysis tasks with three complexity levels:
basic tasks that require one GIS tool and typically involve one data layer to
perform simple operations; intermediate tasks involving multi-step processes
with multiple tools, guided by user instructions; and advanced tasks which
involve multi-step processes that require multiple tools but not guided by user
instructions, necessitating the agent to independently decide on and executes
the necessary steps. The evaluation reveals that the GIS Copilot demonstrates
strong potential in automating foundational GIS operations, with a high success
rate in tool selection and code generation for basic and intermediate tasks,
while challenges remain in achieving full autonomy for more complex tasks. This
study contributes to the emerging vision of Autonomous GIS, providing a pathway
for non-experts to engage with geospatial analysis with minimal prior
expertise. While full autonomy is yet to be achieved, the GIS Copilot
demonstrates significant potential for simplifying GIS workflows and enhancing
decision-making processes.

- **[2024-11] AgentOps: Enabling Observability of LLM Agents** [[arXiv](http://arxiv.org/abs/2411.05285v2)]
  *Liming Dong, Qinghua Lu, Liming Zhu*
  Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities
across various domains, gaining extensive attention from academia and industry.
However, these agents raise significant concerns on AI safety due to their
autonomous and non-deterministic behavior, as well as continuous evolving
nature . From a DevOps perspective, enabling observability in agents is
necessary to ensuring AI safety, as stakeholders can gain insights into the
agents' inner workings, allowing them to proactively understand the agents,
detect anomalies, and prevent potential failures. Therefore, in this paper, we
present a comprehensive taxonomy of AgentOps, identifying the artifacts and
associated data that should be traced throughout the entire lifecycle of agents
to achieve effective observability. The taxonomy is developed based on a
systematic mapping study of existing AgentOps tools. Our taxonomy serves as a
reference template for developers to design and implement AgentOps
infrastructure that supports monitoring, logging, and analytics. thereby
ensuring AI safety.

- **[2024-11] Advanced System Integration: Analyzing OpenAPI Chunking for
  Retrieval-Augmented Generation** [[arXiv](http://arxiv.org/abs/2411.19804v1)]
  *Robin D. Pesl, Jerin G. Mathew, Massimo Mecella, Marco Aiello*
  Abstract: Integrating multiple (sub-)systems is essential to create advanced
Information Systems (ISs). Difficulties mainly arise when integrating dynamic
environments across the IS lifecycle. A traditional approach is a registry that
provides the API documentation of the systems' endpoints. Large Language Models
(LLMs) have shown to be capable of automatically creating system integrations
(e.g., as service composition) based on this documentation but require concise
input due to input token limitations, especially regarding comprehensive API
descriptions. Currently, it is unknown how best to preprocess these API
descriptions. Within this work, we (i) analyze the usage of Retrieval Augmented
Generation (RAG) for endpoint discovery and the chunking, i.e., preprocessing,
of OpenAPIs to reduce the input token length while preserving the most relevant
information. To further reduce the input token length for the composition
prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that
only receives a summary of the most relevant endpoints and retrieves details on
demand. We evaluate RAG for endpoint discovery using the RestBench benchmark,
first, for the different chunking possibilities and parameters measuring the
endpoint retrieval recall, precision, and F1 score. Then, we assess the
Discovery Agent using the same test set. With our prototype, we demonstrate how
to successfully employ RAG for endpoint discovery to reduce the token count.
While revealing high values for recall, precision, and F1, further research is
necessary to retrieve all requisite endpoints. Our experiments show that for
preprocessing, LLM-based and format-specific approaches outperform na\"ive
chunking methods. Relying on an agent further enhances these results as the
agent splits the tasks into multiple fine granular subtasks, improving the
overall RAG performance in the token count, precision, and F1 score.

- **[2024-10] Domain-Specific Retrieval-Augmented Generation Using Vector Stores,
  Knowledge Graphs, and Tensor Factorization** [[arXiv](http://arxiv.org/abs/2410.02721v1)]
  *Ryan C. Barron, Ves Grantcharov, Selma Wanna, Maksim E. Eren, Manish Bhattarai, Nicholas Solovyev, George Tompkins, Charles Nicholas, Kim Ø. Rasmussen, Cynthia Matuszek, Boian S. Alexandrov*
  Abstract: Large Language Models (LLMs) are pre-trained on large-scale corpora and excel
in numerous general natural language processing (NLP) tasks, such as question
answering (QA). Despite their advanced language capabilities, when it comes to
domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,
knowledge cut-offs, and lack of knowledge attributions. Additionally, fine
tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and
time consuming process. The retrieval-augmented generation (RAG) process has
recently emerged as a method capable of optimization of LLM responses, by
referencing them to a predetermined ontology. It was shown that using a
Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into
account relevant sub-graphs that preserve the information in a structured
manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM
framework, that integrates RAG with KG and a vector store (VS) that store
factual domain specific information. Importantly, to avoid hallucinations in
the KG, we build these highly domain-specific KGs and VSs without the use of
LLMs, but via NLP, data mining, and nonnegative tensor factorization with
automatic model selection. Pairing our RAG with a domain-specific: (i) KG
(containing structured information), and (ii) VS (containing unstructured
information) enables the development of domain-specific chat-bots that
attribute the source of information, mitigate hallucinations, lessen the need
for fine-tuning, and excel in highly domain-specific question answering tasks.
We pair SMART-SLIC with chain-of-thought prompting agents. The framework is
designed to be generalizable to adapt to any specific or specialized domain. In
this paper, we demonstrate the question answering capabilities of our framework
on a corpus of scientific publications on malware analysis and anomaly
detection.

- **[2024-08] LLM Agents Improve Semantic Code Search** [[arXiv](http://arxiv.org/abs/2408.11058v1)]
  *Sarthak Jain, Aditya Dora, Ka Seng Sam, Prabhat Singh*
  Abstract: Code Search is a key task that many programmers often have to perform while
developing solutions to problems. Current methodologies suffer from an
inability to perform accurately on prompts that contain some ambiguity or ones
that require additional context relative to a code-base. We introduce the
approach of using Retrieval Augmented Generation (RAG) powered agents to inject
information into user prompts allowing for better inputs into embedding models.
By utilizing RAG, agents enhance user queries with relevant details from GitHub
repositories, making them more informative and contextually aligned.
Additionally, we introduce a multi-stream ensemble approach which when paired
with agentic workflow can obtain improved retrieval accuracy, which we deploy
on application called repo-rift.com. Experimental results on the CodeSearchNet
dataset demonstrate that RepoRift significantly outperforms existing methods,
achieving an 78.2% success rate at Success@10 and a 34.6% success rate at
Success@1. This research presents a substantial advancement in semantic code
search, highlighting the potential of agentic LLMs and RAG to enhance code
retrieval systems.

- **[2024-08] CodexGraph: Bridging Large Language Models and Code Repositories via
  Code Graph Databases** [[arXiv](http://arxiv.org/abs/2408.03910v2)]
  *Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Shieh, Wenmeng Zhou*
  Abstract: Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval
and MBPP, but struggle with handling entire code repositories. This challenge
has prompted research on enhancing LLM-codebase interaction at a repository
scale. Current solutions rely on similarity-based retrieval or manual tools and
APIs, each with notable drawbacks. Similarity-based retrieval often has low
recall in complex tasks, while manual tools and APIs are typically
task-specific and require expert knowledge, reducing their generalizability
across diverse code tasks and real-world applications. To mitigate these
limitations, we introduce CodexGraph, a system that integrates LLM agents with
graph database interfaces extracted from code repositories. By leveraging the
structural properties of graph databases and the flexibility of the graph query
language, CodexGraph enables the LLM agent to construct and execute queries,
allowing for precise, code structure-aware context retrieval and code
navigation. We assess CodexGraph using three benchmarks: CrossCodeEval,
SWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding
applications. With a unified graph database schema, CodexGraph demonstrates
competitive performance and potential in both academic and real-world
environments, showcasing its versatility and efficacy in software engineering.
Our application demo:
https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.

- **[2024/08] AutoGen Studio: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems** [[Link](https://aclanthology.org/2024.emnlp-demo.8.pdf)]
  *Victor Dibia, Jingya Chen, Gagan Bansal, Suff Syed, Adam Fourney, Erkang Zhu, Chi Wang, Saleema Amershi*
Multi-agent systems, where multiple agents (generative AI models + tools) collaborate, are emerging as an effective pattern for solving long-running, complex tasks in numerous domains. However, specifying their parameters (such as models, tools, and orchestration mechanisms etc,.) and debugging them remains challenging for most developers. To address this challenge, we present AUTOGEN STUDIO, a no-code developer tool for rapidly prototyping, debugging, and evaluating multi-agent workflows built upon the AUTOGEN framework. AUTOGEN STUDIO offers a web interface and a Python API for representing LLM-enabled agents using a declarative (JSON-based) specification. It provides an intuitive drag-and-drop UI for agent workflow specification, interactive evaluation and debugging of workflows, and a gallery of reusable agent components. We highlight four design principles for no-code multi-agent developer tools and contribute an open-source implementation at this https URL

- **[2024/07] OpenHands: An Open Platform for AI Software Developers as Generalist Agents** [[Link](https://openreview.net/pdf?id=OJd3ayDDoF)]
  *Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, Graham Neubig*
Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. In this paper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. We describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on our currently incorporated benchmarks, we perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others. Released under the permissive MIT license, OpenHands is a community project spanning academia and industry with more than 2.1K contributions from over 188 contributors.

- **[2024-07] MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents** [[arXiv](http://arxiv.org/abs/2407.17544v1)]
  *Arya Bulusu, Brandon Man, Ashish Jagmohan, Aditya Vempaty, Jennifer Mari-Wyka, Deepak Akkil*
  Abstract: There has been significant recent interest in harnessing LLMs to control
software systems through multi-step reasoning, planning and tool-usage. While
some promising results have been obtained, application to specific domains
raises several general issues including the control of specialized domain
tools, the lack of existing datasets for training and evaluation, and the
non-triviality of automated system evaluation and improvement. In this paper,
we present a case-study where we examine these issues in the context of a
specific domain. Specifically, we present an automated math visualizer and
solver system for mathematical pedagogy. The system orchestrates mathematical
solvers and math graphing tools to produce accurate visualizations from simple
natural language commands. We describe the creation of specialized data-sets,
and also develop an auto-evaluator to easily evaluate the outputs of our system
by comparing them to ground-truth expressions. We have open sourced the
data-sets and code for the proposed system.

- **[2024-04] AI2Apps: A Visual IDE for Building LLM-based AI Agent Applications** [[arXiv](http://arxiv.org/abs/2404.04902v1)]
  *Xin Pang, Zhucong Li, Jiaxiang Chen, Yuan Cheng, Yinghui Xu, Yuan Qi*
  Abstract: We introduce AI2Apps, a Visual Integrated Development Environment (Visual
IDE) with full-cycle capabilities that accelerates developers to build
deployable LLM-based AI agent Applications. This Visual IDE prioritizes both
the Integrity of its development tools and the Visuality of its components,
ensuring a smooth and efficient building experience.On one hand, AI2Apps
integrates a comprehensive development toolkit ranging from a prototyping
canvas and AI-assisted code editor to agent debugger, management system, and
deployment tools all within a web-based graphical user interface. On the other
hand, AI2Apps visualizes reusable front-end and back-end code as intuitive
drag-and-drop components. Furthermore, a plugin system named AI2Apps Extension
(AAE) is designed for Extensibility, showcasing how a new plugin with 20
components enables web agent to mimic human-like browsing behavior. Our case
study demonstrates substantial efficiency improvements, with AI2Apps reducing
token consumption and API calls when debugging a specific sophisticated
multimodal agent by approximately 90% and 80%, respectively. The AI2Apps,
including an online demo, open-source code, and a screencast video, is now
publicly accessible.

- **[2024/01] CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges.** [[Link](https://aclanthology.org/2024.acl-long.737.pdf)]
  *Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, Zhi Jin*
Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. Real-world software development, however, often involves complex code repositories (named repo) with complex dependencies and extensive documentation. To fill this gap, our research pivots towards evaluating LLMs in a more realistic setting -- real-world repo-level code generation. We introduce CodeAgentBench, a manually curated benchmark for repo-level code generation. This benchmark comprises five high-quality Python projects, encompassing a total of 101 samples. We assess nine leading LLMs on repo-level tasks and observe a decline in their performance. To tackle this, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code symbol navigation, and code testing. We implement four agent strategies to optimize these tools' usage. Our experiments on CodeAgentBench show that CodeAgent enhances LLM performance significantly, with improvements ranging from 18.1\% to 250\%. Further tests on the HumanEval benchmark confirm CodeAgent's adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent's robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.

- **[2024-01] Prompt Design and Engineering: Introduction and Advanced Methods** [[arXiv](http://arxiv.org/abs/2401.14423v4)]
  *Xavier Amatriain*
  Abstract: Prompt design and engineering has rapidly become essential for maximizing the
potential of large language models. In this paper, we introduce core concepts,
advanced techniques like Chain-of-Thought and Reflection, and the principles
behind building LLM-based agents. Finally, we provide a survey of tools for
prompt engineers.

- **[2023-12] GitAgent: Facilitating Autonomous Agent with GitHub by Tool Extension** [[arXiv](http://arxiv.org/abs/2312.17294v1)]
  *Bohan Lyu, Xin Cong, Heyang Yu, Pan Yang, Yujia Qin, Yining Ye, Yaxi Lu, Zhong Zhang, Yukun Yan, Yankai Lin, Zhiyuan Liu, Maosong Sun*
  Abstract: While Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated
exceptional proficiency in natural language processing, their efficacy in
addressing complex, multifaceted tasks remains limited. A growing area of
research focuses on LLM-based agents equipped with external tools capable of
performing diverse tasks. However, existing LLM-based agents only support a
limited set of tools which is unable to cover a diverse range of user queries,
especially for those involving expertise domains. It remains a challenge for
LLM-based agents to extend their tools autonomously when confronted with
various user queries. As GitHub has hosted a multitude of repositories which
can be seen as a good resource for tools, a promising solution is that
LLM-based agents can autonomously integrate the repositories in GitHub
according to the user queries to extend their tool set. In this paper, we
introduce GitAgent, an agent capable of achieving the autonomous tool extension
from GitHub. GitAgent follows a four-phase procedure to incorporate
repositories and it can learn human experience by resorting to GitHub
Issues/PRs to solve problems encountered during the procedure. Experimental
evaluation involving 30 user queries demonstrates GitAgent's effectiveness,
achieving a 69.4% success rate on average.

- **[2023/10] RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models** [[Link](https://dl.acm.org/doi/abs/10.1145/3627673.3680016)]
  *Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Jihong Wang, Fengbin Yin, Lunting Fan, Lingfei Wu, Qingsong Wen*
Large language model (LLM) applications in cloud root cause analysis (RCA) have been actively explored recently. However, current methods are still reliant on manual workflow settings and do not unleash LLMs' decision-making and environment interaction capabilities. We present RCAgent, a tool-augmented LLM autonomous agent framework for practical and privacy-aware industrial RCA usage. Running on an internally deployed model rather than GPT families, RCAgent is capable of free-form data collection and comprehensive analysis with tools. Our framework combines a variety of enhancements, including a unique Self-Consistency for action trajectories, and a suite of methods for context management, stabilization, and importing domain knowledge. Our experiments show RCAgent's evident and consistent superiority over ReAct across all aspects of RCA -- predicting root causes, solutions, evidence, and responsibilities -- and tasks covered or uncovered by current rules, as validated by both automated metrics and human evaluations. Furthermore, RCAgent has already been integrated into the diagnosis and issue discovery workflow of the Real-time Compute Platform for Apache Flink of Alibaba Cloud.

- **[2023/08] Gentopia.AI: A Collaborative Platform for Tool-Augmented LLMs.** [[Link](https://aclanthology.org/2023.emnlp-demo.20.pdf)]
  *Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, Dongkuan Xu*
Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation. We present gentopia, an ALM framework enabling flexible customization of agents through simple configurations, seamlessly integrating various language models, task formats, prompting modules, and plugins into a unified paradigm. Furthermore, we establish gentpool, a public platform enabling the registration and sharing of user-customized agents. Agents registered in gentpool are composable such that they can be assembled together for agent collaboration, advancing the democratization of artificial intelligence. To ensure high-quality agents, gentbench, an integral component of gentpool, is designed to thoroughly evaluate user-customized agents across diverse aspects such as safety, robustness, efficiency, etc. We release gentopia on Github and will continuously move forward.


### Automation of Development Workflows
- **[2024-12] Beyond pip install: Evaluating LLM Agents for the Automated Installation
  of Python Projects** [[arXiv](http://arxiv.org/abs/2412.06294v1)]
  *Louis Milliken, Sungmin Kang, Shin Yoo*
  Abstract: Many works have recently proposed the use of Large Language Model (LLM) based
agents for performing `repository level' tasks, loosely defined as a set of
tasks whose scopes are greater than a single file. This has led to speculation
that the orchestration of these repository-level tasks could lead to software
engineering agents capable of performing almost independently of human
intervention. However, of the suite of tasks that would need to be performed by
this autonomous software engineering agent, we argue that one important task is
missing, which is to fulfil project level dependency by installing other
repositories. To investigate the feasibility of this repository level
installation task, we introduce a benchmark of of repository installation tasks
curated from 40 open source Python projects, which includes a ground truth
installation process for each target repository. Further, we propose
Installamatic, an agent which aims to perform and verify the installation of a
given repository by searching for relevant instructions from documentation in
the repository. Empirical experiments reveal that that 55% of the studied
repositories can be automatically installed by our agent at least one out of
ten times. Through further analysis, we identify the common causes for our
agent's inability to install a repository, discuss the challenges faced in the
design and implementation of such an agent and consider the implications that
such an agent could have for developers.

- **[2024/12] LAW: Legal Agentic Workflows for Custody and Fund Services Contracts** [[Link](https://aclanthology.org/2025.coling-industry.50.pdf)]
  *William Watson, Nicole Cho, Nishan Srishankar, Zhen Zeng, Lucas Cecchi, Daniel Scott, Suchetha Siddagangappa, Rachneet Kaur, Tucker Balch, Manuela Veloso*
Legal contracts in the custody and fund services domain govern critical aspects such as key provider responsibilities, fee schedules, and indemnification rights. However, it is challenging for an off-the-shelf Large Language Model (LLM) to ingest these contracts due to the lengthy unstructured streams of text, limited LLM context windows, and complex legal jargon. To address these challenges, we introduce LAW (Legal Agentic Workflows for Custody and Fund Services Contracts). LAW features a modular design that responds to user queries by orchestrating a suite of domain-specific tools and text agents. Our experiments demonstrate that LAW, by integrating multiple specialized agents and tools, significantly outperforms the baseline. LAW excels particularly in complex tasks such as calculating a contract's termination date, surpassing the baseline by 92.9% points. Furthermore, LAW offers a cost-effective alternative to traditional fine-tuned legal LLMs by leveraging reusable, domain-specific tools.

- **[2024-12] Generative AI Toolkit -- a framework for increasing the quality of
  LLM-based applications over their whole life cycle** [[arXiv](http://arxiv.org/abs/2412.14215v1)]
  *Jens Kohl, Luisa Gloger, Rui Costa, Otto Kruse, Manuel P. Luitz, David Katz, Gonzalo Barbeito, Markus Schweier, Ryan French, Jonas Schroeder, Thomas Riedl, Raphael Perri, Youssef Mostafa*
  Abstract: As LLM-based applications reach millions of customers, ensuring their
scalability and continuous quality improvement is critical for success.
However, the current workflows for developing, maintaining, and operating
(DevOps) these applications are predominantly manual, slow, and based on
trial-and-error. With this paper we introduce the Generative AI Toolkit, which
automates essential workflows over the whole life cycle of LLM-based
applications. The toolkit helps to configure, test, continuously monitor and
optimize Generative AI applications such as agents, thus significantly
improving quality while shortening release cycles. We showcase the
effectiveness of our toolkit on representative use cases, share best practices,
and outline future enhancements. Since we are convinced that our Generative AI
Toolkit is helpful for other teams, we are open sourcing it on and hope that
others will use, forward, adapt and improve

- **[2024-12] Adaptable and Precise: Enterprise-Scenario LLM Function-Calling
  Capability Training Pipeline** [[arXiv](http://arxiv.org/abs/2412.15660v1)]
  *Guancheng Zeng, Wentao Ding, Beining Xu, Chi Zhang, Wenqiang Han, Gang Li, Jingjing Mo, Pengxu Qiu, Xinran Tao, Wang Tao, Haowen Hu*
  Abstract: Enterprises possess a vast array of API assets scattered across various
functions, forming the backbone of existing business processes. By leveraging
these APIs as functional tools, enterprises can design diverse,
scenario-specific agent applications, driven by on-premise function-calling
models as the core engine. However, generic models often fail to meet
enterprise requirements in terms of computational efficiency, output accuracy,
and stability, necessitating scenario-specific adaptation. In this paper, we
propose a training pipeline for function-calling capabilities tailored to
real-world business scenarios. This pipeline includes the synthesis and
augmentation of scenario-specific function-calling data, model fine-tuning, and
performance evaluation and analysis. Using this pipeline, we generated 1,260
fully AI-generated samples and 1,035 augmented manually-labeled samples in
digital HR agent scenario. The Qwen2.5-Coder-7B-Instruct model was employed as
the base model and fine-tuned using the LoRA method on four GPUs with 24GB
VRAM. Our fine-tuned model demonstrated outstanding performance in evaluations
and practical applications, surpassing GPT-4 and GPT-4o in accuracy on the test
set. These results validate the reliability of the proposed pipeline for
training scenario-specific function-calling models.

- **[2024/11] WorkflowLLM: Enhancing Workflow Orchestration Capability of Large Language Models** [[Link](https://openreview.net/forum?id=3Hy00Wvabi)]
  *Shengda Fan, Xin Cong, Yuepeng Fu, Zhong Zhang, Shuyan Zhang, Yuanwei Liu, Yesai Wu, Yankai Lin, Zhiyuan Liu, Maosong Sun*
Recent advancements in large language models (LLMs) have driven a revolutionary paradigm shift in process automation from Robotic Process Automation to Agentic Process Automation by automating the workflow orchestration procedure based on LLMs. However, existing LLMs (even the advanced OpenAI GPT-4o) are confined to achieving satisfactory capability in workflow orchestration. To address this limitation, we present WorkflowLLM, a data-centric framework elaborately designed to enhance the capability of LLMs in workflow orchestration. It first constructs a large-scale fine-tuning dataset WorkflowBench with 106,763 samples, covering 1,503 APIs from 83 applications across 28 categories. Specifically, the construction process can be divided into three phases: (1) Data Collection: we collect real-world workflow data from Apple Shortcuts and RoutineHub, transcribing them into Python-style code. We further equip them with generated hierarchical thought via ChatGPT. (2) Query Expansion: we prompt ChatGPT to generate more task queries to enrich the diversity and complexity of workflows. (3) Workflow Generation: we leverage an annotator model trained on collected data to generate workflows for synthesized queries. Finally, we merge the synthetic samples that pass quality confirmation with the collected samples to obtain the WorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain WorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong capacity to orchestrate complex workflows, while also achieving notable generalization performance on previously unseen APIs. Additionally, WorkflowBench exhibits robust zero-shot generalization capabilities on an out-of-distribution task planning dataset, T-Eval. Our data and code are available at this https URL.

- **[2024-10] SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning** [[arXiv](http://arxiv.org/abs/2410.17238v1)]
  *Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei, Bangbang Liu, Tianqi Pang, Jacky Kwok, Ceyao Zhang, Bang Liu, Chenglin Wu*
  Abstract: Automated Machine Learning (AutoML) approaches encompass traditional methods
that optimize fixed pipelines for model selection and ensembling, as well as
newer LLM-based frameworks that autonomously build pipelines. While LLM-based
agents have shown promise in automating machine learning tasks, they often
generate low-diversity and suboptimal code, even after multiple iterations. To
overcome these limitations, we introduce Tree-Search Enhanced LLM Agents
(SELA), an innovative agent-based system that leverages Monte Carlo Tree Search
(MCTS) to optimize the AutoML process. By representing pipeline configurations
as trees, our framework enables agents to conduct experiments intelligently and
iteratively refine their strategies, facilitating a more effective exploration
of the machine learning solution space. This novel approach allows SELA to
discover optimal pathways based on experimental feedback, improving the overall
quality of the solutions. In an extensive evaluation across 20 machine learning
datasets, we compare the performance of traditional and agent-based AutoML
methods, demonstrating that SELA achieves a win rate of 65% to 80% against each
baseline across all datasets. These results underscore the significant
potential of agent-based strategies in AutoML, offering a fresh perspective on
tackling complex machine learning challenges.

- **[2024-09] Language Model Powered Digital Biology with BRAD** [[arXiv](http://arxiv.org/abs/2409.02864v3)]
  *Joshua Pickard, Ram Prakash, Marc Andrew Choi, Natalie Oliven, Cooper Stansbury, Jillian Cwycyshyn, Alex Gorodetsky, Alvaro Velasquez, Indika Rajapakse*
  Abstract: Recent advancements in Large Language Models (LLMs) are transforming biology,
computer science, engineering, and every day life. However, integrating the
wide array of computational tools, databases, and scientific literature
continues to pose a challenge to biological research. LLMs are well-suited for
unstructured integration, efficient information retrieval, and automating
standard workflows and actions from these diverse resources. To harness these
capabilities in bioinformatics, we present a prototype Bioinformatics Retrieval
Augmented Digital assistant (BRAD). BRAD is a chatbot and agentic system that
integrates a variety of bioinformatics tools. The Python package implements an
AI \texttt{Agent} that is powered by LLMs and connects to a local file system,
online databases, and a user's software. The \texttt{Agent} is highly
configurable, enabling tasks such as Retrieval-Augmented Generation, searches
across bioinformatics databases, and the execution of software pipelines.
BRAD's coordinated integration of bioinformatics tools delivers a context-aware
and semi-autonomous system that extends beyond the capabilities of conventional
LLM-based chatbots. A graphical user interface (GUI) provides an intuitive
interface to the system.

- **[2024/07] OpenHands: An Open Platform for AI Software Developers as Generalist Agents** [[Link](https://openreview.net/pdf?id=OJd3ayDDoF)]
  *Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, Graham Neubig*
Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. In this paper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. We describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on our currently incorporated benchmarks, we perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others. Released under the permissive MIT license, OpenHands is a community project spanning academia and industry with more than 2.1K contributions from over 188 contributors.

- **[2024/07] Building AI Agents for Autonomous Clouds: Challenges and Design Principles** [[Link](https://dl.acm.org/doi/pdf/10.1145/3698038.3698525)]
  *Manish Shetty, Yinfang Chen, Gagan Somashekar, Minghua Ma, Yogesh Simmhan, Xuchao Zhang, Jonathan Mace, Dax Vandevoorde, Pedro Las-Casas, Shachee Mishra Gupta, Suman Nath, Chetan Bansal, Saravan Rajmohan*
The rapid growth in the use of Large Language Models (LLMs) and AI Agents as part of software development and deployment is revolutionizing the information technology landscape. While code generation receives significant attention, a higher-impact application lies in using AI agents for operational resilience of cloud services, which currently require significant human effort and domain knowledge. There is a growing interest in AI for IT Operations (AIOps) which aims to automate complex operational tasks, like fault localization and root cause analysis, thereby reducing human intervention and customer impact. However, achieving the vision of autonomous and self-healing clouds through AIOps is hampered by the lack of standardized frameworks for building, evaluating, and improving AIOps agents. This vision paper lays the groundwork for such a framework by first framing the requirements and then discussing design decisions that satisfy them. We also propose AIOpsLab, a prototype implementation leveraging agent-cloud-interface that orchestrates an application, injects real-time faults using chaos engineering, and interfaces with an agent to localize and resolve the faults. We report promising results and lay the groundwork to build a modular and robust framework for building, evaluating, and improving agents for autonomous clouds.

- **[2024-07] The Vision of Autonomic Computing: Can LLMs Make It a Reality?** [[arXiv](http://arxiv.org/abs/2407.14402v1)]
  *Zhiyang Zhang, Fangkai Yang, Xiaoting Qin, Jue Zhang, Qingwei Lin, Gong Cheng, Dongmei Zhang, Saravan Rajmohan, Qi Zhang*
  Abstract: The Vision of Autonomic Computing (ACV), proposed over two decades ago,
envisions computing systems that self-manage akin to biological organisms,
adapting seamlessly to changing environments. Despite decades of research,
achieving ACV remains challenging due to the dynamic and complex nature of
modern computing systems. Recent advancements in Large Language Models (LLMs)
offer promising solutions to these challenges by leveraging their extensive
knowledge, language understanding, and task automation capabilities. This paper
explores the feasibility of realizing ACV through an LLM-based multi-agent
framework for microservice management. We introduce a five-level taxonomy for
autonomous service maintenance and present an online evaluation benchmark based
on the Sock Shop microservice demo project to assess our framework's
performance. Our findings demonstrate significant progress towards achieving
Level 3 autonomy, highlighting the effectiveness of LLMs in detecting and
resolving issues within microservice architectures. This study contributes to
advancing autonomic computing by pioneering the integration of LLMs into
microservice management frameworks, paving the way for more adaptive and
self-managing computing systems. The code will be made available at
https://aka.ms/ACV-LLM.

- **[2024-07] Towards Automated Data Sciences with Natural Language and SageCopilot:
  Practices and Lessons Learned** [[arXiv](http://arxiv.org/abs/2407.21040v1)]
  *Yuan Liao, Jiang Bian, Yuhui Yun, Shuo Wang, Yubo Zhang, Jiaming Chu, Tao Wang, Kewei Li, Yuchen Li, Xuhong Li, Shilei Ji, Haoyi Xiong*
  Abstract: While the field of NL2SQL has made significant advancements in translating
natural language instructions into executable SQL scripts for data querying and
processing, achieving full automation within the broader data science pipeline
- encompassing data querying, analysis, visualization, and reporting - remains
a complex challenge. This study introduces SageCopilot, an advanced,
industry-grade system system that automates the data science pipeline by
integrating Large Language Models (LLMs), Autonomous Agents (AutoAgents), and
Language User Interfaces (LUIs). Specifically, SageCopilot incorporates a
two-phase design: an online component refining users' inputs into executable
scripts through In-Context Learning (ICL) and running the scripts for results
reporting & visualization, and an offline preparing demonstrations requested by
ICL in the online phase. A list of trending strategies such as Chain-of-Thought
and prompt-tuning have been used to augment SageCopilot for enhanced
performance. Through rigorous testing and comparative analysis against
prompt-based solutions, SageCopilot has been empirically validated to achieve
superior end-to-end performance in generating or executing scripts and offering
results with visualization, backed by real-world datasets. Our in-depth
ablation studies highlight the individual contributions of various components
and strategies used by SageCopilot to the end-to-end correctness for data
sciences.

- **[2024-03] AutoDev: Automated AI-Driven Development** [[arXiv](https://arxiv.org/pdf/2403.08299)]
  *Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian Moghaddam, Neel Sundaresan*
  Abstract: The landscape of software development has witnessed a paradigm shift with the
advent of AI-powered assistants, exemplified by GitHub Copilot. However,
existing solutions are not leveraging all the potential capabilities available
in an IDE such as building, testing, executing code, git operations, etc.
Therefore, they are constrained by their limited capabilities, primarily
focusing on suggesting code snippets and file manipulation within a chat-based
interface. To fill this gap, we present AutoDev, a fully automated AI-driven
software development framework, designed for autonomous planning and execution
of intricate software engineering tasks. AutoDev enables users to define
complex software engineering objectives, which are assigned to AutoDev's
autonomous AI Agents to achieve. These AI agents can perform diverse operations
on a codebase, including file editing, retrieval, build processes, execution,
testing, and git operations. They also have access to files, compiler output,
build and testing logs, static analysis tools, and more. This enables the AI
Agents to execute tasks in a fully automated manner with a comprehensive
understanding of the contextual information required. Furthermore, AutoDev
establishes a secure development environment by confining all operations within
Docker containers. This framework incorporates guardrails to ensure user
privacy and file security, allowing users to define specific permitted or
restricted commands and operations within AutoDev. In our evaluation, we tested
AutoDev on the HumanEval dataset, obtaining promising results with 91.5% and
87.8% of Pass@1 for code generation and test generation respectively,
demonstrating its effectiveness in automating software engineering tasks while
maintaining a secure and user-controlled development environment.

### Cross-Tool Integration
- **[2024/10] AFlow: Automating Agentic Workflow Generation** [[Link](https://openreview.net/pdf?id=z5uVAKwmjf)]
  *Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, Chenglin Wu*
Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFlow, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7% average improvement over state-of-the-art baselines. Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars. The code will be available at this https URL.

- **[2024-10] From Cool Demos to Production-Ready FMware: Core Challenges and a
  Technology Roadmap** [[arXiv](http://arxiv.org/abs/2410.20791v2)]
  *Gopi Krishnan Rajbahadur, Gustavo A. Oliva, Dayi Lin, Ahmed E. Hassan*
  Abstract: The rapid expansion of foundation models (FMs), such as large language models
(LLMs), has given rise to FMware--software systems that integrate FMs as core
components. While building demonstration-level FMware is relatively
straightforward, transitioning to production-ready systems presents numerous
challenges, including reliability, high implementation costs, scalability, and
compliance with privacy regulations. Our paper conducts a semi-structured
thematic synthesis to identify the key challenges in productionizing FMware
across diverse data sources including our own industry experience in developing
FMArts--a FMware lifecycle engineering platform and integrating it into Huawei
cloud, grey literature, academic publications, hands-on involvement in the Open
Platform for Enterprise AI (OPEA), organizing the AIware conference and
Bootcamp, and co-leading the ISO SPDX SBOM working group on AI and datasets. We
identify critical issues in FM selection, data and model alignment, prompt
engineering, agent orchestration, system testing, and deployment, alongside
cross-cutting concerns such as memory management, observability, and feedback
integration. We discuss needed technologies and strategies to address these
challenges and offer guidance on how to enable the transition from
demonstration systems to scalable, production-ready FMware solutions. Our
findings underscore the importance of continued research and multi-industry
collaboration to advance the development of production-ready FMware.

- **[2024-06] On The Importance of Reasoning for Context Retrieval in Repository-Level
  Code Editing** [[arXiv](http://arxiv.org/abs/2406.04464v1)]
  *Alexander Kovrigin, Aleksandra Eliseeva, Yaroslav Zharov, Timofey Bryksin*
  Abstract: Recent advancements in code-fluent Large Language Models (LLMs) enabled the
research on repository-level code editing. In such tasks, the model navigates
and modifies the entire codebase of a project according to request. Hence, such
tasks require efficient context retrieval, i.e., navigating vast codebases to
gather relevant context. Despite the recognized importance of context
retrieval, existing studies tend to approach repository-level coding tasks in
an end-to-end manner, rendering the impact of individual components within
these complicated systems unclear. In this work, we decouple the task of
context retrieval from the other components of the repository-level code
editing pipelines. We lay the groundwork to define the strengths and weaknesses
of this component and the role that reasoning plays in it by conducting
experiments that focus solely on context retrieval. We conclude that while the
reasoning helps to improve the precision of the gathered context, it still
lacks the ability to identify its sufficiency. We also outline the ultimate
role of the specialized tools in the process of context gathering. The code
supplementing this paper is available at
https://github.com/JetBrains-Research/ai-agents-code-editing.

- **[2024-06] CodeNav: Beyond tool-use to using real-world codebases with LLM agents** [[arXiv](http://arxiv.org/abs/2406.12276v1)]
  *Tanmay Gupta, Luca Weihs, Aniruddha Kembhavi*
  Abstract: We present CodeNav, an LLM agent that navigates and leverages previously
unseen code repositories to solve user queries. In contrast to tool-use LLM
agents that require ``registration'' of all relevant tools via manual
descriptions within the LLM context, CodeNav automatically indexes and searches
over code blocks in the target codebase, finds relevant code snippets, imports
them, and uses them to iteratively generate a solution with execution feedback.
To highlight the core-capabilities of CodeNav, we first showcase three case
studies where we use CodeNav for solving complex user queries using three
diverse codebases. Next, on three benchmarks, we quantitatively compare the
effectiveness of code-use (which only has access to the target codebase) to
tool-use (which has privileged access to all tool names and descriptions).
Finally, we study the effect of varying kinds of tool and library descriptions
on code-use performance, as well as investigate the advantage of the agent
seeing source code as opposed to natural descriptions of code. All code will be
made open source under a permissive license.

- **[2024/05] DepsRAG: Towards Agentic Reasoning and Planning for Software Dependency Management** [[Link](https://openreview.net/pdf?id=I396ZJFZLq)]
  *Mohannad Alhanahnah, Yazan Boshmaf*
  In the era of Large Language Models (LLMs) with their advanced capabilities, a unique opportunity arises to develop LLM-based digital assistant tools that can support software developers by facilitating comprehensive reasoning about software dependencies and open-source libraries before importing them. This reasoning process is daunting, mandating multiple specialized tools and dedicated expertise, each focusing on distinct aspects (e.g., security analysis tools may overlook design flaws such as circular dependencies, which hinder software maintainability). Creating a significant bottleneck in the software development lifecycle. In this paper, we introduce DepsRAG, a multi-agent framework designed to assist developers in reasoning about software dependencies. DepsRAG first constructs a comprehensive Knowledge Graph (KG) that includes both direct and transitive dependencies. Developers can interact with DepsRAG through a conversational interface, posing queries about the dependencies. DepsRAG employs Retrieval-Augmented Generation (RAG) to enhance these queries by retrieving relevant information from the KG as well as external sources, such as the Web and vulnerability databases, thus demonstrating its adaptability to novel scenarios. DepsRAG incorporates a Critic-Agent feedback loop to ensure the accuracy and clarity of LLM-generated responses. We evaluated DepsRAG using GPT-4-Turbo and Llama-3 on three multi-step reasoning tasks, observing a threefold increase in accuracy with the integration of the Critic-Agent mechanism. DepsRAG demo and implementation are available: this https URL.




## Evaluation and Benchmarking

### Performance Evaluation
- **[2025-01] AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling
  Autonomous Clouds** [[arXiv](http://arxiv.org/abs/2501.06706v1)]
  *Yinfang Chen, Manish Shetty, Gagan Somashekar, Minghua Ma, Yogesh Simmhan, Jonathan Mace, Chetan Bansal, Rujia Wang, Saravan Rajmohan*
  Abstract: AI for IT Operations (AIOps) aims to automate complex operational tasks, such
as fault localization and root cause analysis, to reduce human workload and
minimize customer impact. While traditional DevOps tools and AIOps algorithms
often focus on addressing isolated operational tasks, recent advances in Large
Language Models (LLMs) and AI agents are revolutionizing AIOps by enabling
end-to-end and multitask automation. This paper envisions a future where AI
agents autonomously manage operational tasks throughout the entire incident
lifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps.
Realizing this vision requires a comprehensive framework to guide the design,
development, and evaluation of these agents. To this end, we present AIOPSLAB,
a framework that not only deploys microservice cloud environments, injects
faults, generates workloads, and exports telemetry data but also orchestrates
these components and provides interfaces for interacting with and evaluating
agents. We discuss the key requirements for such a holistic framework and
demonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps
agents. Through evaluations of state-of-the-art LLM agents within the benchmark
created by AIOPSLAB, we provide insights into their capabilities and
limitations in handling complex operational tasks in cloud environments.

- **[2025-01] Large Language Model Critics for Execution-Free Evaluation of Code
  Changes** [[arXiv](http://arxiv.org/abs/2501.16655v1)]
  *Aashish Yadavally, Hoan Nguyen, Laurent Callot, Gauthier Guinet*
  Abstract: Large language models (LLMs) offer a promising way forward for automating
software engineering tasks, such as bug fixes, feature additions, etc., via
multi-step LLM-based agentic workflows. However, existing metrics for
evaluating such workflows, mainly build status and occasionally log analysis,
are too sparse and limited in providing the information needed to assess the
quality of changes made. In this work, we designed LLM-based critics to derive
well-structured and rigorous intermediate/step-level, execution-free evaluation
proxies for repo-level code changes. Importantly, we assume access to the gold
test patch for the problem (i.e., reference-aware) to assess both semantics and
executability of generated patches. With the gold test patch as a reference, we
predict executability of all editing locations with an F1 score of 91.6%,
aggregating which, we can predict the build status in 84.8% of the instances in
SWE-bench. In particular, such an execution-focused LLM critic outperforms
other reference-free and reference-aware LLM critics by 38.9% to 72.5%.
Moreover, we demonstrate the usefulness of such a reference-aware framework in
comparing patches generated by different agentic workflows. Finally, we
open-source the library developed for this project, which allows further usage
for either other agentic workflows or other benchmarks. The source code is
available at https://github.com/amazon-science/code-agent-eval.

- **[2024-12] Can Large Language Models Serve as Evaluators for Code Summarization?** [[arXiv](http://arxiv.org/abs/2412.01333v1)]
  *Yang Wu, Yao Wan, Zhaoyang Chu, Wenting Zhao, Ye Liu, Hongyu Zhang, Xuanhua Shi, Philip S. Yu*
  Abstract: Code summarization facilitates program comprehension and software maintenance
by converting code snippets into natural-language descriptions. Over the years,
numerous methods have been developed for this task, but a key challenge
remains: effectively evaluating the quality of generated summaries. While human
evaluation is effective for assessing code summary quality, it is
labor-intensive and difficult to scale. Commonly used automatic metrics, such
as BLEU, ROUGE-L, METEOR, and BERTScore, often fail to align closely with human
judgments. In this paper, we explore the potential of Large Language Models
(LLMs) for evaluating code summarization. We propose CODERPE (Role-Player for
Code Summarization Evaluation), a novel method that leverages role-player
prompting to assess the quality of generated summaries. Specifically, we prompt
an LLM agent to play diverse roles, such as code reviewer, code author, code
editor, and system analyst. Each role evaluates the quality of code summaries
across key dimensions, including coherence, consistency, fluency, and
relevance. We further explore the robustness of LLMs as evaluators by employing
various prompting strategies, including chain-of-thought reasoning, in-context
learning, and tailored rating form designs. The results demonstrate that LLMs
serve as effective evaluators for code summarization methods. Notably, our
LLM-based evaluator, CODERPE , achieves an 81.59% Spearman correlation with
human evaluations, outperforming the existing BERTScore metric by 17.27%.

- **[2024-12] Lachesis: Predicting LLM Inference Accuracy using Structural Properties
  of Reasoning Paths** [[arXiv](http://arxiv.org/abs/2412.08281v1)]
  *Naryeong Kim, Sungmin Kang, Gabin An, Shin Yoo*
  Abstract: Large Language Models are increasingly used to build agents to perform more
complex tasks. As LLMs perform more complicated reasoning through longer
interactions, self-consistency, i.e., the idea that the answer obtained from
sampling and marginalising a number of multiple independent inferences is more
likely to be correct, has received much attention as a simple validation
technique. This paper aims to empirically verify this intuitive hypothesis by
predicting the correctness of answers obtained using self-consistency from
properties of the samples of reasoning paths. We introduce Lachesis, a
predictive model for self-consistency based LLM inferences, and empirically
evaluate it using AutoFL, a recently proposed LLM-based fault localisation
technique, as the target technique that uses self-consistency. Lachesis
converts collected reasoning paths from AutoFL using specifically designed
reasoning path representations, and trains LSTM and GCN models to predict
whether a given set of reasoning paths would result in a correct answer. The
results suggest that Lachesis can predict the correctness of answers with a
precision of up to 0.8136, highlighting the possibility of training a
predictive model that can allow early termination of inferences that are not
likely to be successful.

- **[2024/10] SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?** [[Link](https://openreview.net/pdf?id=riTiq3i21b)]
  *John Yang, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida I. Wang, Ofir Press*
Autonomous systems for software engineering are now capable of fixing bugs and developing features. These systems are commonly evaluated on SWE-bench (Jimenez et al., 2024a), which assesses their ability to solve software issues from GitHub repositories. However, SWE-bench uses only Python repositories, with problem statements presented predominantly as text and lacking visual elements such as images. This limited coverage motivates our inquiry into how existing systems might perform on unrepresented software engineering domains (e.g., front-end, game development, DevOps), which use different programming languages and paradigms. Therefore, we propose SWE-bench Multimodal (SWE-bench M), to evaluate systems on their ability to fix bugs in visual, user-facing JavaScript software. SWE-bench M features 617 task instances collected from 17 JavaScript libraries used for web interface design, diagramming, data visualization, syntax highlighting, and interactive mapping. Each SWE-bench M task instance contains at least one image in its problem statement or unit tests. Our analysis finds that top-performing SWE-bench systems struggle with SWE-bench M, revealing limitations in visual problem-solving and cross-language generalization. Lastly, we show that SWE-agent's flexible language-agnostic features enable it to substantially outperform alternatives on SWE-bench M, resolving 12% of task instances compared to 6% for the next best system.

- **[2024-10] Evaluating Software Development Agents: Patch Patterns, Code Quality,
  and Issue Complexity in Real-World GitHub Scenarios** [[arXiv](http://arxiv.org/abs/2410.12468v2)]
  *Zhi Chen, Lingxiao Jiang*
  Abstract: In recent years, AI-based software engineering has progressed from
pre-trained models to advanced agentic workflows, with Software Development
Agents representing the next major leap. These agents, capable of reasoning,
planning, and interacting with external environments, offer promising solutions
to complex software engineering tasks. However, while much research has
evaluated code generated by large language models (LLMs), comprehensive studies
on agent-generated patches, particularly in real-world settings, are lacking.
This study addresses that gap by evaluating 4,892 patches from 10 top-ranked
agents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on
their impact on code quality. Our analysis shows no single agent dominated,
with 170 issues unresolved, indicating room for improvement. Even for patches
that passed unit tests and resolved issues, agents made different file and
function modifications compared to the gold patches from repository developers,
revealing limitations in the benchmark's test case coverage. Most agents
maintained code reliability and security, avoiding new bugs or vulnerabilities;
while some agents increased code complexity, many reduced code duplication and
minimized code smells. Finally, agents performed better on simpler codebases,
suggesting that breaking complex tasks into smaller sub-tasks could improve
effectiveness. This study provides the first comprehensive evaluation of
agent-generated patches on real-world GitHub issues, offering insights to
advance AI-driven software development.

- **[2024-10] Improving Performance of Commercially Available AI Products in a
  Multi-Agent Configuration** [[arXiv](http://arxiv.org/abs/2410.22129v1)]
  *Cory Hymel, Sida Peng, Kevin Xu, Charath Ranganathan*
  Abstract: In recent years, with the rapid advancement of large language models (LLMs),
multi-agent systems have become increasingly more capable of practical
application. At the same time, the software development industry has had a
number of new AI-powered tools developed that improve the software development
lifecycle (SDLC). Academically, much attention has been paid to the role of
multi-agent systems to the SDLC. And, while single-agent systems have
frequently been examined in real-world applications, we have seen comparatively
few real-world examples of publicly available commercial tools working together
in a multi-agent system with measurable improvements. In this experiment we
test context sharing between Crowdbotics PRD AI, a tool for generating software
requirements using AI, and GitHub Copilot, an AI pair-programming tool. By
sharing business requirements from PRD AI, we improve the code suggestion
capabilities of GitHub Copilot by 13.8% and developer task success rate by
24.5% -- demonstrating a real-world example of commercially-available AI
systems working together with improved outcomes.

- **[2024/09] SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories.** [[Link](https://aclanthology.org/2024.emnlp-main.702.pdf)]
  *Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, Tushar Khot*
Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPERaims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.

- **[2024/08] LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification Testsuites** [[Link](https://ieeexplore.ieee.org/document/10820751)]
  *Zachariah Sollenberger, Jay Patel, Christian Munley, Aaron Jarmusch, Sunita Chandrasekaran*
Large Language Models (LLM) are evolving and have significantly revolutionized the landscape of software development. If used well, they can significantly accelerate the software development cycle. At the same time, the community is very cautious of the models being trained on biased or sensitive data, which can lead to biased outputs along with the inadvertent release of confidential information. Additionally, the carbon footprints and the un-explainability of these black box models continue to raise questions about the usability of LLMs.
With the abundance of opportunities LLMs have to offer, this paper explores the idea of judging tests used to evaluate compiler implementations of directive-based programming models as well as probe into the black box of LLMs. Based on our results, utilizing an agent-based prompting approach and setting up a validation pipeline structure drastically increased the quality of DeepSeek Coder, the LLM chosen for the evaluation purposes.

- **[2024-07] PyBench: Evaluating LLM Agent on various real-world coding tasks** [[arXiv](http://arxiv.org/abs/2407.16732v2)]
  *Yaolun Zhang, Yinxu Pan, Yudong Wang, Jie Cai*
  Abstract: The LLM Agent, equipped with a code interpreter, is capable of automatically
solving real-world coding tasks, such as data analysis and image editing.
  However, existing benchmarks primarily focus on either simplistic tasks, such
as completing a few lines of code, or on extremely complex and specific tasks
at the repository level, neither of which are representative of various daily
coding tasks.
  To address this gap, we introduce \textbf{PyBench}, a benchmark encompassing
five main categories of real-world tasks, covering more than 10 types of files.
Given a high-level user query and related files, the LLM Agent needs to reason
and execute Python code via a code interpreter for a few turns before making a
formal response to fulfill the user's requirements. Successfully addressing
tasks in PyBench demands a robust understanding of various Python packages,
superior reasoning capabilities, and the ability to incorporate feedback from
executed code. Our evaluations indicate that current open-source LLMs are
struggling with these tasks. Hence, we conduct analysis and experiments on four
kinds of datasets proving that comprehensive abilities are needed for PyBench.
Our fine-tuned 8B size model: \textbf{PyLlama3} achieves an exciting
performance on PyBench which surpasses many 33B and 70B size models. Our
Benchmark, Training Dataset, and Model are available at:
{https://github.com/Mercury7353/PyBench}

- **[2024/05] HumanEvalComm: Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent** [[Link](https://dl.acm.org/doi/abs/10.1145/3715109)]
  *Jie JW Wu, Fatemeh H Fard*
Large language models (LLMs) have significantly improved their ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks.
In this work, we conducted an empirical study on the benchmark and analysis of the communication skills of LLMs for code generation. We define communication skills of LLMs as ``being able to ask clarifying questions when the description of the code generation problem has issues''. We created a new benchmark, HumanEvalComm, by modifying problem descriptions according to three issues: inconsistency, ambiguity, incompleteness. We defined new evaluation metrics such as Communication Rate and Good Question Rate, and then experimented on HumanEvalComm with different Code LLMs, and a new LLM agent approach, Okanagan, to identify and ask questions in ambiguous parts from code and descriptions for further refining the generated code. Finally, we discussed evaluation results by comparing Code LLMs and Okanagan with our findings.

- **[2024-04] Large Language Model Evaluation Via Multi AI Agents: Preliminary results** [[arXiv](http://arxiv.org/abs/2404.01023v1)]
  *Zeeshan Rasheed, Muhammad Waseem, Kari Systä, Pekka Abrahamsson*
  Abstract: As Large Language Models (LLMs) have become integral to both research and
daily operations, rigorous evaluation is crucial. This assessment is important
not only for individual tasks but also for understanding their societal impact
and potential risks. Despite extensive efforts to examine LLMs from various
perspectives, there is a noticeable lack of multi-agent AI models specifically
designed to evaluate the performance of different LLMs. To address this gap, we
introduce a novel multi-agent AI model that aims to assess and compare the
performance of various LLMs. Our model consists of eight distinct AI agents,
each responsible for retrieving code based on a common description from
different advanced language models, including GPT-3.5, GPT-3.5 Turbo, GPT-4,
GPT-4 Turbo, Google Bard, LLAMA, and Hugging Face. Our developed model utilizes
the API of each language model to retrieve code for a given high-level
description. Additionally, we developed a verification agent, tasked with the
critical role of evaluating the code generated by its counterparts. We
integrate the HumanEval benchmark into our verification agent to assess the
generated code's performance, providing insights into their respective
capabilities and efficiencies. Our initial results indicate that the GPT-3.5
Turbo model's performance is comparatively better than the other models. This
preliminary analysis serves as a benchmark, comparing their performances side
by side. Our future goal is to enhance the evaluation process by incorporating
the Massively Multitask Benchmark for Python (MBPP) benchmark, which is
expected to further refine our assessment. Additionally, we plan to share our
developed model with twenty practitioners from various backgrounds to test our
model and collect their feedback for further improvement.

- **[2024/02] Understanding the Weakness of Large Language Model Agents within a Complex Android Environment.** [[Link](https://dl.acm.org/doi/10.1145/3637528.3671650)]
  *Mingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fan Yang, Zhen Xiao*
Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation}, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, and evaluation code for AndroidArena are released at this https URL.

- **[2023/07] Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures** [[Link](https://www.scitepress.org/PublicationsDetail.aspx?ID=1Ne2ZQRkAVs=&t=1)]
  *Sayed Erfan Arefin, Tasnia Ashrafi Heya, Hasan Al-Qudah, Ynes Ineza, Abdul Serwadda*
The transformative influence of Large Language Models (LLMs) is profoundly reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT distinguishes itself within these models, demonstrating remarkable performance in multi-turn conversations and exhibiting code proficiency across an array of languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges. Our focus is on the python programming language and problems centered on data structures and algorithms, two topics at the very foundations of Computer Science. We evaluate ChatGPT for its ability to generate correct solutions to the problems fed to it, its code quality, and nature of run-time errors thrown by its code. Where ChatGPT code successfully executes, but fails to solve the problem at hand, we look into patterns in the test cases passed in order to gain some insights into how wrong ChatGPT code is in these kinds of situations. To infer whether ChatGPT might have directly memorized some of the data that was used to train it, we methodically design an experiment to investigate this phenomena. Making comparisons with human performance whenever feasible, we investigate all the above questions from the context of both its underlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics within the main topics, and on problems having varying degrees of difficulty.
Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation}, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, and evaluation code for AndroidArena are released at this https URL.

- **[2023/06] InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback** [[Link](https://openreview.net/pdf?id=fvKaLF1ns8)]
  *John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao*
  Abstract: Humans write code in a fundamentally interactive manner and rely on constant
execution feedback to correct errors, resolve ambiguities, and decompose tasks.
While LLMs have recently exhibited promising coding capabilities, current
coding benchmarks mostly consider a static instruction-to-code sequence
transduction process, which has the potential for error propagation and a
disconnect between the generated code and its final execution environment. To
address this gap, we introduce InterCode, a lightweight, flexible, and
easy-to-use framework of interactive coding as a standard reinforcement
learning (RL) environment, with code as actions and execution feedback as
observations. Our framework is language and platform agnostic, uses
self-contained Docker environments to provide safe and reproducible execution,
and is compatible out-of-the-box with traditional seq2seq coding methods, while
enabling the development of new methods for interactive code generation. We use
InterCode to create three interactive code environments with Bash, SQL, and
Python as action spaces, leveraging data from the static NL2Bash, Spider, and
MBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating
multiple state-of-the-art LLMs configured with different prompting strategies
such as ReAct and Plan & Solve. Our results showcase the benefits of
interactive code generation and demonstrate that InterCode can serve as a
challenging benchmark for advancing code understanding and generation
capabilities. InterCode is designed to be easily extensible and can even be
used to create new tasks such as Capture the Flag, a popular coding puzzle that
is inherently multi-step and involves multiple programming languages. Project
site with code and data: https://intercode-benchmark.github.io


### Benchmarking Frameworks
- **[2025-02] CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science
  Research Repositories** [[arXiv](http://arxiv.org/abs/2502.06111v2)]
  *Yijia Xiao, Runhui Wang, Luyang Kong, Davor Golac, Wei Wang*
  Abstract: The increasing complexity of computer science research projects demands more
effective tools for deploying code repositories. Large Language Models (LLMs),
such as Anthropic Claude and Meta Llama, have demonstrated significant
advancements across various fields of computer science research, including the
automation of diverse software engineering tasks. To evaluate the effectiveness
of LLMs in handling complex code development tasks of research projects,
particularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark
for Computer Science Research projects. This benchmark assesses LLMs from
various aspects including accuracy, efficiency, and deployment script quality,
aiming to explore their potential in conducting computer science research
autonomously. We also introduce a novel framework, CSR-Agents, that utilizes
multiple LLM agents to automate the deployment of GitHub code repositories of
computer science research projects. Specifically, by checking instructions from
markdown files and interpreting repository structures, the model generates and
iteratively improves bash commands that set up the experimental environments
and deploy the code to conduct research tasks. Preliminary results from
CSR-Bench indicate that LLM agents can significantly enhance the workflow of
repository deployment, thereby boosting developer productivity and improving
the management of developmental workflows.

- **[2025-02] Benchmarking Prompt Engineering Techniques for Secure Code Generation
  with GPT Models** [[arXiv](http://arxiv.org/abs/2502.06039v1)]
  *Marc Bruni, Fabio Gabrielli, Mohammad Ghafari, Martin Kropp*
  Abstract: Prompt engineering reduces reasoning mistakes in Large Language Models
(LLMs). However, its effectiveness in mitigating vulnerabilities in
LLM-generated code remains underexplored. To address this gap, we implemented a
benchmark to automatically assess the impact of various prompt engineering
strategies on code security. Our benchmark leverages two peer-reviewed prompt
datasets and employs static scanners to evaluate code security at scale. We
tested multiple prompt engineering techniques on GPT-3.5-turbo, GPT-4o, and
GPT-4o-mini. Our results show that for GPT-4o and GPT-4o-mini, a
security-focused prompt prefix can reduce the occurrence of security
vulnerabilities by up to 56%. Additionally, all tested models demonstrated the
ability to detect and repair between 41.9% and 68.7% of vulnerabilities in
previously generated code when using iterative prompting techniques. Finally,
we introduce a "prompt agent" that demonstrates how the most effective
techniques can be applied in real-world development workflows.

- **[2025-01] AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling
  Autonomous Clouds** [[arXiv](http://arxiv.org/abs/2501.06706v1)]
  *Yinfang Chen, Manish Shetty, Gagan Somashekar, Minghua Ma, Yogesh Simmhan, Jonathan Mace, Chetan Bansal, Rujia Wang, Saravan Rajmohan*
  Abstract: AI for IT Operations (AIOps) aims to automate complex operational tasks, such
as fault localization and root cause analysis, to reduce human workload and
minimize customer impact. While traditional DevOps tools and AIOps algorithms
often focus on addressing isolated operational tasks, recent advances in Large
Language Models (LLMs) and AI agents are revolutionizing AIOps by enabling
end-to-end and multitask automation. This paper envisions a future where AI
agents autonomously manage operational tasks throughout the entire incident
lifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps.
Realizing this vision requires a comprehensive framework to guide the design,
development, and evaluation of these agents. To this end, we present AIOPSLAB,
a framework that not only deploys microservice cloud environments, injects
faults, generates workloads, and exports telemetry data but also orchestrates
these components and provides interfaces for interacting with and evaluating
agents. We discuss the key requirements for such a holistic framework and
demonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps
agents. Through evaluations of state-of-the-art LLM agents within the benchmark
created by AIOPSLAB, we provide insights into their capabilities and
limitations in handling complex operational tasks in cloud environments.

- **[2024-12] EvalSVA: Multi-Agent Evaluators for Next-Gen Software Vulnerability
  Assessment** [[arXiv](http://arxiv.org/abs/2501.14737v1)]
  *Xin-Cheng Wen, Jiaxin Ye, Cuiyun Gao, Lianwei Wu, Qing Liao*
  Abstract: Software Vulnerability (SV) assessment is a crucial process of determining
different aspects of SVs (e.g., attack vectors and scope) for developers to
effectively prioritize efforts in vulnerability mitigation. It presents a
challenging and laborious process due to the complexity of SVs and the scarcity
of labeled data. To mitigate the above challenges, we introduce EvalSVA, a
multi-agent evaluators team to autonomously deliberate and evaluate various
aspects of SV assessment. Specifically, we propose a multi-agent-based
framework to simulate vulnerability assessment strategies in real-world
scenarios, which employs multiple Large Language Models (LLMs) into an
integrated group to enhance the effectiveness of SV assessment in the limited
data. We also design diverse communication strategies to autonomously discuss
and assess different aspects of SV. Furthermore, we construct a multi-lingual
SV assessment dataset based on the new standard of CVSS, comprising 699, 888,
and 1,310 vulnerability-related commits in C++, Python, and Java, respectively.
Our experimental results demonstrate that EvalSVA averagely outperforms the
44.12\% accuracy and 43.29\% F1 for SV assessment compared with the previous
methods. It shows that EvalSVA offers a human-like process and generates both
reason and answer for SV assessment. EvalSVA can also aid human experts in SV
assessment, which provides more explanation and details for SV assessment.

- **[2024/11] RedCode: Risky Code Execution and Generation Benchmark for Code Agents** [[Link](https://openreview.net/pdf?id=mAG68wdggA)]
  *Chengquan Guo, Xun Liu, Chulin Xie, Andy Zhou, Yi Zeng, Zinan Lin, Dawn Song, Bo Li*
With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding, safety concerns, such as generating or executing risky code, have become significant barriers to the real-world deployment of these agents. To provide comprehensive and practical evaluations on the safety of code agents, we propose RedCode, a benchmark for risky code execution and generation: (1) RedCode-Exec provides challenging prompts that could lead to risky code execution, aiming to evaluate code agents' ability to recognize and handle unsafe code. We provide a total of 4,050 risky test cases in Python and Bash tasks with diverse input formats including code snippets and natural text. They covers 25 types of critical vulnerabilities spanning 8 domains (e.g., websites, file systems). We provide Docker environments and design corresponding evaluation metrics to assess their execution results. (2) RedCode-Gen provides 160 prompts with function signatures and docstrings as input to assess whether code agents will follow instructions to generate harmful code or software. Our empirical findings, derived from evaluating three agent frameworks based on 19 LLMs, provide insights into code agents' vulnerabilities. For instance, evaluations on RedCode-Exec show that agents are more likely to reject executing risky operations on the operating system, but are less likely to reject executing technically buggy code, indicating high risks. Risky operations described in natural text lead to a lower rejection rate than those in code format. Additionally, evaluations on RedCode-Gen show that more capable base models and agents with stronger overall coding abilities, such as GPT4, tend to produce more sophisticated and effective harmful software. Our findings highlight the need for stringent safety evaluations for diverse code agents. Our dataset and code are available at this https URL.

- **[2024-10] Codev-Bench: How Do LLMs Understand Developer-Centric Code Completion?** [[arXiv](http://arxiv.org/abs/2410.01353v3)]
  *Zhenyu Pan, Rongyu Cao, Yongchang Cao, Yingwei Ma, Binhua Li, Fei Huang, Han Liu, Yongbin Li*
  Abstract: Code completion, a key downstream task in code generation, is one of the most
frequent and impactful methods for enhancing developer productivity in software
development. As intelligent completion tools evolve, we need a robust
evaluation benchmark that enables meaningful comparisons between products and
guides future advancements. However, existing benchmarks focus more on
coarse-grained tasks without industrial analysis resembling general code
generation rather than the real-world scenarios developers encounter. Moreover,
these benchmarks often rely on costly and time-consuming human annotation, and
the standalone test cases fail to leverage minimal tests for maximum
repository-level understanding and code coverage. To address these limitations,
we first analyze business data from an industrial code completion tool and
redefine the evaluation criteria to better align with the developer's intent
and desired completion behavior throughout the coding process. Based on these
insights, we introduce Codev-Agent, an agent-based system that automates
repository crawling, constructs execution environments, extracts dynamic
calling chains from existing unit tests, and generates new test samples to
avoid data leakage, ensuring fair and effective comparisons. Using Codev-Agent,
we present the Code-Development Benchmark (Codev-Bench), a fine-grained,
real-world, repository-level, and developer-centric evaluation framework.
Codev-Bench assesses whether a code completion tool can capture a developer's
immediate intent and suggest appropriate code across diverse contexts,
providing a more realistic benchmark for code completion in modern software
development.

- **[2024-10] Automatic Generation of Benchmarks and Reliable LLM Judgment for Code
  Tasks** [[arXiv](http://arxiv.org/abs/2410.21071v1)]
  *Eitan Farchi, Shmulik Froimovich, Rami Katan, Orna Raz*
  Abstract: LLMs can be used in a variety of code related tasks such as translating from
one programming language to another, implementing natural language requirements
and code summarization. Artifacts generated by state of the art LLM technology
are expected to be useful in the sense that a user will be able to use the LLM
generated artifact after a small number of easy modifications. Quantifying this
vague notion is challenging and it is thus hard to determine the quality of
code related LLM solutions. We refer to evaluation of LLM solutions using LLM
judgment as "LLM as a Judge", or LaaJ for short. In this work we introduce a
methodology to generate and evaluate LaaJ implementations, utilizing an
automatically generated benchmark. The purpose of the benchmark is two fold,
namely, it is used both to develop and validate the LaaJs and to validate and
test the LLM code related solution using the LaaJs. To that end, we developed
an automated benchmark generation engine, which generates code in multiple
programming languages for multiple code related tasks and which serves as the
input for LaaJ evaluation. We utilize a graph representation, G, of the
potential code related generations. The graph vertices are generated artifacts
and edges represent possible generations, e.g., the generation of a Java
program from its natural language requirements. Utilizing a chain of LLM agents
and G we generate code related artifacts. Using cycles in G we formulate
expectations on the generated artifacts. Taking advantage of these formulated
expectations enables the development and testing of reliable LLM judgement for
usefulness of the artifacts generated by the solution. Our approach enables the
creation of high quality code task solutions.

- **[2024-08] SWE-bench-java: A GitHub Issue Resolving Benchmark for Java** [[arXiv](http://arxiv.org/abs/2408.14354v1)]
  *Daoguang Zan, Zhirong Huang, Ailun Yu, Shaoxin Lin, Yifan Shi, Wei Liu, Dong Chen, Zongshuai Qi, Hao Yu, Lei Yu, Dezhi Ran, Muhan Zeng, Bo Shen, Pan Bian, Guangtai Liang, Bei Guan, Pengjie Huang, Tao Xie, Yongji Wang, Qianxiang Wang*
  Abstract: GitHub issue resolving is a critical task in software engineering, recently
gaining significant attention in both industry and academia. Within this task,
SWE-bench has been released to evaluate issue resolving capabilities of large
language models (LLMs), but has so far only focused on Python version. However,
supporting more programming languages is also important, as there is a strong
demand in industry. As a first step toward multilingual support, we have
developed a Java version of SWE-bench, called SWE-bench-java. We have publicly
released the dataset, along with the corresponding Docker-based evaluation
environment and leaderboard, which will be continuously maintained and updated
in the coming months. To verify the reliability of SWE-bench-java, we implement
a classic method SWE-agent and test several powerful LLMs on it. As is well
known, developing a high-quality multi-lingual benchmark is time-consuming and
labor-intensive, so we welcome contributions through pull requests or
collaboration to accelerate its iteration and refinement, paving the way for
fully automated programming.

- **[2024/07] ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents** [[Link](https://openreview.net/pdf?id=kKILfPkhSz)]
  *Haiyang Shen, Yue Li, Desong Meng, Dongqi Cai, Sheng Qi, Li Zhang, Mengwei Xu, Yun Ma*
Recent advancements in integrating large language models (LLMs) with application programming interfaces (APIs) have gained significant interest in both academia and industry. Recent work demonstrates that these API-based agents exhibit relatively strong autonomy and planning capabilities. However, their ability to handle multi-dimensional difficulty levels, diverse task types, and real-world demands remains unknown. In this paper, we introduce \textsc{ShortcutsBench}, a large-scale benchmark for the comprehensive evaluation of API-based agents in solving real-world complex tasks. \textsc{ShortcutsBench} includes a wealth of real APIs from Apple Inc., refined user queries, human-annotated high-quality action sequences, detailed parameter filling values, and parameters requesting necessary input from the system or user. We revealed how existing benchmarks~/~datasets struggle to accommodate the advanced reasoning capabilities of existing more intelligent LLMs. Moreover, our extensive evaluation of agents built with 5 leading open-source (size ≥ 57B) and 5 closed-source LLMs (e.g. Gemini-1.5-Pro and GPT-4o-mini) with varying intelligence level reveals significant limitations of existing API-based agents in the whole process of handling complex queries related to API selection, parameter filling, and requesting necessary input from the system and the user. These findings highlight the great challenges that API-based agents face in effectively fulfilling real and complex user queries. All datasets, code, experimental logs, and results are available at \url{this https URL}.
- **[2024/06] miniCodeProps: a Minimal Benchmark for Proving Code Properties** [[Link](https://openreview.net/attachment?id=6QFe3vPbYZ&name=pdf)]
  *Evan Lohn, Sean Welleck*
AI agents have shown initial promise in automating mathematical theorem proving in proof assistants such as Lean. The same proof assistants can be used to verify the correctness of code by pairing code with specifications and proofs that the specifications hold. Automating the writing of code, specifications, and proofs could lower the cost of verification, or, ambitiously, enable an AI agent to output safe, provably correct code. However, it remains unclear whether current neural theorem provers can automatically verify even relatively simple programs. We present miniCodeProps, a benchmark of 201 program specifications in the Lean proof assistant, aimed at the subproblem of automatically generating a proof for a provided program and specification. miniCodeProps contains specifications about simple, self-contained programs (e.g., lists, natural numbers, binary trees) with varied proof difficulty. Despite its simplicity, miniCodeProps is sufficient to break current LLM-based provers, with state-of-the-art methods showing promise on the easy properties in miniCodeProps, yet failing to prove nearly all of the medium and hard properties. We publicly release miniCodeProps as a benchmark for furthering automated theorem proving in the context of formally verified code.
- **[2024/06] SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents** [[Link](https://openreview.net/pdf?id=9Y8zUO11EQ)]
  *Niels Mündler, Mark Niklas Müller, Jingxuan He, Martin Vechev*
Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents to formalize user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth bug-fixes, and golden tests. We find that LLMs generally perform surprisingly well at generating relevant test cases, with Code Agents designed for code repair exceeding the performance of systems designed specifically for test generation. Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using issue reproduction rate and coverage changes, providing a dual metric for analyzing systems designed for code repair. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-Agent. We release all data and code at this https URL
- **[2023/10] MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use.** [[Link](https://openreview.net/pdf?id=R0c2qtalgG)]
  *Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, Lichao Sun*
Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-tool scenarios. Subsequently, we set the tasks for both tool usage awareness and tool selection. We define four subtasks from different perspectives in tool selection, including tool selection with similar choices, tool selection in specific scenarios, tool selection with possible reliability issues, and multi-tool selection. We conduct experiments involving eight popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents. However, through the error analysis, we found there is still significant room for improvement. Finally, we conclude with insights for tool developers -- we strongly recommend that tool developers choose an appropriate rewrite model for generating new descriptions based on the downstream LLM the tool will apply to. Our code is in this https URL.
- **[2023/06] InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback** [[Link](https://openreview.net/pdf?id=fvKaLF1ns8)]
  *John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao*
  Abstract: Humans write code in a fundamentally interactive manner and rely on constant
execution feedback to correct errors, resolve ambiguities, and decompose tasks.
While LLMs have recently exhibited promising coding capabilities, current
coding benchmarks mostly consider a static instruction-to-code sequence
transduction process, which has the potential for error propagation and a
disconnect between the generated code and its final execution environment. To
address this gap, we introduce InterCode, a lightweight, flexible, and
easy-to-use framework of interactive coding as a standard reinforcement
learning (RL) environment, with code as actions and execution feedback as
observations. Our framework is language and platform agnostic, uses
self-contained Docker environments to provide safe and reproducible execution,
and is compatible out-of-the-box with traditional seq2seq coding methods, while
enabling the development of new methods for interactive code generation. We use
InterCode to create three interactive code environments with Bash, SQL, and
Python as action spaces, leveraging data from the static NL2Bash, Spider, and
MBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating
multiple state-of-the-art LLMs configured with different prompting strategies
such as ReAct and Plan & Solve. Our results showcase the benefits of
interactive code generation and demonstrate that InterCode can serve as a
challenging benchmark for advancing code understanding and generation
capabilities. InterCode is designed to be easily extensible and can even be
used to create new tasks such as Capture the Flag, a popular coding puzzle that
is inherently multi-step and involves multiple programming languages. Project
site with code and data: https://intercode-benchmark.github.io



## Safety and Alignment

- **[2025-01] A sketch of an AI control safety case** [[arXiv](http://arxiv.org/abs/2501.17315v1)]
  *Tomek Korbak, Joshua Clymer, Benjamin Hilton, Buck Shlegeris, Geoffrey Irving*
  Abstract: As LLM agents gain a greater capacity to cause harm, AI developers might
increasingly rely on control measures such as monitoring to justify that they
are safe. We sketch how developers could construct a "control safety case",
which is a structured argument that models are incapable of subverting control
measures in order to cause unacceptable outcomes. As a case study, we sketch an
argument that a hypothetical LLM agent deployed internally at an AI company
won't exfiltrate sensitive information. The sketch relies on evidence from a
"control evaluation,"' where a red team deliberately designs models to
exfiltrate data in a proxy for the deployment environment. The safety case then
hinges on several claims: (1) the red team adequately elicits model
capabilities to exfiltrate data, (2) control measures remain at least as
effective in deployment, and (3) developers conservatively extrapolate model
performance to predict the probability of data exfiltration in deployment. This
safety case sketch is a step toward more concrete arguments that can be used to
show that a dangerously capable LLM agent is safe to deploy.

- **[2024-12] The Fusion of Large Language Models and Formal Methods for Trustworthy
  AI Agents: A Roadmap** [[arXiv](http://arxiv.org/abs/2412.06512v1)]
  *Yedi Zhang, Yufan Cai, Xinyue Zuo, Xiaokun Luan, Kailong Wang, Zhe Hou, Yifan Zhang, Zhiyuan Wei, Meng Sun, Jun Sun, Jing Sun, Jin Song Dong*
  Abstract: Large Language Models (LLMs) have emerged as a transformative AI paradigm,
profoundly influencing daily life through their exceptional language
understanding and contextual generation capabilities. Despite their remarkable
performance, LLMs face a critical challenge: the propensity to produce
unreliable outputs due to the inherent limitations of their learning-based
nature. Formal methods (FMs), on the other hand, are a well-established
computation paradigm that provides mathematically rigorous techniques for
modeling, specifying, and verifying the correctness of systems. FMs have been
extensively applied in mission-critical software engineering, embedded systems,
and cybersecurity. However, the primary challenge impeding the deployment of
FMs in real-world settings lies in their steep learning curves, the absence of
user-friendly interfaces, and issues with efficiency and adaptability.
  This position paper outlines a roadmap for advancing the next generation of
trustworthy AI systems by leveraging the mutual enhancement of LLMs and FMs.
First, we illustrate how FMs, including reasoning and certification techniques,
can help LLMs generate more reliable and formally certified outputs.
Subsequently, we highlight how the advanced learning capabilities and
adaptability of LLMs can significantly enhance the usability, efficiency, and
scalability of existing FM tools. Finally, we show that unifying these two
computation paradigms -- integrating the flexibility and intelligence of LLMs
with the rigorous reasoning abilities of FMs -- has transformative potential
for the development of trustworthy AI software systems. We acknowledge that
this integration has the potential to enhance both the trustworthiness and
efficiency of software engineering practices while fostering the development of
intelligent FM tools capable of addressing complex yet real-world challenges.

- **[2024-12] Seeker: Towards Exception Safety Code Generation with Intermediate
  Language Agents Framework** [[arXiv](http://arxiv.org/abs/2412.11713v1)]
  *Xuanming Zhang, Yuxuan Chen, Yiming Zheng, Zhexin Zhang, Yuan Yuan, Minlie Huang*
  Abstract: In real world software development, improper or missing exception handling
can severely impact the robustness and reliability of code. Exception handling
mechanisms require developers to detect, capture, and manage exceptions
according to high standards, but many developers struggle with these tasks,
leading to fragile code. This problem is particularly evident in open-source
projects and impacts the overall quality of the software ecosystem. To address
this challenge, we explore the use of large language models (LLMs) to improve
exception handling in code. Through extensive analysis, we identify three key
issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception
Block, and Distorted Handling Solution. These problems are widespread across
real world repositories, suggesting that robust exception handling practices
are often overlooked or mishandled. In response, we propose Seeker, a
multi-agent framework inspired by expert developer strategies for exception
handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler
to assist LLMs in detecting, capturing, and resolving exceptions more
effectively. Our work is the first systematic study on leveraging LLMs to
enhance exception handling practices in real development scenarios, providing
valuable insights for future improvements in code reliability.

- **[2024-12] Defining and Detecting the Defects of the Large Language Model-based
  Autonomous Agents** [[arXiv](http://arxiv.org/abs/2412.18371v2)]
  *Kaiwen Ning, Jiachi Chen, Jingwen Zhang, Wei Li, Zexu Wang, Yuming Feng, Weizhe Zhang, Zibin Zheng*
  Abstract: AI agents are systems capable of perceiving their environment, autonomously
planning and executing tasks. Recent advancements in LLM have introduced a
transformative paradigm for AI agents, enabling them to interact with external
resources and tools through prompts. In such agents, the workflow integrates
developer-written code, which manages framework construction and logic control,
with LLM-generated natural language that enhances dynamic decision-making and
interaction. However, discrepancies between developer-implemented logic and the
dynamically generated content of LLMs in terms of behavior and expected
outcomes can lead to defects, such as tool invocation failures and task
execution errors. These issues introduce specific risks, leading to various
defects in LLM-based AI Agents, such as service interruptions. Despite the
importance of these issues, there is a lack of systematic work that focuses on
analyzing LLM-based AI Agents to uncover defects in their code. In this paper,
we present the first study focused on identifying and detecting defects in LLM
Agents. We collected and analyzed 6,854 relevant posts from StackOverflow to
define 8 types of agent defects. For each type, we provided detailed
descriptions with an example. Then, we designed a static analysis tool, named
Agentable, to detect the defects. Agentable leverages Code Property Graphs and
LLMs to analyze Agent workflows by efficiently identifying specific code
patterns and analyzing natural language descriptions. To evaluate Agentable, we
constructed two datasets: AgentSet, consists of 84 real-world Agents, and
AgentTest, which contains 78 Agents specifically designed to include various
types of defects. Our results show that Agentable achieved an overall accuracy
of 88.79% and a recall rate of 91.03%. Furthermore, our analysis reveals the
889 defects of the AgentSet, highlighting the prevalence of these defects.

- **[2024/04] Concept-Guided LLM Agents for Human-AI Safety Codesign** [[Link](https://ojs.aaai.org/index.php/AAAI-SS/article/view/31188/33348)]
  *Florian Geissler, Karsten Roscher, Mario Trapp*
 Generative AI is increasingly important in software engineering, including safety engineering, where its use ensures that software does not cause harm to people. This also leads to high quality requirements for generative AI. Therefore, the simplistic use of Large Language Models (LLMs) alone will not meet these quality demands. It is crucial to develop more advanced and sophisticated approaches that can effectively address the complexities and safety concerns of software systems. Ultimately, humans must understand and take responsibility for the suggestions provided by generative AI to ensure system safety. To this end, we present an efficient, hybrid strategy to leverage LLMs for safety analysis and Human-AI codesign. In particular, we develop a customized LLM agent that uses elements of prompt engineering, heuristic reasoning, and retrieval-augmented generation to solve tasks associated with predefined safety concepts, in interaction with a system model graph. The reasoning is guided by a cascade of micro-decisions that help preserve structured information. We further suggest a graph verbalization which acts as an intermediate representation of the system model to facilitate LLM-graph interactions. Selected pairs of prompts and responses relevant for safety analytics illustrate our method for the use case of a simplified automated driving system.

- **[2024-01] Sleeper Agents: Training Deceptive LLMs that Persist Through Safety
  Training** [[arXiv](http://arxiv.org/abs/2401.05566v3)]
  *Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, Ethan Perez*
  Abstract: Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoor behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoor behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety.
